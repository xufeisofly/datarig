[
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "importlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib",
        "description": "importlib",
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "gmtime",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "strftime",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "BinaryIO",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "BinaryIO",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "zstandard",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "zstandard",
        "description": "zstandard",
        "detail": "zstandard",
        "documentation": {}
    },
    {
        "label": "gzip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gzip",
        "description": "gzip",
        "detail": "gzip",
        "documentation": {}
    },
    {
        "label": "jsonlines",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jsonlines",
        "description": "jsonlines",
        "detail": "jsonlines",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "S3Path",
        "importPath": "cloudpathlib",
        "description": "cloudpathlib",
        "isExtraImport": true,
        "detail": "cloudpathlib",
        "documentation": {}
    },
    {
        "label": "S3Path",
        "importPath": "cloudpathlib",
        "description": "cloudpathlib",
        "isExtraImport": true,
        "detail": "cloudpathlib",
        "documentation": {}
    },
    {
        "label": "S3Path",
        "importPath": "cloudpathlib",
        "description": "cloudpathlib",
        "isExtraImport": true,
        "detail": "cloudpathlib",
        "documentation": {}
    },
    {
        "label": "S3Path",
        "importPath": "cloudpathlib",
        "description": "cloudpathlib",
        "isExtraImport": true,
        "detail": "cloudpathlib",
        "documentation": {}
    },
    {
        "label": "boto3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "boto3",
        "description": "boto3",
        "detail": "boto3",
        "documentation": {}
    },
    {
        "label": "pathlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pathlib",
        "description": "pathlib",
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "safe_load",
        "importPath": "yaml",
        "description": "yaml",
        "isExtraImport": true,
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "safe_load",
        "importPath": "yaml",
        "description": "yaml",
        "isExtraImport": true,
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "get_mapper",
        "importPath": "baselines.core.factories",
        "description": "baselines.core.factories",
        "isExtraImport": true,
        "detail": "baselines.core.factories",
        "documentation": {}
    },
    {
        "label": "get_aggregator",
        "importPath": "baselines.core.factories",
        "description": "baselines.core.factories",
        "isExtraImport": true,
        "detail": "baselines.core.factories",
        "documentation": {}
    },
    {
        "label": "get_transform",
        "importPath": "baselines.core.factories",
        "description": "baselines.core.factories",
        "isExtraImport": true,
        "detail": "baselines.core.factories",
        "documentation": {}
    },
    {
        "label": "read_jsonl",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "write_jsonl",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "makedirs_if_missing",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "delete_file",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "is_exists",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "write_jsonl",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "read_jsonl",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "makedirs_if_missing",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "is_s3",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "read_jsonl",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "write_jsonl",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "delete_file",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "is_exists",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "is_s3",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "is_s3",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "read_jsonl",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "write_jsonl",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "is_s3",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "makedirs_if_missing",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "delete_file",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "is_exists",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "list_dir",
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "isExtraImport": true,
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "PROCESS_SETUP_KEY_NAME",
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "isExtraImport": true,
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "PROCESS_END_KEY_NAME",
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "isExtraImport": true,
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "COMMIT_KEY_NAME",
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "isExtraImport": true,
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "GLOBAL_FUNCTIONS",
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "isExtraImport": true,
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "CONTENT",
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "isExtraImport": true,
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "isExtraImport": true,
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "CONTENT",
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "isExtraImport": true,
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "CONTENT",
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "isExtraImport": true,
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "CONTENT",
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "isExtraImport": true,
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "split_paragraphs",
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "isExtraImport": true,
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "split_words",
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "isExtraImport": true,
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "split_sentences",
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "isExtraImport": true,
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "split_paragraphs",
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "isExtraImport": true,
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "split_sentences",
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "isExtraImport": true,
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "split_words",
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "isExtraImport": true,
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "split_paragraphs",
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "isExtraImport": true,
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "split_words",
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "isExtraImport": true,
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "DEDUP_NORMALIZERS",
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "isExtraImport": true,
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "isExtraImport": true,
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "detect_langs",
        "importPath": "langdetect",
        "description": "langdetect",
        "isExtraImport": true,
        "detail": "langdetect",
        "documentation": {}
    },
    {
        "label": "CONTENT",
        "importPath": "core.constants",
        "description": "core.constants",
        "isExtraImport": true,
        "detail": "core.constants",
        "documentation": {}
    },
    {
        "label": "CONTENT",
        "importPath": "core.constants",
        "description": "core.constants",
        "isExtraImport": true,
        "detail": "core.constants",
        "documentation": {}
    },
    {
        "label": "CONTENT",
        "importPath": "core.constants",
        "description": "core.constants",
        "isExtraImport": true,
        "detail": "core.constants",
        "documentation": {}
    },
    {
        "label": "CONTENT",
        "importPath": "core.constants",
        "description": "core.constants",
        "isExtraImport": true,
        "detail": "core.constants",
        "documentation": {}
    },
    {
        "label": "CONTENT",
        "importPath": "core.constants",
        "description": "core.constants",
        "isExtraImport": true,
        "detail": "core.constants",
        "documentation": {}
    },
    {
        "label": "URL",
        "importPath": "core.constants",
        "description": "core.constants",
        "isExtraImport": true,
        "detail": "core.constants",
        "documentation": {}
    },
    {
        "label": "CONTENT",
        "importPath": "core.constants",
        "description": "core.constants",
        "isExtraImport": true,
        "detail": "core.constants",
        "documentation": {}
    },
    {
        "label": "CHUNK",
        "importPath": "core.constants",
        "description": "core.constants",
        "isExtraImport": true,
        "detail": "core.constants",
        "documentation": {}
    },
    {
        "label": "factory_function",
        "importPath": "core.factory_utils",
        "description": "core.factory_utils",
        "isExtraImport": true,
        "detail": "core.factory_utils",
        "documentation": {}
    },
    {
        "label": "factory_function",
        "importPath": "core.factory_utils",
        "description": "core.factory_utils",
        "isExtraImport": true,
        "detail": "core.factory_utils",
        "documentation": {}
    },
    {
        "label": "factory_function",
        "importPath": "core.factory_utils",
        "description": "core.factory_utils",
        "isExtraImport": true,
        "detail": "core.factory_utils",
        "documentation": {}
    },
    {
        "label": "factory_function",
        "importPath": "core.factory_utils",
        "description": "core.factory_utils",
        "isExtraImport": true,
        "detail": "core.factory_utils",
        "documentation": {}
    },
    {
        "label": "factory_function",
        "importPath": "core.factory_utils",
        "description": "core.factory_utils",
        "isExtraImport": true,
        "detail": "core.factory_utils",
        "documentation": {}
    },
    {
        "label": "factory_function",
        "importPath": "core.factory_utils",
        "description": "core.factory_utils",
        "isExtraImport": true,
        "detail": "core.factory_utils",
        "documentation": {}
    },
    {
        "label": "factory_function",
        "importPath": "core.factory_utils",
        "description": "core.factory_utils",
        "isExtraImport": true,
        "detail": "core.factory_utils",
        "documentation": {}
    },
    {
        "label": "fasttext",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fasttext",
        "description": "fasttext",
        "detail": "fasttext",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "unicodedata",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unicodedata",
        "description": "unicodedata",
        "detail": "unicodedata",
        "documentation": {}
    },
    {
        "label": "kenlm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "kenlm",
        "description": "kenlm",
        "detail": "kenlm",
        "documentation": {}
    },
    {
        "label": "sentencepiece",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sentencepiece",
        "description": "sentencepiece",
        "detail": "sentencepiece",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "ngrams",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPTNeoXTokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaTokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPTNeoXTokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaTokenizerFast",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "Blacklist",
        "importPath": "retrie.retrie",
        "description": "retrie.retrie",
        "isExtraImport": true,
        "detail": "retrie.retrie",
        "documentation": {}
    },
    {
        "label": "Blacklist",
        "importPath": "retrie.retrie",
        "description": "retrie.retrie",
        "isExtraImport": true,
        "detail": "retrie.retrie",
        "documentation": {}
    },
    {
        "label": "Blacklist",
        "importPath": "retrie.retrie",
        "description": "retrie.retrie",
        "isExtraImport": true,
        "detail": "retrie.retrie",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "words",
        "importPath": "uniseg.wordbreak",
        "description": "uniseg.wordbreak",
        "isExtraImport": true,
        "detail": "uniseg.wordbreak",
        "documentation": {}
    },
    {
        "label": "PunktSentenceTokenizer",
        "importPath": "nltk.tokenize.punkt",
        "description": "nltk.tokenize.punkt",
        "isExtraImport": true,
        "detail": "nltk.tokenize.punkt",
        "documentation": {}
    },
    {
        "label": "unidecode",
        "importPath": "unidecode",
        "description": "unidecode",
        "isExtraImport": true,
        "detail": "unidecode",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "justext",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "justext",
        "description": "justext",
        "detail": "justext",
        "documentation": {}
    },
    {
        "label": "ParserError",
        "importPath": "lxml.etree",
        "description": "lxml.etree",
        "isExtraImport": true,
        "detail": "lxml.etree",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "process_single_file",
        "importPath": "core",
        "description": "core",
        "isExtraImport": true,
        "detail": "core",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "builtins",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "builtins",
        "description": "builtins",
        "detail": "builtins",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "pytz",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytz",
        "description": "pytz",
        "detail": "pytz",
        "documentation": {}
    },
    {
        "label": "timezone",
        "importPath": "pytz",
        "description": "pytz",
        "isExtraImport": true,
        "detail": "pytz",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "get_aggregated_results",
        "importPath": "aggregated_metrics",
        "description": "aggregated_metrics",
        "isExtraImport": true,
        "detail": "aggregated_metrics",
        "documentation": {}
    },
    {
        "label": "update_args_from_openlm_config",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "generate_tokenized_dataset_json",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "get_source_ref",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "get_source_ref_by_key",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "InMemoryLogger",
        "importPath": "composer.loggers",
        "description": "composer.loggers",
        "isExtraImport": true,
        "detail": "composer.loggers",
        "documentation": {}
    },
    {
        "label": "LoggerDestination",
        "importPath": "composer.loggers",
        "description": "composer.loggers",
        "isExtraImport": true,
        "detail": "composer.loggers",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "composer.trainer",
        "description": "composer.trainer",
        "isExtraImport": true,
        "detail": "composer.trainer",
        "documentation": {}
    },
    {
        "label": "dist",
        "importPath": "composer.utils",
        "description": "composer.utils",
        "isExtraImport": true,
        "detail": "composer.utils",
        "documentation": {}
    },
    {
        "label": "get_device",
        "importPath": "composer.utils",
        "description": "composer.utils",
        "isExtraImport": true,
        "detail": "composer.utils",
        "documentation": {}
    },
    {
        "label": "reproducibility",
        "importPath": "composer.utils",
        "description": "composer.utils",
        "isExtraImport": true,
        "detail": "composer.utils",
        "documentation": {}
    },
    {
        "label": "build_icl_evaluators",
        "importPath": "llmfoundry.utils.builders",
        "description": "llmfoundry.utils.builders",
        "isExtraImport": true,
        "detail": "llmfoundry.utils.builders",
        "documentation": {}
    },
    {
        "label": "build_logger",
        "importPath": "llmfoundry.utils.builders",
        "description": "llmfoundry.utils.builders",
        "isExtraImport": true,
        "detail": "llmfoundry.utils.builders",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "ATTN_ACTIVATIONS",
        "importPath": "open_lm.attention",
        "description": "open_lm.attention",
        "isExtraImport": true,
        "detail": "open_lm.attention",
        "documentation": {}
    },
    {
        "label": "ATTN_SEQ_SCALARS",
        "importPath": "open_lm.attention",
        "description": "open_lm.attention",
        "isExtraImport": true,
        "detail": "open_lm.attention",
        "documentation": {}
    },
    {
        "label": "get_data",
        "importPath": "open_lm.data",
        "description": "open_lm.data",
        "isExtraImport": true,
        "detail": "open_lm.data",
        "documentation": {}
    },
    {
        "label": "broadcast_object",
        "importPath": "open_lm.distributed",
        "description": "open_lm.distributed",
        "isExtraImport": true,
        "detail": "open_lm.distributed",
        "documentation": {}
    },
    {
        "label": "init_distributed_device",
        "importPath": "open_lm.distributed",
        "description": "open_lm.distributed",
        "isExtraImport": true,
        "detail": "open_lm.distributed",
        "documentation": {}
    },
    {
        "label": "is_master",
        "importPath": "open_lm.distributed",
        "description": "open_lm.distributed",
        "isExtraImport": true,
        "detail": "open_lm.distributed",
        "documentation": {}
    },
    {
        "label": "world_info_from_env",
        "importPath": "open_lm.distributed",
        "description": "open_lm.distributed",
        "isExtraImport": true,
        "detail": "open_lm.distributed",
        "documentation": {}
    },
    {
        "label": "world_info_from_env",
        "importPath": "open_lm.distributed",
        "description": "open_lm.distributed",
        "isExtraImport": true,
        "detail": "open_lm.distributed",
        "documentation": {}
    },
    {
        "label": "world_info_from_env",
        "importPath": "open_lm.distributed",
        "description": "open_lm.distributed",
        "isExtraImport": true,
        "detail": "open_lm.distributed",
        "documentation": {}
    },
    {
        "label": "evaluate_loop",
        "importPath": "open_lm.evaluate",
        "description": "open_lm.evaluate",
        "isExtraImport": true,
        "detail": "open_lm.evaluate",
        "documentation": {}
    },
    {
        "label": "evaluate_loop",
        "importPath": "open_lm.evaluate",
        "description": "open_lm.evaluate",
        "isExtraImport": true,
        "detail": "open_lm.evaluate",
        "documentation": {}
    },
    {
        "label": "create_params",
        "importPath": "open_lm.model",
        "description": "open_lm.model",
        "isExtraImport": true,
        "detail": "open_lm.model",
        "documentation": {}
    },
    {
        "label": "create_params",
        "importPath": "open_lm.model",
        "description": "open_lm.model",
        "isExtraImport": true,
        "detail": "open_lm.model",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "open_lm.main",
        "description": "open_lm.main",
        "isExtraImport": true,
        "detail": "open_lm.main",
        "documentation": {}
    },
    {
        "label": "pt_load",
        "importPath": "open_lm.file_utils",
        "description": "open_lm.file_utils",
        "isExtraImport": true,
        "detail": "open_lm.file_utils",
        "documentation": {}
    },
    {
        "label": "SimpleComposerOpenLMCausalLM",
        "importPath": "open_lm.utils.llm_foundry_wrapper",
        "description": "open_lm.utils.llm_foundry_wrapper",
        "isExtraImport": true,
        "detail": "open_lm.utils.llm_foundry_wrapper",
        "documentation": {}
    },
    {
        "label": "OpenLMConfig",
        "importPath": "open_lm.utils.transformers.hf_config",
        "description": "open_lm.utils.transformers.hf_config",
        "isExtraImport": true,
        "detail": "open_lm.utils.transformers.hf_config",
        "documentation": {}
    },
    {
        "label": "OpenLMConfig",
        "importPath": "open_lm.utils.transformers.hf_config",
        "description": "open_lm.utils.transformers.hf_config",
        "isExtraImport": true,
        "detail": "open_lm.utils.transformers.hf_config",
        "documentation": {}
    },
    {
        "label": "OpenLMforCausalLM",
        "importPath": "open_lm.utils.transformers.hf_model",
        "description": "open_lm.utils.transformers.hf_model",
        "isExtraImport": true,
        "detail": "open_lm.utils.transformers.hf_model",
        "documentation": {}
    },
    {
        "label": "OpenLMforCausalLM",
        "importPath": "open_lm.utils.transformers.hf_model",
        "description": "open_lm.utils.transformers.hf_model",
        "isExtraImport": true,
        "detail": "open_lm.utils.transformers.hf_model",
        "documentation": {}
    },
    {
        "label": "OpenLMModel",
        "importPath": "open_lm.utils.transformers.hf_model",
        "description": "open_lm.utils.transformers.hf_model",
        "isExtraImport": true,
        "detail": "open_lm.utils.transformers.hf_model",
        "documentation": {}
    },
    {
        "label": "download_val_data",
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "isExtraImport": true,
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "get_downstream_task_name",
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "isExtraImport": true,
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "load_ppl_yaml",
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "isExtraImport": true,
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "download_val_data",
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "isExtraImport": true,
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "load_ppl_yaml",
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "isExtraImport": true,
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "tok_mult_paths",
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "isExtraImport": true,
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "natural_key",
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "isExtraImport": true,
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "setup_logger",
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "isExtraImport": true,
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "start_partial_model_process",
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "isExtraImport": true,
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "terminate_partial_model_process",
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "isExtraImport": true,
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "CaseInsensitiveDict",
        "importPath": "requests.structures",
        "description": "requests.structures",
        "isExtraImport": true,
        "detail": "requests.structures",
        "documentation": {}
    },
    {
        "label": "resource",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "resource",
        "description": "resource",
        "detail": "resource",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "psutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil",
        "description": "psutil",
        "detail": "psutil",
        "documentation": {}
    },
    {
        "label": "ray",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ray",
        "description": "ray",
        "detail": "ray",
        "documentation": {}
    },
    {
        "label": "memory_summary",
        "importPath": "ray._private.internal_api",
        "description": "ray._private.internal_api",
        "isExtraImport": true,
        "detail": "ray._private.internal_api",
        "documentation": {}
    },
    {
        "label": "memory_summary",
        "importPath": "ray._private.internal_api",
        "description": "ray._private.internal_api",
        "isExtraImport": true,
        "detail": "ray._private.internal_api",
        "documentation": {}
    },
    {
        "label": "_check_pyarrow_version",
        "importPath": "ray.data._internal.util",
        "description": "ray.data._internal.util",
        "isExtraImport": true,
        "detail": "ray.data._internal.util",
        "documentation": {}
    },
    {
        "label": "Block",
        "importPath": "ray.data.block",
        "description": "ray.data.block",
        "isExtraImport": true,
        "detail": "ray.data.block",
        "documentation": {}
    },
    {
        "label": "BlockMetadata",
        "importPath": "ray.data.block",
        "description": "ray.data.block",
        "isExtraImport": true,
        "detail": "ray.data.block",
        "documentation": {}
    },
    {
        "label": "DataContext",
        "importPath": "ray.data.context",
        "description": "ray.data.context",
        "isExtraImport": true,
        "detail": "ray.data.context",
        "documentation": {}
    },
    {
        "label": "DataContext",
        "importPath": "ray.data.context",
        "description": "ray.data.context",
        "isExtraImport": true,
        "detail": "ray.data.context",
        "documentation": {}
    },
    {
        "label": "Datasource",
        "importPath": "ray.data.datasource",
        "description": "ray.data.datasource",
        "isExtraImport": true,
        "detail": "ray.data.datasource",
        "documentation": {}
    },
    {
        "label": "ReadTask",
        "importPath": "ray.data.datasource",
        "description": "ray.data.datasource",
        "isExtraImport": true,
        "detail": "ray.data.datasource",
        "documentation": {}
    },
    {
        "label": "tarfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tarfile",
        "description": "tarfile",
        "detail": "tarfile",
        "documentation": {}
    },
    {
        "label": "simdjson",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "simdjson",
        "description": "simdjson",
        "detail": "simdjson",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "process_single_file",
        "importPath": "baselines.core",
        "description": "baselines.core",
        "isExtraImport": true,
        "detail": "baselines.core",
        "documentation": {}
    },
    {
        "label": "GLOBAL_FUNCTIONS",
        "importPath": "ray_processing",
        "description": "ray_processing",
        "isExtraImport": true,
        "detail": "ray_processing",
        "documentation": {}
    },
    {
        "label": "generate_untokenized_dataset_json",
        "importPath": "ray_processing.utils",
        "description": "ray_processing.utils",
        "isExtraImport": true,
        "detail": "ray_processing.utils",
        "documentation": {}
    },
    {
        "label": "get_source_ref",
        "importPath": "ray_processing.utils",
        "description": "ray_processing.utils",
        "isExtraImport": true,
        "detail": "ray_processing.utils",
        "documentation": {}
    },
    {
        "label": "get_source_ref_by_key",
        "importPath": "ray_processing.utils",
        "description": "ray_processing.utils",
        "isExtraImport": true,
        "detail": "ray_processing.utils",
        "documentation": {}
    },
    {
        "label": "replace_prefix",
        "importPath": "training.dataset_reference",
        "description": "training.dataset_reference",
        "isExtraImport": true,
        "detail": "training.dataset_reference",
        "documentation": {}
    },
    {
        "label": "DatasetReference",
        "importPath": "training.dataset_reference",
        "description": "training.dataset_reference",
        "isExtraImport": true,
        "detail": "training.dataset_reference",
        "documentation": {}
    },
    {
        "label": "tokenize_shuffle",
        "importPath": "open_lm.datapreprocess.ray",
        "description": "open_lm.datapreprocess.ray",
        "isExtraImport": true,
        "detail": "open_lm.datapreprocess.ray",
        "documentation": {}
    },
    {
        "label": "git",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "git",
        "description": "git",
        "detail": "git",
        "documentation": {}
    },
    {
        "label": "pytest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytest",
        "description": "pytest",
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "mock_s3",
        "importPath": "moto",
        "description": "moto",
        "isExtraImport": true,
        "detail": "moto",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "baselines.mappers.enrichers.enrichers",
        "description": "baselines.mappers.enrichers.enrichers",
        "isExtraImport": true,
        "detail": "baselines.mappers.enrichers.enrichers",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "isExtraImport": true,
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "\\",
        "importPath": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "description": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "isExtraImport": true,
        "detail": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "description": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "isExtraImport": true,
        "detail": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "baselines.mappers.filters",
        "description": "baselines.mappers.filters",
        "isExtraImport": true,
        "detail": "baselines.mappers.filters",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "baselines.mappers.filters",
        "description": "baselines.mappers.filters",
        "isExtraImport": true,
        "detail": "baselines.mappers.filters",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "baselines.mappers.filters",
        "description": "baselines.mappers.filters",
        "isExtraImport": true,
        "detail": "baselines.mappers.filters",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "isExtraImport": true,
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "baselines.aggregators",
        "description": "baselines.aggregators",
        "isExtraImport": true,
        "detail": "baselines.aggregators",
        "documentation": {}
    },
    {
        "label": "process_single_file",
        "importPath": "baselines.core.processor",
        "description": "baselines.core.processor",
        "isExtraImport": true,
        "detail": "baselines.core.processor",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "baselines.mappers.splitters",
        "description": "baselines.mappers.splitters",
        "isExtraImport": true,
        "detail": "baselines.mappers.splitters",
        "documentation": {}
    },
    {
        "label": "FileStream",
        "importPath": "fastwarc.stream_io",
        "description": "fastwarc.stream_io",
        "isExtraImport": true,
        "detail": "fastwarc.stream_io",
        "documentation": {}
    },
    {
        "label": "GZipStream",
        "importPath": "fastwarc.stream_io",
        "description": "fastwarc.stream_io",
        "isExtraImport": true,
        "detail": "fastwarc.stream_io",
        "documentation": {}
    },
    {
        "label": "FileStream",
        "importPath": "fastwarc.stream_io",
        "description": "fastwarc.stream_io",
        "isExtraImport": true,
        "detail": "fastwarc.stream_io",
        "documentation": {}
    },
    {
        "label": "GZipStream",
        "importPath": "fastwarc.stream_io",
        "description": "fastwarc.stream_io",
        "isExtraImport": true,
        "detail": "fastwarc.stream_io",
        "documentation": {}
    },
    {
        "label": "ArchiveIterator",
        "importPath": "fastwarc.warc",
        "description": "fastwarc.warc",
        "isExtraImport": true,
        "detail": "fastwarc.warc",
        "documentation": {}
    },
    {
        "label": "WarcRecordType",
        "importPath": "fastwarc.warc",
        "description": "fastwarc.warc",
        "isExtraImport": true,
        "detail": "fastwarc.warc",
        "documentation": {}
    },
    {
        "label": "ArchiveIterator",
        "importPath": "fastwarc.warc",
        "description": "fastwarc.warc",
        "isExtraImport": true,
        "detail": "fastwarc.warc",
        "documentation": {}
    },
    {
        "label": "WarcRecordType",
        "importPath": "fastwarc.warc",
        "description": "fastwarc.warc",
        "isExtraImport": true,
        "detail": "fastwarc.warc",
        "documentation": {}
    },
    {
        "label": "botocore",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "botocore",
        "description": "botocore",
        "detail": "botocore",
        "documentation": {}
    },
    {
        "label": "trafilatura",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "trafilatura",
        "description": "trafilatura",
        "detail": "trafilatura",
        "documentation": {}
    },
    {
        "label": "extract_plain_text",
        "importPath": "resiliparse.extract.html2text",
        "description": "resiliparse.extract.html2text",
        "isExtraImport": true,
        "detail": "resiliparse.extract.html2text",
        "documentation": {}
    },
    {
        "label": "fsspec",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fsspec",
        "description": "fsspec",
        "detail": "fsspec",
        "documentation": {}
    },
    {
        "label": "APPROX_TOKENS_PER_WARC",
        "importPath": "gen_common_crawl_paths",
        "description": "gen_common_crawl_paths",
        "isExtraImport": true,
        "detail": "gen_common_crawl_paths",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "click",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "click",
        "description": "click",
        "detail": "click",
        "documentation": {}
    },
    {
        "label": "NoCredentialsError",
        "importPath": "botocore.exceptions",
        "description": "botocore.exceptions",
        "isExtraImport": true,
        "detail": "botocore.exceptions",
        "documentation": {}
    },
    {
        "label": "NoCredentialsError",
        "importPath": "botocore.exceptions",
        "description": "botocore.exceptions",
        "isExtraImport": true,
        "detail": "botocore.exceptions",
        "documentation": {}
    },
    {
        "label": "build_table_dfs",
        "importPath": "tools.expdb",
        "description": "tools.expdb",
        "isExtraImport": true,
        "detail": "tools.expdb",
        "documentation": {}
    },
    {
        "label": "filter_df",
        "importPath": "tools.expdb",
        "description": "tools.expdb",
        "isExtraImport": true,
        "detail": "tools.expdb",
        "documentation": {}
    },
    {
        "label": "merge_uuid_references",
        "importPath": "tools.expdb",
        "description": "tools.expdb",
        "isExtraImport": true,
        "detail": "tools.expdb",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "loguru",
        "description": "loguru",
        "isExtraImport": true,
        "detail": "loguru",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "build_table_dfs",
        "importPath": "expdb",
        "description": "expdb",
        "isExtraImport": true,
        "detail": "expdb",
        "documentation": {}
    },
    {
        "label": "filter_df",
        "importPath": "expdb",
        "description": "expdb",
        "isExtraImport": true,
        "detail": "expdb",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "matplotlib.ticker",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.ticker",
        "description": "matplotlib.ticker",
        "detail": "matplotlib.ticker",
        "documentation": {}
    },
    {
        "label": "update_args_from_openlm_config",
        "importPath": "eval.utils",
        "description": "eval.utils",
        "isExtraImport": true,
        "detail": "eval.utils",
        "documentation": {}
    },
    {
        "label": "queue",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "queue",
        "description": "queue",
        "detail": "queue",
        "documentation": {}
    },
    {
        "label": "Queue",
        "importPath": "queue",
        "description": "queue",
        "isExtraImport": true,
        "detail": "queue",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "HfApi",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "HfFolder",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "CommitOperationAdd",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "preupload_lfs_files",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "create_commit",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "urllib.request",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib.request",
        "description": "urllib.request",
        "detail": "urllib.request",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "importlib.metadata",
        "description": "importlib.metadata",
        "isExtraImport": true,
        "detail": "importlib.metadata",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "importlib.metadata",
        "description": "importlib.metadata",
        "isExtraImport": true,
        "detail": "importlib.metadata",
        "documentation": {}
    },
    {
        "label": "ModelReference",
        "importPath": "training.model_reference",
        "description": "training.model_reference",
        "isExtraImport": true,
        "detail": "training.model_reference",
        "documentation": {}
    },
    {
        "label": "ModelReference",
        "importPath": "training.model_reference",
        "description": "training.model_reference",
        "isExtraImport": true,
        "detail": "training.model_reference",
        "documentation": {}
    },
    {
        "label": "Hyperparameters",
        "importPath": "training.hyperparameters",
        "description": "training.hyperparameters",
        "isExtraImport": true,
        "detail": "training.hyperparameters",
        "documentation": {}
    },
    {
        "label": "available_scales",
        "importPath": "training.hyperparameters",
        "description": "training.hyperparameters",
        "isExtraImport": true,
        "detail": "training.hyperparameters",
        "documentation": {}
    },
    {
        "label": "Hyperparameters",
        "importPath": "training.hyperparameters",
        "description": "training.hyperparameters",
        "isExtraImport": true,
        "detail": "training.hyperparameters",
        "documentation": {}
    },
    {
        "label": "get_scale_config",
        "importPath": "training.hyperparameters",
        "description": "training.hyperparameters",
        "isExtraImport": true,
        "detail": "training.hyperparameters",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "get_open_lm_args",
        "importPath": "training.params",
        "description": "training.params",
        "isExtraImport": true,
        "detail": "training.params",
        "documentation": {}
    },
    {
        "label": "parse_dcnlp_args",
        "importPath": "training.params",
        "description": "training.params",
        "isExtraImport": true,
        "detail": "training.params",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "install",
        "importPath": "setuptools.command.install",
        "description": "setuptools.command.install",
        "isExtraImport": true,
        "detail": "setuptools.command.install",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "CONTENT",
        "kind": 5,
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "peekOfCode": "CONTENT = \"text\"\nURL = \"url\"\nCHUNK = \"chunk\"\n# Stats file keys\nPROCESS_SETUP_KEY_NAME = 'process_setup'\nPROCESS_END_KEY_NAME = 'process_finished'\nCOMMIT_KEY_NAME = 'commit'\nGLOBAL_FUNCTIONS = {\n\t'exact_dedup': None,\n}",
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "URL",
        "kind": 5,
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "peekOfCode": "URL = \"url\"\nCHUNK = \"chunk\"\n# Stats file keys\nPROCESS_SETUP_KEY_NAME = 'process_setup'\nPROCESS_END_KEY_NAME = 'process_finished'\nCOMMIT_KEY_NAME = 'commit'\nGLOBAL_FUNCTIONS = {\n\t'exact_dedup': None,\n}",
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "CHUNK",
        "kind": 5,
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "peekOfCode": "CHUNK = \"chunk\"\n# Stats file keys\nPROCESS_SETUP_KEY_NAME = 'process_setup'\nPROCESS_END_KEY_NAME = 'process_finished'\nCOMMIT_KEY_NAME = 'commit'\nGLOBAL_FUNCTIONS = {\n\t'exact_dedup': None,\n}",
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "PROCESS_SETUP_KEY_NAME",
        "kind": 5,
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "peekOfCode": "PROCESS_SETUP_KEY_NAME = 'process_setup'\nPROCESS_END_KEY_NAME = 'process_finished'\nCOMMIT_KEY_NAME = 'commit'\nGLOBAL_FUNCTIONS = {\n\t'exact_dedup': None,\n}",
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "PROCESS_END_KEY_NAME",
        "kind": 5,
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "peekOfCode": "PROCESS_END_KEY_NAME = 'process_finished'\nCOMMIT_KEY_NAME = 'commit'\nGLOBAL_FUNCTIONS = {\n\t'exact_dedup': None,\n}",
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "COMMIT_KEY_NAME",
        "kind": 5,
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "peekOfCode": "COMMIT_KEY_NAME = 'commit'\nGLOBAL_FUNCTIONS = {\n\t'exact_dedup': None,\n}",
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "GLOBAL_FUNCTIONS",
        "kind": 5,
        "importPath": "baselines.core.constants",
        "description": "baselines.core.constants",
        "peekOfCode": "GLOBAL_FUNCTIONS = {\n\t'exact_dedup': None,\n}",
        "detail": "baselines.core.constants",
        "documentation": {}
    },
    {
        "label": "ProfilingData",
        "kind": 6,
        "importPath": "baselines.core.factories",
        "description": "baselines.core.factories",
        "peekOfCode": "class ProfilingData:\n    execution_time: float\n_BASELINES_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.append(_BASELINES_PATH)\ndef _normalize_path_from_root(pth):\n    if os.path.isabs(pth):\n        raise ValueError(f\"Absolute are not supported\")\n    return os.path.join(_BASELINES_PATH, pth)\ndef _fix_builtin_path_to_relative(file_path):\n    relative_path = os.path.relpath(file_path, _BASELINES_PATH)  # Get the relative path",
        "detail": "baselines.core.factories",
        "documentation": {}
    },
    {
        "label": "get_mapper",
        "kind": 2,
        "importPath": "baselines.core.factories",
        "description": "baselines.core.factories",
        "peekOfCode": "def get_mapper(func, _profile=False, _safe=False, **kwargs):\n    \"\"\"\n    Factory function for creating a partial function with the given arguments.\n    :param func: The function to be called.\n    :param _profile: If True, the partial function will be wrapped to return a 2-tuple with the results,\n        and the profiling (Pinfo instead of just the results.\n    :param _safe: If True, the partial function will be wrapped to return return a raised error if one is thrown, rather than raise it\n    :param kwargs: The arguments to be passed to the function.\n    :return: A partial function with the given arguments.\n    \"\"\"",
        "detail": "baselines.core.factories",
        "documentation": {}
    },
    {
        "label": "get_aggregator",
        "kind": 2,
        "importPath": "baselines.core.factories",
        "description": "baselines.core.factories",
        "peekOfCode": "def get_aggregator(func, **kwargs):\n    \"\"\"\n    Factory function for creating a partial function with the given arguments.\n    :param func: The function to be called.\n    :param kwargs: The arguments to be passed to the function.\n    :return: A partial function with the given arguments.\n    \"\"\"\n    return _load_function(func, _AGGREGATORS_MODULES, **kwargs)\ndef get_transform(func, **kwargs):\n    \"\"\"",
        "detail": "baselines.core.factories",
        "documentation": {}
    },
    {
        "label": "get_transform",
        "kind": 2,
        "importPath": "baselines.core.factories",
        "description": "baselines.core.factories",
        "peekOfCode": "def get_transform(func, **kwargs):\n    \"\"\"\n    Factory function for creating a partial function with the given arguments, for a transform of a value to\n    be sent to aggregation.\n    :param func: The function to be called.\n    :param kwargs: The arguments to be passed to the function.\n    :return: A partial function with the given arguments.\n    \"\"\"\n    return _load_function(func, _TRANSFORMS_MODULES, **kwargs)",
        "detail": "baselines.core.factories",
        "documentation": {}
    },
    {
        "label": "_BASELINES_PATH",
        "kind": 5,
        "importPath": "baselines.core.factories",
        "description": "baselines.core.factories",
        "peekOfCode": "_BASELINES_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.append(_BASELINES_PATH)\ndef _normalize_path_from_root(pth):\n    if os.path.isabs(pth):\n        raise ValueError(f\"Absolute are not supported\")\n    return os.path.join(_BASELINES_PATH, pth)\ndef _fix_builtin_path_to_relative(file_path):\n    relative_path = os.path.relpath(file_path, _BASELINES_PATH)  # Get the relative path\n    module_path = os.path.splitext(relative_path)[0]  # Remove .py extension\n    module_path = module_path.replace('..', '.').replace(os.sep, '.')  # Replace path separator with a dot",
        "detail": "baselines.core.factories",
        "documentation": {}
    },
    {
        "label": "_MAPPERS_MODULES",
        "kind": 5,
        "importPath": "baselines.core.factories",
        "description": "baselines.core.factories",
        "peekOfCode": "_MAPPERS_MODULES = _get_package_modules(_normalize_path_from_root('mappers'))  # assumes working dir is root dir!\n_AGGREGATORS_MODULES = [_fix_builtin_path_to_relative(_normalize_path_from_root('aggregators'))]\n# currently the transforms used for aggreagtors sit within the same module\n_TRANSFORMS_MODULES = [_fix_builtin_path_to_relative(_normalize_path_from_root('aggregators'))]\ndef _import_module_from_path(path):\n    try:\n        spec = importlib.util.spec_from_file_location(\"module_name\", path)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n    except Exception as e:",
        "detail": "baselines.core.factories",
        "documentation": {}
    },
    {
        "label": "_AGGREGATORS_MODULES",
        "kind": 5,
        "importPath": "baselines.core.factories",
        "description": "baselines.core.factories",
        "peekOfCode": "_AGGREGATORS_MODULES = [_fix_builtin_path_to_relative(_normalize_path_from_root('aggregators'))]\n# currently the transforms used for aggreagtors sit within the same module\n_TRANSFORMS_MODULES = [_fix_builtin_path_to_relative(_normalize_path_from_root('aggregators'))]\ndef _import_module_from_path(path):\n    try:\n        spec = importlib.util.spec_from_file_location(\"module_name\", path)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n    except Exception as e:\n        raise FileNotFoundError(f\"Failed to import module {path}. Make sure the given custom function is in the \"",
        "detail": "baselines.core.factories",
        "documentation": {}
    },
    {
        "label": "_TRANSFORMS_MODULES",
        "kind": 5,
        "importPath": "baselines.core.factories",
        "description": "baselines.core.factories",
        "peekOfCode": "_TRANSFORMS_MODULES = [_fix_builtin_path_to_relative(_normalize_path_from_root('aggregators'))]\ndef _import_module_from_path(path):\n    try:\n        spec = importlib.util.spec_from_file_location(\"module_name\", path)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n    except Exception as e:\n        raise FileNotFoundError(f\"Failed to import module {path}. Make sure the given custom function is in the \"\n                                f\"correct path, given as a relative to the working directory ({os.getcwd()})\")\n    return module",
        "detail": "baselines.core.factories",
        "documentation": {}
    },
    {
        "label": "is_factory",
        "kind": 2,
        "importPath": "baselines.core.factory_utils",
        "description": "baselines.core.factory_utils",
        "peekOfCode": "def is_factory(func):\n    return hasattr(func, '_is_factory')\ndef factory_function(func):\n    def wrapper(*args, **kwargs):\n        return func(*args, **kwargs)\n    wrapper._is_factory = True\n    return wrapper\ndef initialize_mapper(func, **kwargs):\n    if is_factory(func):\n        return func(**kwargs)",
        "detail": "baselines.core.factory_utils",
        "documentation": {}
    },
    {
        "label": "factory_function",
        "kind": 2,
        "importPath": "baselines.core.factory_utils",
        "description": "baselines.core.factory_utils",
        "peekOfCode": "def factory_function(func):\n    def wrapper(*args, **kwargs):\n        return func(*args, **kwargs)\n    wrapper._is_factory = True\n    return wrapper\ndef initialize_mapper(func, **kwargs):\n    if is_factory(func):\n        return func(**kwargs)\n    return func",
        "detail": "baselines.core.factory_utils",
        "documentation": {}
    },
    {
        "label": "initialize_mapper",
        "kind": 2,
        "importPath": "baselines.core.factory_utils",
        "description": "baselines.core.factory_utils",
        "peekOfCode": "def initialize_mapper(func, **kwargs):\n    if is_factory(func):\n        return func(**kwargs)\n    return func",
        "detail": "baselines.core.factory_utils",
        "documentation": {}
    },
    {
        "label": "is_s3",
        "kind": 2,
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "peekOfCode": "def is_s3(file_path: str):\n    return file_path.startswith(\"s3://\")\ndef is_compressed(file_path: str):\n    return any(file_path.endswith(z) for z in (\".zst\", \".zstd\", \".gz\"))\ndef delete_file(file_path: str):\n    \"\"\"Deletes the file at the given path (local or S3). If the file does not exist, raises an error.\n    May also raise if this is a directory rather than a file\"\"\"\n    if is_s3(file_path):\n        s3_path = S3Path(file_path)\n        if s3_path.exists():",
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "is_compressed",
        "kind": 2,
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "peekOfCode": "def is_compressed(file_path: str):\n    return any(file_path.endswith(z) for z in (\".zst\", \".zstd\", \".gz\"))\ndef delete_file(file_path: str):\n    \"\"\"Deletes the file at the given path (local or S3). If the file does not exist, raises an error.\n    May also raise if this is a directory rather than a file\"\"\"\n    if is_s3(file_path):\n        s3_path = S3Path(file_path)\n        if s3_path.exists():\n            s3_path.unlink()  # This deletes the file\n        else:",
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "delete_file",
        "kind": 2,
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "peekOfCode": "def delete_file(file_path: str):\n    \"\"\"Deletes the file at the given path (local or S3). If the file does not exist, raises an error.\n    May also raise if this is a directory rather than a file\"\"\"\n    if is_s3(file_path):\n        s3_path = S3Path(file_path)\n        if s3_path.exists():\n            s3_path.unlink()  # This deletes the file\n        else:\n            raise FileNotFoundError(f\"{file_path} does not exist.\")\n    else:",
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "is_exists",
        "kind": 2,
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "peekOfCode": "def is_exists(file_path: str):\n    \"\"\"Checks if the file at the given path (local or S3) exists\"\"\"\n    if is_s3(file_path):\n        s3_path = S3Path(file_path)\n        return s3_path.exists() and s3_path.is_file()\n    else:\n        return os.path.isfile(file_path)\ndef _jsonl_bytes_reader(fh: BinaryIO):\n    with io.TextIOWrapper(fh, encoding=\"utf-8\") as text_reader:\n        with jsonlines.Reader(text_reader) as jsonl_reader:",
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "read_jsonl",
        "kind": 2,
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "peekOfCode": "def read_jsonl(file_path: str):\n    \"\"\"Read a JSONL file from a given path (local or S3).\"\"\"\n    if is_s3(file_path):\n        path = S3Path(file_path)\n    else:\n        path = LocalPath(file_path)\n    if any(file_path.endswith(z) for z in (\".zst\", \".zstd\")):\n        with path.open('rb') as f:\n            with zstd.ZstdDecompressor().stream_reader(f) as reader:\n                for line in _jsonl_bytes_reader(reader):",
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "write_jsonl",
        "kind": 2,
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "peekOfCode": "def write_jsonl(data, file_path: str, mode: str = \"w\"):\n    \"\"\"Write data to a JSONL file at a given path (local or S3).\"\"\"\n    if is_s3(file_path):\n        path = S3Path(file_path)\n    else:\n        path = LocalPath(file_path)\n    if is_compressed(file_path):\n        data = [json.dumps(d) for d in data]\n        data = \"\\n\".join(data).encode('utf8')\n    if any(file_path.endswith(z) for z in (\".zst\", \".zstd\")):",
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "makedirs_if_missing",
        "kind": 2,
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "peekOfCode": "def makedirs_if_missing(dir_path: str):\n    \"\"\"\n    Create directories for the provided path if they do not exist.\n    For S3 paths, this function is a no-op because S3 does not have a real notion of directories.\n    Parameters:\n    - dir_path (str): The directory path. Can be a local filesystem path or an S3 URI.\n    Returns:\n    - None\n    \"\"\"\n    if is_s3(dir_path):",
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "list_dir",
        "kind": 2,
        "importPath": "baselines.core.file_utils",
        "description": "baselines.core.file_utils",
        "peekOfCode": "def list_dir(dirname) -> List[str]:\n    \"\"\"List the contents of a directory, excluding hidden files and directories. always as full abs path,\n    alawys as list of strings\n    \"\"\"\n    if is_s3(dirname):\n        s3_directory = S3Path(dirname)\n        return [str(f) for f in s3_directory.iterdir() if not f.name.startswith(\".\")]  # exclude hidden files\n    else:\n        return [os.path.abspath(os.path.join(dirname, f)) for f in os.listdir(dirname) if not f.startswith(\".\")]",
        "detail": "baselines.core.file_utils",
        "documentation": {}
    },
    {
        "label": "commit",
        "kind": 2,
        "importPath": "baselines.core.processor",
        "description": "baselines.core.processor",
        "peekOfCode": "def commit(pages, stats, output_path, stats_output_path):\n    logger.info(f\"committing pages to {output_path} (and stats to {stats_output_path})\")\n    t = time.time()\n    write_jsonl(pages, output_path)\n    stats.append({'name': COMMIT_KEY_NAME, 'secs': time.time() - t})\n    write_jsonl(stats, stats_output_path, 'a')\n    stats.clear()\n    logger.info(f\"commit took {time.time() - t} seconds\")\ndef _get_output_paths(base_output_path, jsonl_relpath):\n    # TODO - need to allow overwrite?",
        "detail": "baselines.core.processor",
        "documentation": {}
    },
    {
        "label": "process_single_file",
        "kind": 2,
        "importPath": "baselines.core.processor",
        "description": "baselines.core.processor",
        "peekOfCode": "def process_single_file(config_data: Dict[str, Any], raw_data_dirpath: str, jsonl_relpath: str, source_name: str, \n                        base_output_path: str, workers: int = 1, overwrite: bool = False) -> Tuple[str, str]:\n    \"\"\"\n    :param config_data: A processed config (from yaml) that specifies the steps to be taken\n    :param raw_data_dirpath: The path to the top data directory in the data hierarchy (from which to mirror\n                                the output path)\n    :param jsonl_relpath: path to the input jsonl file with the text to be processed, relative to raw_data_dirpath\n    :param source_name: The name of the source of the jsonl file\n    :param base_output_path: path to the output directory where to save the resulting jsonl file with the\n                        processed text as well as the stats",
        "detail": "baselines.core.processor",
        "documentation": {}
    },
    {
        "label": "apply_partial_func_sequential",
        "kind": 2,
        "importPath": "baselines.core.processor",
        "description": "baselines.core.processor",
        "peekOfCode": "def apply_partial_func_sequential(counters, execution_times, new_pages, pages, partial_func):\n    _parse_func_results(map(partial_func, pages), counters, execution_times, new_pages)\ndef _worker(worker_num, step, pages, input_queue, output_list):\n    logger.info(f\"Worker {worker_num} has started processing jobs.\")\n    # Create the partial function for each item in the YAML\n    # Assumption - if the function has some intiialization to do, it was done as part of a cluster creation and the overhead will be amortized across pages\n    partial_func = get_mapper(**step, _profile=True, _safe=True)\n    while True:\n        try:\n            # Get a job from the input queue.",
        "detail": "baselines.core.processor",
        "documentation": {}
    },
    {
        "label": "apply_partial_func_parallel",
        "kind": 2,
        "importPath": "baselines.core.processor",
        "description": "baselines.core.processor",
        "peekOfCode": "def apply_partial_func_parallel(counters, execution_times, new_pages, pages, step, n_workers):\n    # Create the input queue.\n    input_queue = multiprocessing.Queue()\n    # Create the shared list.\n    manager = multiprocessing.Manager()\n    shared_list = manager.list()\n    # Create the workers.\n    workers = [multiprocessing.Process(target=_worker, args=(i, step, pages, input_queue, shared_list)) for i in\n               range(n_workers)]\n    # Start the workers.",
        "detail": "baselines.core.processor",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "baselines.core.processor",
        "description": "baselines.core.processor",
        "peekOfCode": "logger = logging.getLogger(__name__)\n# Set the logging level\nlogger.setLevel(logging.DEBUG)  # For example, set level to DEBUG\n# Create a StreamHandler for stdout\nstdout_handler = logging.StreamHandler()\nstdout_handler.setLevel(logging.INFO)  # Optionally set a different level for stdout\n# Create a formatter and set it for the handler\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nstdout_handler.setFormatter(formatter)\n# Add the stdout handler to the logger",
        "detail": "baselines.core.processor",
        "documentation": {}
    },
    {
        "label": "stdout_handler",
        "kind": 5,
        "importPath": "baselines.core.processor",
        "description": "baselines.core.processor",
        "peekOfCode": "stdout_handler = logging.StreamHandler()\nstdout_handler.setLevel(logging.INFO)  # Optionally set a different level for stdout\n# Create a formatter and set it for the handler\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nstdout_handler.setFormatter(formatter)\n# Add the stdout handler to the logger\nlogger.addHandler(stdout_handler)\nCOMMIT_COMMAND = 'commit'\n# Indices in the mapper counters (what happened to each processed page)\nERRORS_INDEX = -1",
        "detail": "baselines.core.processor",
        "documentation": {}
    },
    {
        "label": "formatter",
        "kind": 5,
        "importPath": "baselines.core.processor",
        "description": "baselines.core.processor",
        "peekOfCode": "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nstdout_handler.setFormatter(formatter)\n# Add the stdout handler to the logger\nlogger.addHandler(stdout_handler)\nCOMMIT_COMMAND = 'commit'\n# Indices in the mapper counters (what happened to each processed page)\nERRORS_INDEX = -1\nREMOVED_INDEX = 0\nKEPT_INDEX = 1\nSPLIT_INDEX = 2",
        "detail": "baselines.core.processor",
        "documentation": {}
    },
    {
        "label": "COMMIT_COMMAND",
        "kind": 5,
        "importPath": "baselines.core.processor",
        "description": "baselines.core.processor",
        "peekOfCode": "COMMIT_COMMAND = 'commit'\n# Indices in the mapper counters (what happened to each processed page)\nERRORS_INDEX = -1\nREMOVED_INDEX = 0\nKEPT_INDEX = 1\nSPLIT_INDEX = 2\ndef commit(pages, stats, output_path, stats_output_path):\n    logger.info(f\"committing pages to {output_path} (and stats to {stats_output_path})\")\n    t = time.time()\n    write_jsonl(pages, output_path)",
        "detail": "baselines.core.processor",
        "documentation": {}
    },
    {
        "label": "ERRORS_INDEX",
        "kind": 5,
        "importPath": "baselines.core.processor",
        "description": "baselines.core.processor",
        "peekOfCode": "ERRORS_INDEX = -1\nREMOVED_INDEX = 0\nKEPT_INDEX = 1\nSPLIT_INDEX = 2\ndef commit(pages, stats, output_path, stats_output_path):\n    logger.info(f\"committing pages to {output_path} (and stats to {stats_output_path})\")\n    t = time.time()\n    write_jsonl(pages, output_path)\n    stats.append({'name': COMMIT_KEY_NAME, 'secs': time.time() - t})\n    write_jsonl(stats, stats_output_path, 'a')",
        "detail": "baselines.core.processor",
        "documentation": {}
    },
    {
        "label": "REMOVED_INDEX",
        "kind": 5,
        "importPath": "baselines.core.processor",
        "description": "baselines.core.processor",
        "peekOfCode": "REMOVED_INDEX = 0\nKEPT_INDEX = 1\nSPLIT_INDEX = 2\ndef commit(pages, stats, output_path, stats_output_path):\n    logger.info(f\"committing pages to {output_path} (and stats to {stats_output_path})\")\n    t = time.time()\n    write_jsonl(pages, output_path)\n    stats.append({'name': COMMIT_KEY_NAME, 'secs': time.time() - t})\n    write_jsonl(stats, stats_output_path, 'a')\n    stats.clear()",
        "detail": "baselines.core.processor",
        "documentation": {}
    },
    {
        "label": "KEPT_INDEX",
        "kind": 5,
        "importPath": "baselines.core.processor",
        "description": "baselines.core.processor",
        "peekOfCode": "KEPT_INDEX = 1\nSPLIT_INDEX = 2\ndef commit(pages, stats, output_path, stats_output_path):\n    logger.info(f\"committing pages to {output_path} (and stats to {stats_output_path})\")\n    t = time.time()\n    write_jsonl(pages, output_path)\n    stats.append({'name': COMMIT_KEY_NAME, 'secs': time.time() - t})\n    write_jsonl(stats, stats_output_path, 'a')\n    stats.clear()\n    logger.info(f\"commit took {time.time() - t} seconds\")",
        "detail": "baselines.core.processor",
        "documentation": {}
    },
    {
        "label": "SPLIT_INDEX",
        "kind": 5,
        "importPath": "baselines.core.processor",
        "description": "baselines.core.processor",
        "peekOfCode": "SPLIT_INDEX = 2\ndef commit(pages, stats, output_path, stats_output_path):\n    logger.info(f\"committing pages to {output_path} (and stats to {stats_output_path})\")\n    t = time.time()\n    write_jsonl(pages, output_path)\n    stats.append({'name': COMMIT_KEY_NAME, 'secs': time.time() - t})\n    write_jsonl(stats, stats_output_path, 'a')\n    stats.clear()\n    logger.info(f\"commit took {time.time() - t} seconds\")\ndef _get_output_paths(base_output_path, jsonl_relpath):",
        "detail": "baselines.core.processor",
        "documentation": {}
    },
    {
        "label": "line_counter",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.enrichers",
        "description": "baselines.mappers.enrichers.enrichers",
        "peekOfCode": "def line_counter(text: str, paragraph_end: str = '\\n', remove_empty: bool = True) -> int:\n    '''\n    A wrapper over split_paragraphs, returning the number of line/paragraphs in a page.\n    @param text: The text to split.\n    @param paragraph_end: The sequence of character that marks a line/paragraph end. default: '\\n' is used for lines, but can be '\\n\\n' for paragraphs.\n    @param remove_empty: Whether to remove empty lines/paragraphs.\n    Returns: The number of lines/paragraphs in the text.\n    >>> line_counter('Hello\\n\\nWorld and all\\nand beyond')\n    3\n    '''",
        "detail": "baselines.mappers.enrichers.enrichers",
        "documentation": {}
    },
    {
        "label": "line_counter_enricher",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.enrichers",
        "description": "baselines.mappers.enrichers.enrichers",
        "peekOfCode": "def line_counter_enricher(page: Dict, paragraph_end: str = '\\n', remove_empty: bool = True,\n                          key: str = 'num_lines', overwrite: bool = False) -> List[Dict]:\n    '''\n    Enriches a page with the number of line/paragraphs in the text.\n    @param page: The page to enrich.\n    @param paragraph_end: The sequence of character that marks a line/paragraph end. default: '\\n' is used for lines, but can be '\\n\\n' for paragraphs.\n    @param remove_empty: Whether to remove empty lines/paragraphs.\n    @param key: The key to use for the enrichment.\n    @param overwrite: Whether to overwrite an existing key.\n    Returns: The enriched page, with an additional key 'num_sentences' containing the number of sentences in the text.",
        "detail": "baselines.mappers.enrichers.enrichers",
        "documentation": {}
    },
    {
        "label": "word_counter_enricher",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.enrichers",
        "description": "baselines.mappers.enrichers.enrichers",
        "peekOfCode": "def word_counter_enricher(page: Dict, key: str = 'word_count', overwrite: bool = False, ignore_punctuation=True, **kwargs) -> List[Dict]:\n    '''\n    Enriches a page with the number of words in the text.\n    @param page: The page to enrich.\n    @param key: The key to use for the enrichment.\n    @param overwrite: Whether to overwrite an existing key.\n    @kwargs: Extra arguments passed into split_words such as model and ignore_whitespace. The default for ignore_punctuation is set to be True for more\n    accurate word counts.  \n    Returns: The enriched page, with an additional key 'num_sentences' containing the number of sentences in the text.\n    '''",
        "detail": "baselines.mappers.enrichers.enrichers",
        "documentation": {}
    },
    {
        "label": "load_fasttext_model",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "def load_fasttext_model():\n    if os.path.exists(MODEL_SUBDIRECTORY):\n        model_path = os.path.join(MODEL_SUBDIRECTORY, FASTTEXT_MODEL_FILENAME)\n    else:\n        model_path = os.path.join(MODEL_DIRECTORY, FASTTEXT_MODEL_FILENAME)\n    return fasttext.load_model(model_path)\ndef get_fasttext_lang_prob(model: fasttext.FastText._FastText, text: str) -> (str, float):\n    '''\n    Function to detect the language of a given text using FastText model.\n    Parameters:",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "get_fasttext_lang_prob",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "def get_fasttext_lang_prob(model: fasttext.FastText._FastText, text: str) -> (str, float):\n    '''\n    Function to detect the language of a given text using FastText model.\n    Parameters:\n        model (fasttext.FastText._FastText): The FastText model to use for language detection.\n        text (str): The text whose language is to be detected.\n    Returns:\n        str: The detected language.\n        prob: The probability of the detected language.\n    '''",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "get_langdetect_lang_prob",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "def get_langdetect_lang_prob(text: str) -> (str, float):\n    detected_lang = detect_langdetect(text)\n    return {x.lang: x.prob for x in detected_lang}\ndef is_space_or_punct(s: str) -> bool:\n    '''\n    Check if a string is empty, or contains only spaces or punctuation.\n    Parameters:\n        s (str): The string to check.\n    Returns:\n        bool: True if the string is empty, or contains only spaces or punctuation, otherwise False.",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "is_space_or_punct",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "def is_space_or_punct(s: str) -> bool:\n    '''\n    Check if a string is empty, or contains only spaces or punctuation.\n    Parameters:\n        s (str): The string to check.\n    Returns:\n        bool: True if the string is empty, or contains only spaces or punctuation, otherwise False.\n    '''\n    punct = set(string.punctuation)\n    for char in s:",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "detect_lang_paragraph_helper",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "def detect_lang_paragraph_helper(text: str, detect_func: Callable, tokenizer: str = 'blingfire', *args) -> Dict[\n    str, Dict[str, Union[float, int]]]:\n    '''\n    Detects the language(s) of a given text paragraph.\n    This function splits the text into individual sentences. It then employs the given detection function to identify the\n    language of each sentence. Subsequently, the probabilities are averaged for each language across all sentences,\n     and the output includes the count of sentences for each detected language.\n    Parameters:\n        text (str): The paragraph of text to detect the language for.\n        detect_func (Callable): The function used to detect the language of a sentence.",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "reduce_language_probabilities",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "def reduce_language_probabilities(lang_dict: Dict[str, List[float]]) -> Dict[str, Dict[str, Union[float, int]]]:\n    \"\"\"\n    Reduces the language probabilities dictionary to average probabilities and sentence counts.\n    Parameters:\n        lang_dict (Dict[str, List[float]]): Dictionary containing languages and their probabilities.\n    Returns:\n        Dict[str, Dict[str, Union[float, int]]]: Reduced dictionary with average probabilities and sentence counts.\n    \"\"\"\n    reduced_dict = {}\n    for lang, probs in lang_dict.items():",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "detect_lang_whole_page_langdetect",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "def detect_lang_whole_page_langdetect(text: str, seed=None) -> Dict[str, float]:\n    '''\n    Detect the language of an entire page using the langdetect model, without splitting into sentences first\n    Parameters:\n        text (str): The text whose languages are to be detected.\n        seed (Otpional[int]): The seed to use to make this deterministic.\n    Returns:\n        Dict where keys are strings specifying individual languages and values are predicted probabilities/\n        Returns an empty list if the text is empty or contains only spaces or punctuation.\n    '''",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "detect_lang_whole_page_fasttext",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "def detect_lang_whole_page_fasttext(model: fasttext.FastText._FastText, text: str, seed=None) -> List[str]:\n    '''\n    Detect the language of an entire page using the fasttext model, without splitting into sentences first\n    Parameters:\n        model (fasttext.FastText._FastText): The FastText model to use for language detection.\n        text (str): The text whose languages are to be detected.\n    Returns:\n        Returns: Dict[str, List[float]]: A dictionary with languages as keys and predicted probabilities as values.\n        Returns an empty dictionary if the text is empty or contains only spaces or punctuation.\n    '''",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "detect_lang_whole_page_enricher",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "def detect_lang_whole_page_enricher(model: str, key_prefix: str = \"language_id_whole_page\",\n                                    overwrite: bool = False, seed=None) -> Callable[[Dict], List[Dict]]:\n    '''\n    Enrichers a page with language detection information based upon all the text in the whole page. Using the fasttext model,\n    it is necessary to remove extra newline characters before performing inference.\n    Parameters:\n        model (str): The model to use for detection. Currently supports 'fasttext' and 'langdetect'.\n        key (str): The key to use for storing the language detection information in the page.\n        overwrite (bool): Whether to overwrite the existing language detection information in the page.\n        seed (Otpional[int]): The seed to use to make this deterministic.",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "detect_lang_paragraph_enricher",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "def detect_lang_paragraph_enricher(model: str, tokenizer: str, key_prefix: str = \"language_id_paragraph\",\n                                   overwrite: bool = False) -> Callable[[Dict], List[Dict]]:\n    '''\n    Enrichers the page with language detection information.\n    This function uses the detect_lang_paragraph_helper that splits the given paragraph to sentences,\n    detects the language of each sentence, and returns the average probabilities and sentence counts for each language.\n    Parameters:\n        model (str): The model to use for detection. Currently supports 'fasttext' and 'langdetect'.\n        tokenizer (str): The tokenizer to use for splitting the text into sentences. Currently supports 'blingfire' and 'nltk'.\n        key (str): The key to use for storing the language detection information in the page.",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "FASTTEXT",
        "kind": 5,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "FASTTEXT = 'fasttext'\nLANGDETECT = 'langdetect'\n# Mapping of models to their respective detection functions\nimport os\nimport fasttext\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))\nMODEL_SUBDIRECTORY = \"baselines/mappers/enrichers/language_id_enrichment_models\"\nFASTTEXT_MODEL_FILENAME = \"lid.176.bin\"\nMODEL_DIRECTORY = os.path.join(PROJECT_ROOT, MODEL_SUBDIRECTORY)\ndef load_fasttext_model():",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "LANGDETECT",
        "kind": 5,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "LANGDETECT = 'langdetect'\n# Mapping of models to their respective detection functions\nimport os\nimport fasttext\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))\nMODEL_SUBDIRECTORY = \"baselines/mappers/enrichers/language_id_enrichment_models\"\nFASTTEXT_MODEL_FILENAME = \"lid.176.bin\"\nMODEL_DIRECTORY = os.path.join(PROJECT_ROOT, MODEL_SUBDIRECTORY)\ndef load_fasttext_model():\n    if os.path.exists(MODEL_SUBDIRECTORY):",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "PROJECT_ROOT",
        "kind": 5,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))\nMODEL_SUBDIRECTORY = \"baselines/mappers/enrichers/language_id_enrichment_models\"\nFASTTEXT_MODEL_FILENAME = \"lid.176.bin\"\nMODEL_DIRECTORY = os.path.join(PROJECT_ROOT, MODEL_SUBDIRECTORY)\ndef load_fasttext_model():\n    if os.path.exists(MODEL_SUBDIRECTORY):\n        model_path = os.path.join(MODEL_SUBDIRECTORY, FASTTEXT_MODEL_FILENAME)\n    else:\n        model_path = os.path.join(MODEL_DIRECTORY, FASTTEXT_MODEL_FILENAME)\n    return fasttext.load_model(model_path)",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "MODEL_SUBDIRECTORY",
        "kind": 5,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "MODEL_SUBDIRECTORY = \"baselines/mappers/enrichers/language_id_enrichment_models\"\nFASTTEXT_MODEL_FILENAME = \"lid.176.bin\"\nMODEL_DIRECTORY = os.path.join(PROJECT_ROOT, MODEL_SUBDIRECTORY)\ndef load_fasttext_model():\n    if os.path.exists(MODEL_SUBDIRECTORY):\n        model_path = os.path.join(MODEL_SUBDIRECTORY, FASTTEXT_MODEL_FILENAME)\n    else:\n        model_path = os.path.join(MODEL_DIRECTORY, FASTTEXT_MODEL_FILENAME)\n    return fasttext.load_model(model_path)\ndef get_fasttext_lang_prob(model: fasttext.FastText._FastText, text: str) -> (str, float):",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "FASTTEXT_MODEL_FILENAME",
        "kind": 5,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "FASTTEXT_MODEL_FILENAME = \"lid.176.bin\"\nMODEL_DIRECTORY = os.path.join(PROJECT_ROOT, MODEL_SUBDIRECTORY)\ndef load_fasttext_model():\n    if os.path.exists(MODEL_SUBDIRECTORY):\n        model_path = os.path.join(MODEL_SUBDIRECTORY, FASTTEXT_MODEL_FILENAME)\n    else:\n        model_path = os.path.join(MODEL_DIRECTORY, FASTTEXT_MODEL_FILENAME)\n    return fasttext.load_model(model_path)\ndef get_fasttext_lang_prob(model: fasttext.FastText._FastText, text: str) -> (str, float):\n    '''",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "MODEL_DIRECTORY",
        "kind": 5,
        "importPath": "baselines.mappers.enrichers.language_id_enrichers",
        "description": "baselines.mappers.enrichers.language_id_enrichers",
        "peekOfCode": "MODEL_DIRECTORY = os.path.join(PROJECT_ROOT, MODEL_SUBDIRECTORY)\ndef load_fasttext_model():\n    if os.path.exists(MODEL_SUBDIRECTORY):\n        model_path = os.path.join(MODEL_SUBDIRECTORY, FASTTEXT_MODEL_FILENAME)\n    else:\n        model_path = os.path.join(MODEL_DIRECTORY, FASTTEXT_MODEL_FILENAME)\n    return fasttext.load_model(model_path)\ndef get_fasttext_lang_prob(model: fasttext.FastText._FastText, text: str) -> (str, float):\n    '''\n    Function to detect the language of a given text using FastText model.",
        "detail": "baselines.mappers.enrichers.language_id_enrichers",
        "documentation": {}
    },
    {
        "label": "load_fasttext_model",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "description": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "peekOfCode": "def load_fasttext_model(model_filename):\n    if os.path.exists(MODEL_SUBDIRECTORY):\n        model_path = os.path.join(MODEL_SUBDIRECTORY, model_filename)\n    else:\n        model_path = os.path.join(PROJECT_ROOT, MODEL_SUBDIRECTORY, model_filename)\n    assert os.path.exists(model_path), (\n        f\"Model {model_path} does not exist. \"\n        \"Please download the model to this path before running a baselines pipeline involving fasttext filtering. \"\n        \"See https://github.com/mlfoundations/dclm/blob/main/baselines/README.md#fasttext-filtering for more details.\"\n    )",
        "detail": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "documentation": {}
    },
    {
        "label": "classify_fasttext_hq_prob",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "description": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "peekOfCode": "def classify_fasttext_hq_prob(model: fasttext.FastText._FastText, content: str) -> dict:\n    '''\n    This function classifies a given text as either 'CC' or 'Wikipedia' and returns the label along with its probability.\n    Parameters:\n    model (fasttext.FastText._FastText): The FastText model to use for the classification.\n    content (str): The text to classify.\n    Returns:\n    dict: A value for 'hq_prob' - the probability to be a high-quality page.\n    '''\n    # Initialize an empty dictionary for the output",
        "detail": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "documentation": {}
    },
    {
        "label": "classify_fasttext_hq_prob_enricher",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "description": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "peekOfCode": "def classify_fasttext_hq_prob_enricher(model_filename=RPJ_MODEL_FILENAME, key: str = \"fasttext_hq_prob\", overwrite: bool = False) -> Callable[\n    [Dict], List[Dict]]:\n    '''\n    Enriches the given page with the text type (CC or Wikipedia).\n    Parameters:\n        page (dict): The page to enrich.\n        model_filename (str): The name of the fasttext model file. Assumes it is placed in MODEL_SUBDIRECTORY.\n        key (str): The key to store the text type under.\n        overwrite (bool): Whether to overwrite the existing value of the key.\n    Returns:",
        "detail": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "documentation": {}
    },
    {
        "label": "PROJECT_ROOT",
        "kind": 5,
        "importPath": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "description": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "peekOfCode": "PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))\nMODEL_SUBDIRECTORY = \"baselines/mappers/enrichers/quality_prediction_enrichment_models\"\nRPJ_MODEL_FILENAME = 'model.bin'\ndef load_fasttext_model(model_filename):\n    if os.path.exists(MODEL_SUBDIRECTORY):\n        model_path = os.path.join(MODEL_SUBDIRECTORY, model_filename)\n    else:\n        model_path = os.path.join(PROJECT_ROOT, MODEL_SUBDIRECTORY, model_filename)\n    assert os.path.exists(model_path), (\n        f\"Model {model_path} does not exist. \"",
        "detail": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "documentation": {}
    },
    {
        "label": "MODEL_SUBDIRECTORY",
        "kind": 5,
        "importPath": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "description": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "peekOfCode": "MODEL_SUBDIRECTORY = \"baselines/mappers/enrichers/quality_prediction_enrichment_models\"\nRPJ_MODEL_FILENAME = 'model.bin'\ndef load_fasttext_model(model_filename):\n    if os.path.exists(MODEL_SUBDIRECTORY):\n        model_path = os.path.join(MODEL_SUBDIRECTORY, model_filename)\n    else:\n        model_path = os.path.join(PROJECT_ROOT, MODEL_SUBDIRECTORY, model_filename)\n    assert os.path.exists(model_path), (\n        f\"Model {model_path} does not exist. \"\n        \"Please download the model to this path before running a baselines pipeline involving fasttext filtering. \"",
        "detail": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "documentation": {}
    },
    {
        "label": "RPJ_MODEL_FILENAME",
        "kind": 5,
        "importPath": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "description": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "peekOfCode": "RPJ_MODEL_FILENAME = 'model.bin'\ndef load_fasttext_model(model_filename):\n    if os.path.exists(MODEL_SUBDIRECTORY):\n        model_path = os.path.join(MODEL_SUBDIRECTORY, model_filename)\n    else:\n        model_path = os.path.join(PROJECT_ROOT, MODEL_SUBDIRECTORY, model_filename)\n    assert os.path.exists(model_path), (\n        f\"Model {model_path} does not exist. \"\n        \"Please download the model to this path before running a baselines pipeline involving fasttext filtering. \"\n        \"See https://github.com/mlfoundations/dclm/blob/main/baselines/README.md#fasttext-filtering for more details.\"",
        "detail": "baselines.mappers.enrichers.quality_prediction_enrichers_calc_fasttext",
        "documentation": {}
    },
    {
        "label": "SentencePiece",
        "kind": 6,
        "importPath": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "description": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "peekOfCode": "class SentencePiece:\n    def __init__(\n            self,\n            model: str,\n    ):\n        super().__init__()\n        self.sp = sentencepiece.SentencePieceProcessor()\n        self.sp.load(str(model))\n    def do(self, text: dict) -> dict:\n        tokenized = self.sp.encode_as_pieces(text)",
        "detail": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "documentation": {}
    },
    {
        "label": "KenlmModel",
        "kind": 6,
        "importPath": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "description": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "peekOfCode": "class KenlmModel:\n    digit_re: re.Pattern = re.compile(r\"\\d\")\n    unicode_punct: Dict[str, str] = {\n        \"，\": \",\",\n        \"。\": \".\",\n        \"、\": \",\",\n        \"„\": '\"',\n        \"”\": '\"',\n        \"“\": '\"',\n        \"«\": '\"',",
        "detail": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "documentation": {}
    },
    {
        "label": "ken_lm_perplexity_enricher",
        "kind": 2,
        "importPath": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "description": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "peekOfCode": "def ken_lm_perplexity_enricher(key: str = \"kenlm_perplexity\", overwrite: bool = False) -> Callable[[Dict], List[Dict]]:\n    '''\n    Enriches a page with the perplexity of its content.\n    :param key: the key to use for storing the perplexity\n    :param overwrite: whether to overwrite an existing key\n    :return: a function that enriches a page\n    '''\n    ken_lm_model = KenlmModel.from_pretrained(\"wikipedia\", \"en\")\n    def enrich(page: Dict) -> List[Dict]:\n        assert overwrite or key not in page, f\"cannot overwrite an existing key {key}\"",
        "detail": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "description": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "peekOfCode": "model = KenlmModel.from_pretrained(\"wikipedia\", \"en\")\n# Calculate and print the perplexity for a formal sentence with correct grammar\nprint(model.get_perplexity(\"I am very perplexed\"))\n# Output: 341.3 (low perplexity, since sentence style is formal and with no grammar mistakes)\n# Calculate and print the perplexity for a colloquial sentence with grammar mistakes\nprint(model.get_perplexity(\"im hella trippin\"))\n# Output: 46793.5 (high perplexity, since the sentence is colloquial and contains grammar mistakes)\n'''\nimport os\nimport re",
        "detail": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "documentation": {}
    },
    {
        "label": "PROJECT_ROOT",
        "kind": 5,
        "importPath": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "description": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "peekOfCode": "PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))\nMODEL_SUBDIRECTORY = \"baselines/mappers/enrichers/quality_prediction_enrichment_models\"\nclass SentencePiece:\n    def __init__(\n            self,\n            model: str,\n    ):\n        super().__init__()\n        self.sp = sentencepiece.SentencePieceProcessor()\n        self.sp.load(str(model))",
        "detail": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "documentation": {}
    },
    {
        "label": "MODEL_SUBDIRECTORY",
        "kind": 5,
        "importPath": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "description": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "peekOfCode": "MODEL_SUBDIRECTORY = \"baselines/mappers/enrichers/quality_prediction_enrichment_models\"\nclass SentencePiece:\n    def __init__(\n            self,\n            model: str,\n    ):\n        super().__init__()\n        self.sp = sentencepiece.SentencePieceProcessor()\n        self.sp.load(str(model))\n    def do(self, text: dict) -> dict:",
        "detail": "baselines.mappers.enrichers.quality_prediction_enrichers_kenlm_model",
        "documentation": {}
    },
    {
        "label": "github_extension_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def github_extension_filter(page: Dict, filename_key: str = 'filename', allowed_extensions: Union[Tuple[str], List[str]]=None) -> List[Dict]:\n    \"\"\"\n    Removes files that do not have an extension from a set of allowed extensions. If the filename is not present, then removes the page.\n    Args:\n    page (Dict): A dictionary representing a JSON object.\n    filename_key (str): Key name in the dictionary where the filename is stored. Defaults to 'filename'.\n    allowed_extensions (set): A set of allowed file extensions.\n    Returns:\n    List[Dict]: A list containing the input JSON object if its file extension is in the allowed set, or an empty list otherwise.\n    \"\"\"",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "line_length_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def line_length_filter(page: Dict, length_type: str = 'max', max_length=1000) -> List[Dict]:\n    \"\"\"\n    Remove files whose average or maximum line lengths do not fall within designated ranges.\n    Args:\n    page (Dict): A dictionary representing a JSON object.\n    length_type (str): The type of length to measure. Options are 'max' and 'avg'. Defaults to the values from RPJs\n    max_length (int): The maximum allowed line length. Defaults to 1000 characters.\n    Returns:\n    List[Dict]: A list containing the input JSON object if it meets the length criteria, or an empty list otherwise.\n    \"\"\"",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "alphanumeric_char_ratio_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def alphanumeric_char_ratio_filter(page: Dict, max_alnum_ratio=0.25) -> List[Dict]:\n    \"\"\"\n    Discard files whose proportion of alphanumeric characters is less than a specified ratio.\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.\n    max_ratio -- The maximum allowed ratio of alphanumeric characters. Defaults to 0.25, which rpj-v1 \n            uses to filter github pages.\n    Returns:\n    A list containing the input JSON object if it passes the filter, or an empty list if it doesn't.",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "alphabetic_characters_to_tokens_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def alphabetic_characters_to_tokens_filter(tokenizer_name: str = \"EleutherAI/pythia-6.9b-deduped\") -> List[Dict]:\n    \"\"\"\n    Files with a ratio between the number of alphabetical characters and the number tokens of less than 1.5. This is used in rpj-v1's\n    filtering of its github source. \n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.\n    tokenizer_name -- The name of the tokenizer used to get token counts. Defaults to the one in rpj-v1's\n            version of this filter for github. \n    max_ratio -- The maximum allowed ratio of alphabetical characters, defaults to rpj-v1's choice of 1.5 for github",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "massive_web_repetition_filters",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def massive_web_repetition_filters(page: Dict, skip_paragraph=False) -> List[Dict]:\n    \"\"\"\n    Applies the repetition filters from Gopher (Rae et al., 2021)\n    Calls repetition_filter across many different granularities\n    for {2,3,4}-grams we need to count the fraction of characters in the most common n-gram, and \n    for {5,6,7,8,9,10}-grams we count the function of characters appearing in n-grams that repeat more than once\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.\n    skip_paragraph -- If True, skips the paragraph-based filters, such as in the case where text extraction does ",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "repetition_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def repetition_filter(page: Dict, granularity: Union[str, int], max_fraction: float, \n                      count_characters: bool=True, ngram_char_ratio: str=None, ignore_case: bool=False, cache: Dict=None) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object based on the ratio of repetition at {line, paragraph, n-gram} granularity of the CONTENT field.\n    This function measures the ratio of repetition at various granularities.\n    If the ratio of repetition is greater than `max_fraction`, it returns an empty list.\n    If the length is less/equal to `max_fraction`, it returns a list containing the original JSON object.\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "page_length_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def page_length_filter(page: Dict, length_type: str, min_length: int = 1,\n                       max_length: int = float('inf'), **kwargs) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object based on the length of the CONTENT field.\n    This function measures page length according to a specified atomic unit (e.g., char, word, sentence,\n    line, paragraph).\n    If the length is less than `min_length`, it returns an empty list. \n    If the length is greater/equal to `min_length`, it returns a list containing the original JSON object.\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "substring_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def substring_filter(banlist: Union[str, List] = None, banlist_from_fname: str = None,\n                     location: str = 'any', case_sensitive=False, exact_word=False) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object by removing any document that contains a particular substring contained\n    in a banlist. Also, one may specify whether the banned substring must exist at the beginning/end of \n    the page. Use cases include \n        - lorem ipsum filter from C4 (banlist = \"lorem ipsum\", location = 'any') from C4\n        - curly bracket filter from C4 (banlist = \"{\", location = 'any') from C4\n        - checking for LDNOOBW to filter for in appropriate content from C4\n    This function is implemented as a factory_function since we provide the option to load in a banlist from",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "bullet_count_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def bullet_count_filter(page: Dict, max_bullet_start_ratio: float = 0.9) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object based on the number of lines starting with bullet in the CONTENT field.\n    This function measures the ratio of lines starting with bullet,\n        i.e., (the number of lines starting with bullet) / (the number of lines).\n    If the ratio of lines starting with bullet is greater than `max_bullet_start_ratio`, it returns an empty list.\n    If the ratio of lines starting with bullet is less/equal to `max_bullet_start_ratio`, it returns a list containing the original JSON object.\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "ellipsis_count_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def ellipsis_count_filter(page: Dict, max_ellipsis_end_ratio: float = 0.3) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object based on the number of lines ending with ellipsis in the CONTENT field.\n    This function measures the ratio of lines ending with ellipsis,\n        i.e., (the number of lines ending with ellipsis) / (the number of lines).\n    If the ratio of lines ending with ellipsis is greater than `max_ellipsis_end_ratio`, it returns an empty list.\n    If the ratio of lines ending with ellipsis is less/equal to `max_ellipsis_end_ratio`, it returns a list containing the original JSON object.\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "stop_word_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def stop_word_filter(page: Dict, count_unique: bool = False, min_stop_word: int = 2) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object based on the number of stop words in the text.\n    This function measures the number of stop words (i.e., the, be, to, of, and, that, have, with).\n    If the number of stop words is less than `min_stop_word`, it returns an empty list.\n    If the number of stop words is greater/equal to `min_stop_word`, it returns a list containing the original JSON object.\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.\n    count_unique -- Whether to only count unique stop words instead of all instances of stop words",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "word_length_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def word_length_filter(page: Dict, min_length: int = 0, max_length: int = float('inf')) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object based on average word length in the CONTENT field.\n    This function measures average word length.\n    If the length is less than `min_length` or greater than `max_length`, it returns an empty list.\n    If the length is greater/equal to `min_length` and less/equal to `max_length`, it returns a list containing the original JSON object.\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.\n    min_length -- The minimum length threshold for average word length",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "symbol_ratio_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def symbol_ratio_filter(page: Dict, max_symbol_to_word_ratio: float = 0.1) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object based on the symbol to word ratio according to the CONTENT field.\n    This function measures the symbol to word ratio, where symbols are hash and ellipsis.\n    If the symbol to word ratio is greater than `max_symbol_to_word_ratio`, it returns an empty list.\n    If the length is less/equal to `max_symbol_to_word_ratio`, it returns a list containing the original JSON object.\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.\n    max_symbol_to_word_ratio -- The maximum symbol to word ratio",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "word_removal_ratio_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def word_removal_ratio_filter(page: Dict, prev_word_count_key: str, new_word_count_key: Optional[str] = None,\n                              max_removed_ratio: float = 0.05, ignore_punctuation=True, **kwargs) -> Optional[Dict]:\n    \"\"\"\n    Filter out pages where the number of words removed by other modifiers is more than percent_removed_threshold (defaults to 5%). Note: this \n    method assumes that you have previously counted the number of words in the document with word_counter_enricher.  \n    Arguments:\n    page: The page dictionary containing the content and additional metadata.\n    prev_word_count_key: The key to use for retrieving the \"before modifiers\" word count.\n    new_word_count_key: The key to use for the \"after modifiers\" word count. If not provided, it will be calculated within this method. \n    max_removed_ratio: The ratio of words that can be removed without filtering out the page (default 0.05, equivalent to 5%).",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "alphabetic_word_ratio_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "def alphabetic_word_ratio_filter(page: Dict, max_ratio: float = 0.2) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object based on the percentage of words that do not contain\n    at least one alphabetic character.\n    This function calculates the percentage of words in the CONTENT that contain at least\n    one alphabetic character. If this percentage exceeds the provided threshold, the function\n    returns an empty list. If the percentage is less than or equal to the threshold, the\n    function returns a list containing the original JSON object.\n    Non-alphabetic characters include digits, punctuation, spaces, tabs, and newline characters.\n    Arguments:",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "RPJ_GITHUB_EXTENSIONS",
        "kind": 5,
        "importPath": "baselines.mappers.filters.content_filters",
        "description": "baselines.mappers.filters.content_filters",
        "peekOfCode": "RPJ_GITHUB_EXTENSIONS = (\".asm\", \".bat\", \".cmd\", \".c\", \".h\", \".cs\", \".cpp\",\n                        \".hpp\", \".c++\", \".h++\", \".cc\", \".hh\", \".C\", \".H\",\n                        \".cmake\", \".css\", \".dockerfile\", \".f90\", \".f\", \".f03\",\n                        \".f08\", \".f77\", \".f95\", \".for\", \".fpp\", \".go\", \".hs\",\n                        \".html\", \".java\", \".js\", \".jl\", \".lua\", \".md\",\n                        \".markdown\", \".php\", \".php3\", \".php4\", \".php5\",\n                        \".phps\", \".phpt\", \".pl\", \".pm\", \".pod\", \".perl\",\n                        \".ps1\", \".psd1\", \".psm1\", \".py\", \".rb\", \".rs\", \".sql\",\n                        \".scala\", \".sh\", \".bash\", \".command\", \".zsh\", \".ts\",\n                        \".tsx\", \".tex\", \".vb\", \"Dockerfile\", \"Makefile\",",
        "detail": "baselines.mappers.filters.content_filters",
        "documentation": {}
    },
    {
        "label": "random_sampling_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.metadata_filters",
        "description": "baselines.mappers.filters.metadata_filters",
        "peekOfCode": "def random_sampling_filter(page, keep_probability=0.1):\n    \"\"\"\n    Filter the JSON objects randomly based on a random coinflip, in order to subsample according to a specified probability. \n    Arguments:\n    page -- A dictionary representation of the page.\n    keep_probability -- What proportion of pages to keep\n    Returns:\n    A list containing the page if the language is in the keep_languages list and exceeds the threshold, otherwise an empty list.\n    \"\"\"\n    assert 0 <= keep_probability <= 1",
        "detail": "baselines.mappers.filters.metadata_filters",
        "documentation": {}
    },
    {
        "label": "language_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.metadata_filters",
        "description": "baselines.mappers.filters.metadata_filters",
        "peekOfCode": "def language_filter(page: Dict, keep_languages: List[str], key='language_id_whole_page_langdetect', threshold=0.0) -> \\\n        List[Dict]:\n    \"\"\"\n    Filter the JSON objects by keeping only the ones that have the specified language, with the option to provide a threshold on\n    the predicted probability\n    Arguments:\n    page -- A dictionary representation of the page.\n    keep_languages -- A list of languages to keep.\n    key -- The metadata key for LID, defaults to the key for using langdetect on the whole page\n    threshold -- A probability threshold for detecting a language",
        "detail": "baselines.mappers.filters.metadata_filters",
        "documentation": {}
    },
    {
        "label": "quality_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.metadata_filters",
        "description": "baselines.mappers.filters.metadata_filters",
        "peekOfCode": "def quality_filter(page: Dict, key: str = 'fasttext_hq_prob', threshold: float=0.0, lower_better: bool=False, key_must_exist: bool=True) -> List[Dict]:\n    \"\"\"\n    Filters the JSON objects based on a quality score (e.g. from a model-based prediction). \n    Arguments: \n    page -- A dictionary representation of the page. \n    key -- A string specifying which quality score, the default is the default key produced by the fasttext hq_prob model\n    threshold -- A float indicating the minimum quality required to keep the page. \n    lower_better - A bool for whether lower quality score is better (e.g., for perplexity, lower_better should be True).\n    key_must_exist - A bool for whether the key must exist for all pages. If False, will filter out pages that are missing the key\n    Returns:",
        "detail": "baselines.mappers.filters.metadata_filters",
        "documentation": {}
    },
    {
        "label": "url_substring_filter",
        "kind": 2,
        "importPath": "baselines.mappers.filters.metadata_filters",
        "description": "baselines.mappers.filters.metadata_filters",
        "peekOfCode": "def url_substring_filter(banlist: Union[str, List] = None, banlist_from_fname: str = None, ignore_chars: List[str] = None, \n                         num_banned_substrs: int = 1, exact_domain_match: bool=False, match_substrings=True, case_sensitive=False) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object by URL\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a 'url' field\n            that contains the urls for the pages to be analyzed\n    banlist -- A list of banned substrs to look for within a url.\n    banlist_from_fname -- Gives the option to load in a large banlist from a .txt file where each substring\n                          is on a spearate line. It can also take in a .pkl file containing a pre-compiled regex",
        "detail": "baselines.mappers.filters.metadata_filters",
        "documentation": {}
    },
    {
        "label": "do_once",
        "kind": 2,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "def do_once(func):\n    \"\"\"\n    A decorator that runs a function only once.\n    \"\"\"\n    done = set()\n    def wrapper(*args, **kwargs):\n        fn_str_parts = [func.__name__]\n        if len(args) > 0:\n            args = [str(a) for a in args]\n            args_str = \"_\".join(args)",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "split_paragraphs",
        "kind": 2,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "def split_paragraphs(text: str, paragraph_end='\\n', remove_empty: bool = True) -> List[str]:\n    r\"\"\"\n    Split a string into paragraphs. A paragraph is defined as a sequence of zero or more characters, followed\n    by a newline character(s), or a sequence of one or more characters, followed by the end of the string.\n    Args:\n        text (str): The text to split.\n        paragraph_end (str): The sequence of character that marks a paragraph end. default: '\\n', but can be '\\n\\n' for example.\n        remove_empty (bool): Whether to remove empty paragraphs.\n    Returns:\n        List[str]: A list of strings where each string represents a paragraph from the original text.",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "split_sentences",
        "kind": 2,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "def split_sentences(text: str, remove_empty: bool = True, tokenizer='blingfire', tokenizer_lang=None) -> List[str]:\n    \"\"\"\n    Split a string into sentences.\n    Note - this is not perfect (as you can see by the e.g. example below)\n    Args:\n        text (str): The text to split.\n        remove_empty (bool): Whether to remove empty sentences.\n        tokenizer (str): The tokenizer to use. one of 'blingfire' or 'nltk'. default: 'blingfire'\n    Returns:\n        List[str]: A list of strings where each string represents a sentence from the original text.",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "split_words",
        "kind": 2,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "def split_words(text: str, model='fasttext', ignore_punctuation: bool = False, ignore_whitespace: bool = True) -> \\\n        List[str]:\n    \"\"\"\n    Counts the number of words in a text string.\n    Args:\n        text (str): The text to count words in.\n        model (str): The tokenizer model to use. one of 'uniseg' and 'fasttext'.\n        ignore_punctuation (bool): Whether to ignore punctuation.\n        ignore_whitespace (bool): Whether to ignore whitespace.\n    Returns:",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "join_sentences",
        "kind": 2,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "def join_sentences(lines: List[str], sep=' ') -> str:\n    \"\"\"\n    Join a list of sentences into a single string (paragraph).\n    Args:\n        lines (List[str]): The list of sentences to join.\n        sep (str): The separator to use.\n    Returns:\n        str: A single string made by joining the input list of strings.\n    \"\"\"\n    return sep.join(lines)",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "join_paragraphs",
        "kind": 2,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "def join_paragraphs(paragraphs: List[str], sep='\\n') -> str:\n    \"\"\"\n    Join a list of paragraphs into a single string.\n    Args:\n        paragraphs (List[str]): The list of paragraphs to join.\n        sep (str): The separator to use.\n    Returns:\n        str: A single string made by joining the input list of strings.\n    \"\"\"\n    return sep.join(paragraphs)",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "normalize_url",
        "kind": 2,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "def normalize_url(url: str) -> str:\n    \"\"\"\n    Normalizes urls as a way to assist with dedup. The specific rule is taken\n    from the TFDS C4 repo: \n    https://github.com/tensorflow/datasets/blob/fbacae9034d61870ae8d639c7d3f4a667c434879/\n    tensorflow_datasets/text/c4_utils.py#L501\n    Args:\n        url (str): The url to normalize\n    Returns:\n        str: A normalized url ",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "normalize_whitespace_and_lowercase",
        "kind": 2,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "def normalize_whitespace_and_lowercase(text: str) -> str:\n    \"\"\"\n    Normalizes paragraphs by stripping whitespace and converting to lowercase. \n    Args:\n        text (str): The text to normalize\n    Returns:\n        str: A string that is case and {leading, trailing} whitespace-normalized\n    \"\"\"\n    return text.strip().lower()\ndef normalize_timestamps(timestamp: str, date_format = '%Y-%m-%dT%H:%M:%SZ', default_val=-1.0) -> float:",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "normalize_timestamps",
        "kind": 2,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "def normalize_timestamps(timestamp: str, date_format = '%Y-%m-%dT%H:%M:%SZ', default_val=-1.0) -> float:\n    \"\"\"\n    Converts timestamp strings into a float which can be used for comparisons.\n    Args:\n        timestamp (str): The timestamp to convert\n        date_format (str): The string specifying the format for the timestamp\n        default_val (float): The value assigned to any timestamp where the timestamp and date_format\n        are not compatible with each other. \n    Returns:\n        float: A numeric representation of the timestamp",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "hash_text",
        "kind": 2,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "def hash_text(text: str):\n    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\nUNICODE_PUNCT = {\n    \"，\": \",\",\n    \"。\": \".\",\n    \"、\": \",\",\n    \"„\": '\"',\n    \"”\": '\"',\n    \"“\": '\"',\n    \"«\": '\"',",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "ccnet_dedup_normalizer",
        "kind": 2,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "def ccnet_dedup_normalizer(line: str) -> str:\n    \"\"\"\n    Normalizes the string by removing punctuation and non-printable characters and accent characters and replacing all digits with 0.\n    Args:\n        line (str): string to convert\n    Returns:\n        str: A normalized string\n    \"\"\"\n    line = line.strip()\n    if not line:",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "sent_tokenizer",
        "kind": 5,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "sent_tokenizer = None\n@do_once\ndef _prep_nltk_tokenizer(tokenizer_lang):\n    try:\n        nltk.data.find('tokenizers/punkt')\n        global sent_tokenizer\n        if tokenizer_lang in ['en', 'english']:\n            # This is the specific tokenizer used in C4\n            sent_tokenizer = nltk.data.load(\"nltk:tokenizers/punkt/english.pickle\")\n        else:",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "UNICODE_PUNCT",
        "kind": 5,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "UNICODE_PUNCT = {\n    \"，\": \",\",\n    \"。\": \".\",\n    \"、\": \",\",\n    \"„\": '\"',\n    \"”\": '\"',\n    \"“\": '\"',\n    \"«\": '\"',\n    \"»\": '\"',\n    \"１\": '\"',",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "UNICODE_PUNCT_RE",
        "kind": 5,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "UNICODE_PUNCT_RE = re.compile(f\"[{''.join(UNICODE_PUNCT.keys())}]\")\nNON_PRINTING_CHARS_RE = re.compile(f\"[{''.join(map(chr, list(range(0,32)) + list(range(127,160))))}]\")\nDIGIT_RE = re.compile(r\"\\d\")\nPUNCT_OR_NON_PRINTING_CHARS_RE = re.compile(\n    (UNICODE_PUNCT_RE.pattern + NON_PRINTING_CHARS_RE.pattern).replace(\"][\", \"\")\n)\ndef ccnet_dedup_normalizer(line: str) -> str:\n    \"\"\"\n    Normalizes the string by removing punctuation and non-printable characters and accent characters and replacing all digits with 0.\n    Args:",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "NON_PRINTING_CHARS_RE",
        "kind": 5,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "NON_PRINTING_CHARS_RE = re.compile(f\"[{''.join(map(chr, list(range(0,32)) + list(range(127,160))))}]\")\nDIGIT_RE = re.compile(r\"\\d\")\nPUNCT_OR_NON_PRINTING_CHARS_RE = re.compile(\n    (UNICODE_PUNCT_RE.pattern + NON_PRINTING_CHARS_RE.pattern).replace(\"][\", \"\")\n)\ndef ccnet_dedup_normalizer(line: str) -> str:\n    \"\"\"\n    Normalizes the string by removing punctuation and non-printable characters and accent characters and replacing all digits with 0.\n    Args:\n        line (str): string to convert",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "DIGIT_RE",
        "kind": 5,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "DIGIT_RE = re.compile(r\"\\d\")\nPUNCT_OR_NON_PRINTING_CHARS_RE = re.compile(\n    (UNICODE_PUNCT_RE.pattern + NON_PRINTING_CHARS_RE.pattern).replace(\"][\", \"\")\n)\ndef ccnet_dedup_normalizer(line: str) -> str:\n    \"\"\"\n    Normalizes the string by removing punctuation and non-printable characters and accent characters and replacing all digits with 0.\n    Args:\n        line (str): string to convert\n    Returns:",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "PUNCT_OR_NON_PRINTING_CHARS_RE",
        "kind": 5,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "PUNCT_OR_NON_PRINTING_CHARS_RE = re.compile(\n    (UNICODE_PUNCT_RE.pattern + NON_PRINTING_CHARS_RE.pattern).replace(\"][\", \"\")\n)\ndef ccnet_dedup_normalizer(line: str) -> str:\n    \"\"\"\n    Normalizes the string by removing punctuation and non-printable characters and accent characters and replacing all digits with 0.\n    Args:\n        line (str): string to convert\n    Returns:\n        str: A normalized string",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "DEDUP_NORMALIZERS",
        "kind": 5,
        "importPath": "baselines.mappers.core_utils",
        "description": "baselines.mappers.core_utils",
        "peekOfCode": "DEDUP_NORMALIZERS = {\n    'normalize_url': normalize_url,\n    'normalize_timestamps': normalize_timestamps,\n    'normalize_whitespace_and_lowercase': normalize_whitespace_and_lowercase,\n    'hash_text': hash_text,\n    'ccnet_dedup_normalizer': ccnet_dedup_normalizer\n}",
        "detail": "baselines.mappers.core_utils",
        "documentation": {}
    },
    {
        "label": "starcoder_v2_repo_splitter",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def starcoder_v2_repo_splitter(page: Dict, max_files=1000, delete_content=True):\n    \"\"\"\n    Modifies the input JSON object by splitting the contents of large repos into smaller chunks.\n    Args:\n    page (Dict): A dictionary representing a JSON object. It should have a CONTENT field\n                 that contains the text to be analyzed. page corresponds to a single repo\n                 where 'content' is a list of dictionaries, each containing 'text' and 'filename'\n                 (along with other metadatda)\n    max_files (int): The maximum number of files to include in each chunk. The default is 1000.\n    delete_content (bool): Whether or not to delete the CONTENT field from the resulting pages.              ",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "starcoder_v2_format_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def starcoder_v2_format_modifier(page: Dict, add_metadata_prob: float=0.5, add_sentinels: bool=True) -> List[Dict]:\n    \"\"\"\n    Modifies the input JSON object by formatting the content to match the StarCoder V2 format. \n    This includes concatenating all files in a repo together and gives the option to add metadata \n    information (i.e., repo name, file name, etc.) to the content.\n    Args:\n    page (Dict): A dictionary representing a JSON object. It should have a CONTENT field\n                 that contains the text to be analyzed. page corresponds to a single repo\n                 where 'content' is a list of dictionaries, each containing 'text' and 'filename'\n                 (along with other metadatda)",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "arxiv_appendix_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def arxiv_appendix_modifier() -> List[Dict]:\n    \"\"\"\n    Modify the input JSON object by removing any content after the first occurrence of either \\appendix, \\bibliography,\n    or variations of \\begin{references}. \n    Taken from rpj_v1: https://github.com/togethercomputer/RedPajama-Data/blob/rp_v1/data_prep/arxiv/arxiv_cleaner.py\n    Args:\n        page (Dict): A dictionary representing a JSON object. It should have a CONTENT field\n                     that contains the text to be analyzed.\n    Returns:\n        List[Dict]: A list containing the input JSON object with content after the specified headers removed.",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "arxiv_comment_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def arxiv_comment_modifier(remove_multiline=False) -> List[Dict]:\n    \"\"\"\n    Modify the input JSON object by removing LaTeX comments from the content.\n    This includes both single-line comments (starting with '%') and multi-line comments\n    enclosed within \\begin{comment} and \\end{comment} tags. It also removes in-line comments\n    that are not at the start of a line. \n    Taken from rpj_v1: https://github.com/togethercomputer/RedPajama-Data/blob/rp_v1/data_prep/arxiv/arxiv_cleaner.py\n    Args:\n        page (Dict): A dictionary representing a JSON object. It should have a CONTENT field\n                     that contains the text to be analyzed.",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "arxiv_macro_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def arxiv_macro_modifier() -> List[Dict]:\n    \"\"\"\n    Modify the input JSON object by inline-expanding LaTeX macros defined via \\newcommand and \\def\n    that have no arguments. This function targets macros that are defined but not expanded.\n    Taken from rpj_v1: https://github.com/togethercomputer/RedPajama-Data/blob/rp_v1/data_prep/arxiv/arxiv_cleaner.py\n    Args:\n    page (Dict): A dictionary representing a JSON object. It should have a CONTENT field\n                 that contains the text to be analyzed.\n    Returns:\n    List[Dict]: A list containing the input JSON object with specified LaTeX macros inline-expanded.",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "arxiv_section_strip_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def arxiv_section_strip_modifier() -> List[Dict]:\n    \"\"\"\n    Modify the input JSON object by removing any content before the first occurrence of a LaTeX section command\n    or other section-like headers (e.g., chapter, subsection). This aims to strip the preamble or any content\n    that precedes the main body of a LaTeX document.\n    Taken from rpj_v1: https://github.com/togethercomputer/RedPajama-Data/blob/rp_v1/data_prep/arxiv/arxiv_cleaner.py\n    Args:\n        page (Dict): A dictionary representing a JSON object. It should have a CONTENT field\n                     that contains the text to be analyzed.\n    Returns:",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "stackexchange_list_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def stackexchange_list_modifier(page: Dict) -> List[Dict]:\n    \"\"\"\n    Modifies the input JSON object for stackexchange pages by replacing HTML list tags with newline and bullet point symbols. \n    Specifically, this function replaces <li> tags with \"\\n*\" and <ol> tags with \"\\n\" to standardize list elements.\n    It derives from the stackexchange processing pipeline from rpj_v1 and therefore operates on the page's 'question' and \n    'answers' keys instead of CONTENT. \n    Args:\n    page (Dict): A dictionary representing a JSON object. It should have a 'question' field and optionally a 'answers' field\n                 that contains the text to be analyzed.\n    Returns:",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "stackexchange_answer_sort_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def stackexchange_answer_sort_modifier(page: Dict, descending=True) -> List[Dict]:\n    \"\"\"\n    Modifies the input JSON object for stackexchange pages by sorting the answers by their score metadata.\n    It derives from the stackexchange processing pipeline from rpj_v1 and therefore operates on the page's 'question' and \n    'answers' keys instead of CONTENT. If 'answers' is not present, it is a no-op.\n    Args:\n    page (Dict): A dictionary representing a JSON object. It should have a 'question' field and optionally a 'answers' field\n                 that contains the text to be analyzed.\n    descending (bool): When True, puts the highest scoring answers first. When False, puts lowest first.\n    Returns:",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "stackexchange_html_extraction_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def stackexchange_html_extraction_modifier():\n    \"\"\"\n    Modifies the input JSON object for stackexchange pages by extracting the html for questions and answers. Uses BeautifulSoup\n    as per the implementation in rpj_v1.  \n    It derives from the stackexchange processing pipeline from rpj_v1 and therefore operates on the page's 'question' and \n    'answers' keys instead of CONTENT.\n    Args:\n    page (Dict): A dictionary representing a JSON object. It should have a 'question' field and optionally a 'answers' field\n                 that contains the text to be analyzed.\n    Returns:",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "stackexchange_qa_formatter",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def stackexchange_qa_formatter(page: Dict, remove_qa=False) -> List[Dict]:\n    \"\"\"\n    Modifies the input JSON object for stackexchange pages by combining the 'question' and 'answers' fields into one string.\n    Specifically, this uses the format \n    Q: Question text\n    A: Answer 1 text\n    A: Answer 2 text\n    ⋮\n    A: Answer N text\n    It derives from the stackexchange processing pipeline from rpj_v1 and therefore operates on the page's 'question' and ",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "move_url_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def move_url_modifier(page: Dict) -> List[Dict]:\n    page[URL] = page['metadata']['WARC-Target-URI']\n    return [page]\ndef key_name_modifier(page: Dict, old_key='content', new_key='text', allow_overwrite=False) -> List[Dict]:\n    \"\"\"\n    Changes the name of a key in a page dictionary. Primarily used for handling outdated raw sources \n    where the CONTENT key is \"content\" instead of \"text.\" If old_key is not present, this function\n    is a no-op. If new_key is already present, allow_overwrite must be set to True for the function to \n    overwrite the previous value in new_key. \n    Arguments:",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "key_name_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def key_name_modifier(page: Dict, old_key='content', new_key='text', allow_overwrite=False) -> List[Dict]:\n    \"\"\"\n    Changes the name of a key in a page dictionary. Primarily used for handling outdated raw sources \n    where the CONTENT key is \"content\" instead of \"text.\" If old_key is not present, this function\n    is a no-op. If new_key is already present, allow_overwrite must be set to True for the function to \n    overwrite the previous value in new_key. \n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            containing the raw HTML data obtained from CC\n    old_key -- The name of the existing key that should be renamed",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "html_content_extraction_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def html_content_extraction_modifier(page: Dict) -> List[Dict]:\n    \"\"\"\n    Uses the `justext` package to replace the full HTML with extracted text.\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            containing the raw HTML data obtained from CC\n    Returns:\n    A list containing the input JSON object that replaces the html in CONTENT with extracted text\n    \"\"\"\n    try:",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "substring_line_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def substring_line_modifier(banlist: Union[str, List], case_sensitive=False,\n                            location='any', max_length=None, remove_substring_only=False) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object - Remove lines that contain the given substring\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a 'content' field\n            that contains the text to be analyzed.\n    banlist -- The list of substrings that is banned\n    case_sensitive -- Whether to ignore case when checking for banlist items in specific lines\n    location -- Where the substring exists in the line. Options are {prefix, suffix, any}",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "punctuation_line_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def punctuation_line_modifier(remove_ellipses=False):\n    \"\"\"\n    Filters the input JSON object - Remove lines if they do not end in a punctuation mark\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a 'content' field\n            that contains the text to be analyzed.\n    remove_ellipses -- A boolean that specifies whether ellipses should count as punctuation. \n    To replicate the behavior of the C4 TFDS codebase, it shoud be set to True. \n    Returns:\n    A list containing the input JSON object if it passes the filter,",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "line_length_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def line_length_modifier(page: Dict, min_length=0, max_length=float('inf')) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object - Remove lines with word counts outside accepted range\n    (ps - Ideally, may want optional argument for prefix/suffix/substring banlist)\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a 'content' field\n            that contains the text to be analyzed.\n    min_length -- Minimum number of words to keep a line (inclusive).\n    max_length -- Maximum number of words allowed to keep a line (inclusive).\n    Returns:",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "word_length_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def word_length_modifier(page: Dict, max_length=1000, **kwargs) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object - Remove lines where the word with the largest length goes\n    strictly over max_length. \n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a 'content' field\n            that contains the text to be analyzed.\n    max_length -- Maximum allowed length of a particular word. \n    kwargs -- kwargs for split_words\n    Returns:",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "uppercase_ratio_line_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def uppercase_ratio_line_modifier(page: Dict, max_ratio=0.5) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object - Remove lines where uppercase characers exceed a certain ratio\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a 'text' field\n            that contains the text to be analyzed.\n    max_ratio -- The maximum allowed ratio of uppercase characters\n    Returns:\n    A list containing the input JSON object if it passes the filter,\n    or an empty list if it doesn't.",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "numeric_ratio_line_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def numeric_ratio_line_modifier(page: Dict, max_ratio=1.0) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object - Remove lines if numerical characters exceed a certain ratio\n    (ps - Falcon removes lines which contain 100% numerical characters)\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.\n    max_ratio -- The maximum allowed ratio of numeric characters. Note that max_ratio is not\n            a strict threshold. To replicate the RefinedWeb rule which checks for all characters\n            being numeric, set max_ratio as something like 1 - eps for eps = 1e-6.",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "citation_removal_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def citation_removal_modifier() -> List[Dict]:\n    \"\"\"\n    Modifies the input JSON object - Remove text related to citations (Wiki-format)\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a 'content' field\n            that contains the text to be analyzed.\n    Returns:\n    A list containing the input JSON object with citation-related text removed.\n    \"\"\"\n    citation_regex = re.compile(r\"\\[\\d*\\]|\\[edit\\]|\\[citation needed\\]\")",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "url_removal_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def url_removal_modifier(tlds_filepath=\"baselines/mappers/iana_tlds.txt\"):\n    \"\"\"\n    Modifies the input JSON object - Removes all urls within the content of a page, relying\n    on two regexes for finding URLs: one relies upon a \"vocab list\" of existing top-level domains (TLDs),\n    such as \".com\", \".org\", etc.; the other detects IP addresses\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a 'content' field\n            that contains the text to be analyzed.\n    tlds_filepath -- Path to a text file where the TLDs vocab is stored. The default is the path\n            to the full list from IANA (https://www.iana.org/domains/root/db) assuming you are in the",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "counter_line_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def counter_line_modifier() -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object - Remove lines if it is a counter (e.g. 3 likes)\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.\n    Returns:\n    A list containing the input JSON object if it passes the filter,\n    or an empty list if it doesn't.\n    \"\"\"",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "within_page_dedup",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def within_page_dedup(page: Dict, granularity: str = 'line', normalize=True, **kwargs) -> List[Dict]:\n    \"\"\"\n    Modifies the input JSON object by removing exactly repeated text\n    This function looks for exact duplicates according to a specified atomic unit. Right now this suppoers \"line\" and \"paragraph\"\n    Only the first occurrence of a repeated unit of text is kept. \n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.\n    granularity -- An string or int for how the repetition is measured. Accepted otions are {\"line\", \"paragraph\"} \n    normalize -- Whether or not to normalize the text within each unit (i.e., convert to lowercase and strip whitespace)",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "newline_removal_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def newline_removal_modifier(max_consecutive=2):\n    \"\"\"\n    This modifier normalizes line spacing by controlling for the maximum allowed consecutive newline characters ('\\n')\n    within a page.\n    Arguments:\n    - page (Dict): A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.\n    - max_consecutive (int): The maximum number of consecutive newline characters to allow.\n    Returns:\n    A list containing the modified version of the input JSON object. ",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "split_lines_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def split_lines_modifier(page, delimiter='\\n'):\n    \"\"\"\n    This modifier splits the content of a page into a list of list of lines based on a delimiter\n    If the page is empty, it instead gets removed. \n    Arguments:\n    - page (Dict): A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.\n    - delimiter (int): The character or substring to split on\n    Returns:\n    A list containing the modified version of the input JSON object. ",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "join_lines_modifier",
        "kind": 2,
        "importPath": "baselines.mappers.modifiers",
        "description": "baselines.mappers.modifiers",
        "peekOfCode": "def join_lines_modifier(page, delimiter='\\n'):\n    \"\"\"\n    This modifier joins the content of a page if it is in a list (such as the output from deduplication).\n    The specific delimiter to join on may be specified. \n    Arguments:\n    - page (Dict): A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.\n    - delimiter (int): The character or substring to join on\n    Returns:\n    A list containing the modified version of the input JSON object. ",
        "detail": "baselines.mappers.modifiers",
        "documentation": {}
    },
    {
        "label": "percentiles",
        "kind": 2,
        "importPath": "baselines.aggregators",
        "description": "baselines.aggregators",
        "peekOfCode": "def percentiles(values):\n    \"\"\"\n    Calculate percentiles of a list of values. In case the percentile is not in the list, the nearest value is returned.\n    Arguments:\n    values -- A list of values.\n    Returns:\n    A dictionary with the percentiles.\n    \"\"\"\n    if not isinstance(values, list):\n        raise TypeError(\"Values must be a list.\")",
        "detail": "baselines.aggregators",
        "documentation": {}
    },
    {
        "label": "histogram",
        "kind": 2,
        "importPath": "baselines.aggregators",
        "description": "baselines.aggregators",
        "peekOfCode": "def histogram(values):\n    \"\"\"\n    Calculate a histogram of a list of values.\n    Arguments:\n    values -- A list of values.\n    Returns:\n    A dictionary with the histogram.\n    \"\"\"\n    if isinstance(values[0], str):\n        return Counter(values)",
        "detail": "baselines.aggregators",
        "documentation": {}
    },
    {
        "label": "threshold_transform",
        "kind": 2,
        "importPath": "baselines.aggregators",
        "description": "baselines.aggregators",
        "peekOfCode": "def threshold_transform(weighted_values: Dict[str, float], threshold, default='unknown') -> str:\n    \"\"\"\n    Given a dict where the keys are weighted by the values, return the key with the highest value,\n    if it is above the given threshold (otherwise return the default value).\n    Arguments:\n    weighted_values -- A dict where the keys are weighted by the values.\n    threshold -- The threshold to use.\n    default -- The default value to return if the threshold is not met.\n    Returns:\n    The key with the highest value, if it is above the given threshold (otherwise return the default value).",
        "detail": "baselines.aggregators",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "baselines.process_single_file",
        "description": "baselines.process_single_file",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--yaml\", required=True, help=\"Path to the YAML file.\")\n    parser.add_argument(\"--raw_data_dirpath\", required=True, help=\"he path to the top data directory in the data \"\n                                                                  \"hierarchy (from which to mirror the output path)\")\n    parser.add_argument(\"--jsonl\", required=True, help=\"path to the input JSONL file with the text to be processed, \"\n                                                       \"relative to raw_data_dirpath\")\n    parser.add_argument(\"--source_name\", required=True, help=\"The name of the source of the jsonl file.\")\n    parser.add_argument(\"--output_dir\", required=True, help=\"Path to the output dir of the processed file.\")\n    parser.add_argument(\"--workers\", type=int, required=False, default=1,",
        "detail": "baselines.process_single_file",
        "documentation": {}
    },
    {
        "label": "split_train_val",
        "kind": 2,
        "importPath": "baselines.train_fasttext_classifier",
        "description": "baselines.train_fasttext_classifier",
        "peekOfCode": "def split_train_val(train_input, valid_frac):\n    with open(train_input, \"r\") as f:\n        lines = f.read().splitlines()\n    random.shuffle(lines)\n    num_train = int(len(lines) * (1-valid_frac))\n    train_lines = lines[:num_train]\n    valid_lines = lines[num_train:]\n    out_base, file_ext = os.path.splitext(train_input)\n    train_path = out_base + \"_train\" + file_ext\n    valid_path = out_base + \"_valid\" + file_ext",
        "detail": "baselines.train_fasttext_classifier",
        "documentation": {}
    },
    {
        "label": "print_results",
        "kind": 2,
        "importPath": "baselines.train_fasttext_classifier",
        "description": "baselines.train_fasttext_classifier",
        "peekOfCode": "def print_results(N, p, r):\n    print(\"Num samples\\t\" + str(N))\n    print(\"Precision@{}\\t{:.3f}\".format(1, p))\n    print(\"Recall@{}\\t{:.3f}\".format(1, r))\ndef get_args():\n    # I/O arguments\n    parser = argparse.ArgumentParser(description=\"Train fasttext classifiers for quality prediction.\")\n    parser.add_argument(\"--input\", help=\"Training file for fasttext classifier (in fasttext format).\", required=True)\n    parser.add_argument(\"--valid_input\", help='Data file for validating fasttext classifier', type=str)\n    parser.add_argument(\"--valid_frac\", help='Holdout ratio for validation. Will split from the training data.', type=float, default=0)",
        "detail": "baselines.train_fasttext_classifier",
        "documentation": {}
    },
    {
        "label": "get_args",
        "kind": 2,
        "importPath": "baselines.train_fasttext_classifier",
        "description": "baselines.train_fasttext_classifier",
        "peekOfCode": "def get_args():\n    # I/O arguments\n    parser = argparse.ArgumentParser(description=\"Train fasttext classifiers for quality prediction.\")\n    parser.add_argument(\"--input\", help=\"Training file for fasttext classifier (in fasttext format).\", required=True)\n    parser.add_argument(\"--valid_input\", help='Data file for validating fasttext classifier', type=str)\n    parser.add_argument(\"--valid_frac\", help='Holdout ratio for validation. Will split from the training data.', type=float, default=0)\n    parser.add_argument(\"--output_dir\", help=\"Output dir at which to save the model binary.\", default=\"./mappers/enrichers/quality_prediction_enrichment_models/\")\n    parser.add_argument(\"--name\", required=True, help=\"Name of the model to save.\")\n    parser.add_argument(\"--seed\", help=\"Random seed\", type=int, default=42)\n    # Hyperparameters for fasttext classifier. For more descriptions and additional hyperparmaeters, see (https://fasttext.cc/docs/en/options.html)",
        "detail": "baselines.train_fasttext_classifier",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "baselines.train_fasttext_classifier",
        "description": "baselines.train_fasttext_classifier",
        "peekOfCode": "def main():\n    args = get_args()\n    random.seed(args.seed)\n    train_input = args.input\n    train_was_split = False\n    if args.valid_input:\n        valid_input = args.valid_input\n    elif args.valid_frac > 0:\n        train_input, valid_input = split_train_val(train_input, args.valid_frac)\n        train_was_split = True",
        "detail": "baselines.train_fasttext_classifier",
        "documentation": {}
    },
    {
        "label": "gen_parser",
        "kind": 2,
        "importPath": "eval.aggregated_metrics",
        "description": "eval.aggregated_metrics",
        "peekOfCode": "def gen_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--eval_meta_data\", default=f\"{os.path.dirname(__file__)}/eval_meta_data.csv\", help=\"Eval meta data file\"\n    )\n    parser.add_argument(\n        \"--additional_aggregation\",\n        default=f\"{os.path.dirname(__file__)}/additional_aggregation.json\",\n        help=\"Eval aggregation file\",\n    )",
        "detail": "eval.aggregated_metrics",
        "documentation": {}
    },
    {
        "label": "get_aggregated_results",
        "kind": 2,
        "importPath": "eval.aggregated_metrics",
        "description": "eval.aggregated_metrics",
        "peekOfCode": "def get_aggregated_results(data, eval_metadata, aggregation_json):\n    data[\"missing tasks\"] = str(\n        [task for task in eval_metadata[\"Eval Task\"] if task not in data[\"eval_metrics\"][\"icl\"]]\n    )\n    eval_metadata[\"results\"] = eval_metadata[\"Eval Task\"].map(data[\"eval_metrics\"][\"icl\"])\n    eval_metadata[\"centered results\"] = (\n        eval_metadata[\"results\"].astype(float) - 0.01 * eval_metadata[\"Random baseline\"].astype(float)\n    ) / (1.0 - 0.01 * eval_metadata[\"Random baseline\"].astype(float))\n    result_df = eval_metadata.groupby(\"Task Category\").agg({\"centered results\": \"mean\"}).reset_index()\n    data[\"aggregated_task_categories_centered\"] = result_df.set_index(\"Task Category\").to_dict()[\"centered results\"]",
        "detail": "eval.aggregated_metrics",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "eval.aggregated_metrics",
        "description": "eval.aggregated_metrics",
        "peekOfCode": "def main():\n    parser = gen_parser()\n    args = parser.parse_args()\n    eval_metadata = pd.read_csv(args.eval_meta_data)\n    with open(args.eval_results, \"r\") as f:\n        data = json.load(f)\n    with open(args.additional_aggregation, \"r\") as f:\n        aggregation_json = json.load(f)\n    data = get_aggregated_results(data, eval_metadata, aggregation_json)\n    with open(args.eval_results, \"w\") as f:",
        "detail": "eval.aggregated_metrics",
        "documentation": {}
    },
    {
        "label": "setup_for_distributed",
        "kind": 2,
        "importPath": "eval.eval_openlm_ckpt",
        "description": "eval.eval_openlm_ckpt",
        "peekOfCode": "def setup_for_distributed(is_master):\n    def print(*args, **kwargs):\n        force = kwargs.pop(\"force\", False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n    __builtin__.print = print\ndef convert_gpqa(gpqa_dir, outdir):\n    os.makedirs(outdir, exist_ok=True)\n    for filename in [\"gpqa_main.csv\", \"gpqa_diamond.csv\", \"gpqa_extended.csv\"]:\n        inpath = os.path.join(gpqa_dir, filename)",
        "detail": "eval.eval_openlm_ckpt",
        "documentation": {}
    },
    {
        "label": "convert_gpqa",
        "kind": 2,
        "importPath": "eval.eval_openlm_ckpt",
        "description": "eval.eval_openlm_ckpt",
        "peekOfCode": "def convert_gpqa(gpqa_dir, outdir):\n    os.makedirs(outdir, exist_ok=True)\n    for filename in [\"gpqa_main.csv\", \"gpqa_diamond.csv\", \"gpqa_extended.csv\"]:\n        inpath = os.path.join(gpqa_dir, filename)\n        outpath = os.path.join(outdir, Path(inpath).with_suffix(\".jsonl\").name)\n        with open(outpath, \"w\") as f:\n            df = pd.read_csv(inpath)\n            rng = random.Random(42)\n            for i in range(df.shape[0]):\n                question = df[\"Question\"][i]",
        "detail": "eval.eval_openlm_ckpt",
        "documentation": {}
    },
    {
        "label": "check_and_download_data",
        "kind": 2,
        "importPath": "eval.eval_openlm_ckpt",
        "description": "eval.eval_openlm_ckpt",
        "peekOfCode": "def check_and_download_data():\n    if not os.path.exists(\"local_data\"):\n        current_dir = os.path.dirname(os.path.realpath(__file__))\n        if os.path.exists(f\"{current_dir}/local_data\"):\n            shutil.copytree(f\"{current_dir}/local_data\", \"local_data\")\n        else:\n            if dist.get_global_rank() == 0:\n                print(\"local_data folder does not exist. Running bash script...\")\n                script_path = os.path.join(current_dir, \"download_eval_data.sh\")\n                subprocess.call([script_path])",
        "detail": "eval.eval_openlm_ckpt",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "eval.eval_openlm_ckpt",
        "description": "eval.eval_openlm_ckpt",
        "peekOfCode": "def evaluate(model, tokenizer, cfg):\n    cfg.dist_timeout = cfg.get(\"dist_timeout\", 600.0)\n    reproducibility.seed_all(cfg.seed)\n    dist.initialize_dist(get_device(None), timeout=cfg.dist_timeout)\n    setup_for_distributed(dist.get_global_rank() == 0)\n    # Check if the data is downloaded, if not, download it.\n    check_and_download_data()\n    composer_model = SimpleComposerOpenLMCausalLM(model, tokenizer)\n    icl_tasks_w_categories = list(\n        filter(lambda x: 0 if \"has_categories\" not in x else x[\"has_categories\"], cfg.icl_tasks)",
        "detail": "eval.eval_openlm_ckpt",
        "documentation": {}
    },
    {
        "label": "set_args_for_val",
        "kind": 2,
        "importPath": "eval.eval_openlm_ckpt",
        "description": "eval.eval_openlm_ckpt",
        "peekOfCode": "def set_args_for_val(args, data, key):\n    setattr(args, \"val_data\", data)\n    setattr(args, \"val_data_key\", key)\n    setattr(args, \"squash_mask_left\", True)\n    setattr(args, \"target_mask_individual\", 50400)\n    setattr(args, \"target_mask_left\", 50300)\n    setattr(args, \"val_seq_ci\", True)\n    setattr(args, \"val_tok_ci\", True)\n    return args\ndef dump_or_update_output(args, local_rank, eval_metrics=None, helm_eval_metrics=None, helm_reference_uuid=None):",
        "detail": "eval.eval_openlm_ckpt",
        "documentation": {}
    },
    {
        "label": "dump_or_update_output",
        "kind": 2,
        "importPath": "eval.eval_openlm_ckpt",
        "description": "eval.eval_openlm_ckpt",
        "peekOfCode": "def dump_or_update_output(args, local_rank, eval_metrics=None, helm_eval_metrics=None, helm_reference_uuid=None):\n    date_format = \"%Y_%m_%d-%H_%M_%S\"\n    date = datetime.now(tz=pytz.utc)\n    date = date.astimezone(timezone(\"US/Pacific\"))\n    date = date.strftime(date_format)\n    output = {\n        \"uuid\": helm_reference_uuid if helm_reference_uuid else str(uuid.uuid4()),\n        \"model\": args.model,\n        \"creation_date\": date,\n    }",
        "detail": "eval.eval_openlm_ckpt",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "eval.eval_openlm_ckpt",
        "description": "eval.eval_openlm_ckpt",
        "peekOfCode": "def main():\n    \"\"\"\n    Usage:\n    python eval_openlm_ckpt.py --checkpoint <path_to_openlm_checkpoint>  --model <name_of_model_config> --eval-yaml <path_to_eval_yaml> --tokenizer <tokenizer_name_or_path>\n    example:\n    cd eval\n    python eval_openlm_ckpt.py --checkpoint ../checkpoints/llama2_7b.pt --model llama2_7b.json --eval-yaml in_memory_hf_eval.yaml --tokenizer <path_to_tokenizer>\n    multi-gpu example:\n    cd eval\n    torchrun --nproc_per_node 3 eval_openlm_ckpt.py --checkpoint ../checkpoints/llama2_7b.pt --model llama2_7b.json --eval-yaml in_memory_hf_eval.yaml --tokenizer <path_to_tokenizer>",
        "detail": "eval.eval_openlm_ckpt",
        "documentation": {}
    },
    {
        "label": "builtin_print",
        "kind": 5,
        "importPath": "eval.eval_openlm_ckpt",
        "description": "eval.eval_openlm_ckpt",
        "peekOfCode": "builtin_print = __builtin__.print\ndef setup_for_distributed(is_master):\n    def print(*args, **kwargs):\n        force = kwargs.pop(\"force\", False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n    __builtin__.print = print\ndef convert_gpqa(gpqa_dir, outdir):\n    os.makedirs(outdir, exist_ok=True)\n    for filename in [\"gpqa_main.csv\", \"gpqa_diamond.csv\", \"gpqa_extended.csv\"]:",
        "detail": "eval.eval_openlm_ckpt",
        "documentation": {}
    },
    {
        "label": "submit_to_slack",
        "kind": 2,
        "importPath": "eval.submit",
        "description": "eval.submit",
        "peekOfCode": "def submit_to_slack(filename):\n    data = json.load(open(filename))\n    score = data.get(\"aggregated_centered_results\", -1)\n    low_var_score = data.get(\"low_variance_datasets\", -1)\n    mmlu = data[\"eval_metrics\"][\"icl\"].get(\"mmlu_fewshot\", -1)\n    name = data[\"name\"]\n    model = data[\"model\"]\n    uuid = data[\"uuid\"]\n    url = f\"https://github.com/mlfoundations/dcnlp/tree/main/{filename}\"\n    message = f\"New submission ({model}). Low Variance Score: {low_var_score:.4f}., Aggregated centered score: {score:.4f}. MMLU 5-shot Score: {mmlu: .4f}. Name: {name}. UUID: {uuid}. Full results at {url}\"",
        "detail": "eval.submit",
        "documentation": {}
    },
    {
        "label": "update_args_from_openlm_config",
        "kind": 2,
        "importPath": "eval.utils",
        "description": "eval.utils",
        "peekOfCode": "def update_args_from_openlm_config(args):\n    with open(args.config, \"r\") as f:\n        config = yaml.safe_load(f)\n    for k, v in config.items():\n        if k == \"model\" and args.model != None:\n            continue\n        if v == \"None\":\n            v = None\n        # we changed args\n        if k == \"batch_size\":",
        "detail": "eval.utils",
        "documentation": {}
    },
    {
        "label": "tar_to_entries",
        "kind": 2,
        "importPath": "ray_processing.dedup_jsonl",
        "description": "ray_processing.dedup_jsonl",
        "peekOfCode": "def tar_to_entries(\n    batch, input_overlap, content_key=\"text\", normalize=None, selection_key=None, selection_normalize=None\n):\n    all_rows = {\"uid\": [], \"s3_filename\": [], \"local_index\": []}\n    if selection_key is not None:\n        all_rows[selection_key] = []\n    for idx in range(len(batch[\"bytes\"])):\n        if batch[\"path\"][idx].endswith(\".zstd\"):\n            with zstd.ZstdDecompressor().stream_reader(batch[\"bytes\"][idx]) as reader:\n                batch[\"bytes\"][idx] = reader.read()",
        "detail": "ray_processing.dedup_jsonl",
        "documentation": {}
    },
    {
        "label": "get_dupe_rows",
        "kind": 2,
        "importPath": "ray_processing.dedup_jsonl",
        "description": "ray_processing.dedup_jsonl",
        "peekOfCode": "def get_dupe_rows(g, selection_key=None, reverse=False):\n    # assumption that number of duplicates in group are small and hence can call min/max\n    # return list of duplicates outside of keep_idx for future removal\n    if len(g) == 1:\n        return {}\n    if selection_key is None:\n        keep_idx = 0\n    else:\n        keep_fn = np.argmax if reverse else np.argmin\n        keep_idx = keep_fn(g[selection_key])",
        "detail": "ray_processing.dedup_jsonl",
        "documentation": {}
    },
    {
        "label": "drop_dupe_rows",
        "kind": 2,
        "importPath": "ray_processing.dedup_jsonl",
        "description": "ray_processing.dedup_jsonl",
        "peekOfCode": "def drop_dupe_rows(g, output_path, content_key, input_overlap, local_stats_dir):\n    # Download s3_filename from S3.\n    # Remove duplicates in local_index.\n    # Upload back to s3.\n    # (make sure not to upload directly back to the same filename)\n    drop_indices = g[\"local_index\"]\n    s3_filename = input_overlap + g[\"s3_filename\"][0]\n    input_parts = s3_filename.replace(\"s3://\", \"\").split(\"/\")\n    bucket = input_parts.pop(0)\n    key = \"/\".join(input_parts)",
        "detail": "ray_processing.dedup_jsonl",
        "documentation": {}
    },
    {
        "label": "write_unmodified_local_stats",
        "kind": 2,
        "importPath": "ray_processing.dedup_jsonl",
        "description": "ray_processing.dedup_jsonl",
        "peekOfCode": "def write_unmodified_local_stats(s3_filepath, local_stats_dir, input_overlap, content_key):\n    s3_filepath = s3_filepath.replace(\"s3://\", \"\").replace(input_overlap, \"\")\n    shard_name = s3_filepath.replace(\"_processed.jsonl\", \".jsonl\").split(\".jsonl\")[0]\n    stats_out_path = os.path.join(local_stats_dir, shard_name.lstrip(\"/\") + \"_stats.jsonl\")\n    write_jsonl(\n        [{\"name\": \"exact_dedup\", \"content_key\": content_key, \"pages_in\": \"no_op\", \"pages_out\": \"no_op\"}],\n        stats_out_path,\n        \"a\",\n    )\ndef dedup_jsonl(",
        "detail": "ray_processing.dedup_jsonl",
        "documentation": {}
    },
    {
        "label": "dedup_jsonl",
        "kind": 2,
        "importPath": "ray_processing.dedup_jsonl",
        "description": "ray_processing.dedup_jsonl",
        "peekOfCode": "def dedup_jsonl(\n    input_dir,\n    shard_files=None,\n    base_output_path=None,\n    working_dir=None,\n    sync_to_input=False,\n    content_key=\"text\",\n    normalize=None,\n    selection_key=None,\n    selection_normalize=None,",
        "detail": "ray_processing.dedup_jsonl",
        "documentation": {}
    },
    {
        "label": "split_helper",
        "kind": 5,
        "importPath": "ray_processing.dedup_jsonl",
        "description": "ray_processing.dedup_jsonl",
        "peekOfCode": "split_helper = lambda ck: ck if isinstance(ck, list) else [ck]\njoin_helper = lambda ck: ck[0]\ndef tar_to_entries(\n    batch, input_overlap, content_key=\"text\", normalize=None, selection_key=None, selection_normalize=None\n):\n    all_rows = {\"uid\": [], \"s3_filename\": [], \"local_index\": []}\n    if selection_key is not None:\n        all_rows[selection_key] = []\n    for idx in range(len(batch[\"bytes\"])):\n        if batch[\"path\"][idx].endswith(\".zstd\"):",
        "detail": "ray_processing.dedup_jsonl",
        "documentation": {}
    },
    {
        "label": "join_helper",
        "kind": 5,
        "importPath": "ray_processing.dedup_jsonl",
        "description": "ray_processing.dedup_jsonl",
        "peekOfCode": "join_helper = lambda ck: ck[0]\ndef tar_to_entries(\n    batch, input_overlap, content_key=\"text\", normalize=None, selection_key=None, selection_normalize=None\n):\n    all_rows = {\"uid\": [], \"s3_filename\": [], \"local_index\": []}\n    if selection_key is not None:\n        all_rows[selection_key] = []\n    for idx in range(len(batch[\"bytes\"])):\n        if batch[\"path\"][idx].endswith(\".zstd\"):\n            with zstd.ZstdDecompressor().stream_reader(batch[\"bytes\"][idx]) as reader:",
        "detail": "ray_processing.dedup_jsonl",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "ray_processing.process",
        "description": "ray_processing.process",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--source_ref_paths\", help=\"paths to untokenized datasets refs, comma or space separated\", type=str, nargs=\"+\"\n    )\n    parser.add_argument(\n        \"--raw_data_dirpath\",\n        help=\"the path to the top data directory in the data hierarchy (from which to mirror the output path)\",\n    )\n    parser.add_argument(\"--shard_list_file\", type=str, default=None, help=\"Path to a file containing a list of input shards.\")",
        "detail": "ray_processing.process",
        "documentation": {}
    },
    {
        "label": "process_local_chunk",
        "kind": 2,
        "importPath": "ray_processing.process",
        "description": "ray_processing.process",
        "peekOfCode": "def process_local_chunk(\n    config_data, raw_data_dirpath, jsonl_relpath, source_name, base_output_path, workers, overwrite\n):\n    try:\n        _, _, pages_in, pages_out = process_single_file(\n            config_data=config_data,\n            raw_data_dirpath=raw_data_dirpath,\n            jsonl_relpath=jsonl_relpath,\n            source_name=source_name,\n            base_output_path=base_output_path,",
        "detail": "ray_processing.process",
        "documentation": {}
    },
    {
        "label": "to_iterator",
        "kind": 2,
        "importPath": "ray_processing.process",
        "description": "ray_processing.process",
        "peekOfCode": "def to_iterator(obj_ids, batch_size=100):\n    while obj_ids:\n        done, obj_ids = ray.wait(obj_ids, num_returns=min(batch_size, len(obj_ids)))\n        for d in done:\n            yield ray.get(d)\ndef list_shard_files(data_dirpath, num_shards=None, shard_list_file=None, shard_list_filters=None):\n    assert bool(shard_list_file) ^ bool(data_dirpath), \"Either shard_list_file or data_dirpath must be provided, but not both.\"\n    def get_files_in_directory(data_dirpath):\n        # 获取指定目录下的所有文件\n        files = [f for f in os.listdir(data_dirpath) if all(s not in f for s in ['stats', 'global_stats.json'])]",
        "detail": "ray_processing.process",
        "documentation": {}
    },
    {
        "label": "list_shard_files",
        "kind": 2,
        "importPath": "ray_processing.process",
        "description": "ray_processing.process",
        "peekOfCode": "def list_shard_files(data_dirpath, num_shards=None, shard_list_file=None, shard_list_filters=None):\n    assert bool(shard_list_file) ^ bool(data_dirpath), \"Either shard_list_file or data_dirpath must be provided, but not both.\"\n    def get_files_in_directory(data_dirpath):\n        # 获取指定目录下的所有文件\n        files = [f for f in os.listdir(data_dirpath) if all(s not in f for s in ['stats', 'global_stats.json'])]\n        return files\n    if shard_list_file is not None:\n        with open(shard_list_file, \"r\") as f:\n            shard_files = f.read().splitlines()\n    elif is_s3(data_dirpath):",
        "detail": "ray_processing.process",
        "documentation": {}
    },
    {
        "label": "RAY_CHUNK_SUCCESS",
        "kind": 5,
        "importPath": "ray_processing.process",
        "description": "ray_processing.process",
        "peekOfCode": "RAY_CHUNK_SUCCESS = 1\nRAY_CHUNK_FAILURE = 0\nLOCAL_CHUNK = \"local\"\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--source_ref_paths\", help=\"paths to untokenized datasets refs, comma or space separated\", type=str, nargs=\"+\"\n    )\n    parser.add_argument(\n        \"--raw_data_dirpath\",",
        "detail": "ray_processing.process",
        "documentation": {}
    },
    {
        "label": "RAY_CHUNK_FAILURE",
        "kind": 5,
        "importPath": "ray_processing.process",
        "description": "ray_processing.process",
        "peekOfCode": "RAY_CHUNK_FAILURE = 0\nLOCAL_CHUNK = \"local\"\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--source_ref_paths\", help=\"paths to untokenized datasets refs, comma or space separated\", type=str, nargs=\"+\"\n    )\n    parser.add_argument(\n        \"--raw_data_dirpath\",\n        help=\"the path to the top data directory in the data hierarchy (from which to mirror the output path)\",",
        "detail": "ray_processing.process",
        "documentation": {}
    },
    {
        "label": "LOCAL_CHUNK",
        "kind": 5,
        "importPath": "ray_processing.process",
        "description": "ray_processing.process",
        "peekOfCode": "LOCAL_CHUNK = \"local\"\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--source_ref_paths\", help=\"paths to untokenized datasets refs, comma or space separated\", type=str, nargs=\"+\"\n    )\n    parser.add_argument(\n        \"--raw_data_dirpath\",\n        help=\"the path to the top data directory in the data hierarchy (from which to mirror the output path)\",\n    )",
        "detail": "ray_processing.process",
        "documentation": {}
    },
    {
        "label": "add_tokenize_shuffle_args",
        "kind": 2,
        "importPath": "ray_processing.tokenize_shuffle",
        "description": "ray_processing.tokenize_shuffle",
        "peekOfCode": "def add_tokenize_shuffle_args(parser):\n    # Args to be fed into tokenize_shuffle\n    parser.add_argument(\"--input\", help=\"input path\", type=str)\n    parser.add_argument(\"--output\", help=\"output path\", type=str, required=True)\n    parser.add_argument(\"--content_key\", type=str, default=\"text\")\n    parser.add_argument(\"--seqlen\", type=int, default=2048)\n    parser.add_argument(\"--tokenizer\", type=str, default=\"EleutherAI/gpt-neox-20b\")\n    parser.add_argument(\"--wds_chunk_size\", type=int, default=8192)\n    parser.add_argument(\"--seed\", type=int, default=42)\n    parser.add_argument(\"--subset\", type=int, default=None)",
        "detail": "ray_processing.tokenize_shuffle",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "ray_processing.tokenize_shuffle",
        "description": "ray_processing.tokenize_shuffle",
        "peekOfCode": "def main(args, dcnlp_arg_names):\n    # Before proceeding with tokenization, make sure that an existing json won't be overwritten\n    json_path = f\"exp_data/datasets/tokenized/{args.readable_name}.json\"\n    if not args.overwrite:\n        assert not os.path.exists(\n            json_path\n        ), f\"{json_path} already exists. Try changing --readable_name or deleting the existing file.\"\n    # Collect the dataset urls from the source reference json paths, if no explicity jsons provided, tries to search for them based on --input\n    source_refs = None\n    if args.source_ref_paths is not None:",
        "detail": "ray_processing.tokenize_shuffle",
        "documentation": {}
    },
    {
        "label": "DIR",
        "kind": 5,
        "importPath": "ray_processing.tokenize_shuffle",
        "description": "ray_processing.tokenize_shuffle",
        "peekOfCode": "DIR = pathlib.Path(__file__).parent.absolute()\ndef add_tokenize_shuffle_args(parser):\n    # Args to be fed into tokenize_shuffle\n    parser.add_argument(\"--input\", help=\"input path\", type=str)\n    parser.add_argument(\"--output\", help=\"output path\", type=str, required=True)\n    parser.add_argument(\"--content_key\", type=str, default=\"text\")\n    parser.add_argument(\"--seqlen\", type=int, default=2048)\n    parser.add_argument(\"--tokenizer\", type=str, default=\"EleutherAI/gpt-neox-20b\")\n    parser.add_argument(\"--wds_chunk_size\", type=int, default=8192)\n    parser.add_argument(\"--seed\", type=int, default=42)",
        "detail": "ray_processing.tokenize_shuffle",
        "documentation": {}
    },
    {
        "label": "get_source_ref_by_key",
        "kind": 2,
        "importPath": "ray_processing.utils",
        "description": "ray_processing.utils",
        "peekOfCode": "def get_source_ref_by_key(search_value, key=\"name\", tokenized=False):\n    for f in glob.glob(f\"{DATASET_REFS_DIR}/*/*.json\"):\n        with open(f, \"r\") as file:\n            ref = json.load(open(f, \"r\"))\n            if ref.get(key, None) == search_value and ref.get(\"tokenized\", None) == tokenized:\n                return ref\n    return None\ndef get_source_ref(source_ref_path):\n    with open(source_ref_path, \"r\") as file:\n        return json.load(file)",
        "detail": "ray_processing.utils",
        "documentation": {}
    },
    {
        "label": "get_source_ref",
        "kind": 2,
        "importPath": "ray_processing.utils",
        "description": "ray_processing.utils",
        "peekOfCode": "def get_source_ref(source_ref_path):\n    with open(source_ref_path, \"r\") as file:\n        return json.load(file)\ndef count_tokens(manifest_url, seqlen=2049):\n    with S3Path(manifest_url).open(\"r\") as f:\n        manifest = [json.loads(line) for line in f]\n    num_tokens = sum(int(line[\"num_sequences\"]) for line in manifest) * seqlen\n    return num_tokens\ndef get_local_dir_size(directory):\n    total_size = 0",
        "detail": "ray_processing.utils",
        "documentation": {}
    },
    {
        "label": "count_tokens",
        "kind": 2,
        "importPath": "ray_processing.utils",
        "description": "ray_processing.utils",
        "peekOfCode": "def count_tokens(manifest_url, seqlen=2049):\n    with S3Path(manifest_url).open(\"r\") as f:\n        manifest = [json.loads(line) for line in f]\n    num_tokens = sum(int(line[\"num_sequences\"]) for line in manifest) * seqlen\n    return num_tokens\ndef get_local_dir_size(directory):\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(directory):\n        for filename in filenames:\n            filepath = os.path.join(dirpath, filename)",
        "detail": "ray_processing.utils",
        "documentation": {}
    },
    {
        "label": "get_local_dir_size",
        "kind": 2,
        "importPath": "ray_processing.utils",
        "description": "ray_processing.utils",
        "peekOfCode": "def get_local_dir_size(directory):\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(directory):\n        for filename in filenames:\n            filepath = os.path.join(dirpath, filename)\n            total_size += os.path.getsize(filepath)\n    return total_size\ndef get_s3_dir_size(dataset_path):\n    if not is_s3(dataset_path):\n        return get_local_dir_size(dataset_path)",
        "detail": "ray_processing.utils",
        "documentation": {}
    },
    {
        "label": "get_s3_dir_size",
        "kind": 2,
        "importPath": "ray_processing.utils",
        "description": "ray_processing.utils",
        "peekOfCode": "def get_s3_dir_size(dataset_path):\n    if not is_s3(dataset_path):\n        return get_local_dir_size(dataset_path)\n    bucket, prefix = dataset_path.replace(\"s3://\", \"\").split(\"/\", 1)\n    total_size = 0\n    for i, obj in enumerate(boto3.resource(\"s3\").Bucket(bucket).objects.filter(Prefix=prefix)):\n        total_size += obj.size\n    return total_size\ndef get_git_info():\n    repo = git.Repo(search_parent_directories=True)",
        "detail": "ray_processing.utils",
        "documentation": {}
    },
    {
        "label": "get_git_info",
        "kind": 2,
        "importPath": "ray_processing.utils",
        "description": "ray_processing.utils",
        "peekOfCode": "def get_git_info():\n    repo = git.Repo(search_parent_directories=True)\n    dcnlp_commit_hash = repo.head.object.hexsha\n    dcnlp_diff = repo.git.diff(repo.head.commit.tree)\n    return dcnlp_commit_hash, dcnlp_diff\ndef generate_untokenized_dataset_json(args, source_refs, base_output_path, data_key=\".json.zstd\"):\n    sources = [{\"uuid\": s[\"uuid\"], \"name\": s[\"name\"]} for s in source_refs] if source_refs else []\n    dcnlp_commit_hash, dcnlp_diff = get_git_info()\n    dataset_json = {\n        \"uuid\": str(uuid.uuid4().__str__()),",
        "detail": "ray_processing.utils",
        "documentation": {}
    },
    {
        "label": "generate_untokenized_dataset_json",
        "kind": 2,
        "importPath": "ray_processing.utils",
        "description": "ray_processing.utils",
        "peekOfCode": "def generate_untokenized_dataset_json(args, source_refs, base_output_path, data_key=\".json.zstd\"):\n    sources = [{\"uuid\": s[\"uuid\"], \"name\": s[\"name\"]} for s in source_refs] if source_refs else []\n    dcnlp_commit_hash, dcnlp_diff = get_git_info()\n    dataset_json = {\n        \"uuid\": str(uuid.uuid4().__str__()),\n        \"name\": args.readable_name,\n        \"creation_date\": datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\"),\n        \"dataset_url\": os.path.join(base_output_path, \"processed_data/\"),\n        \"manifest_url\": None,\n        \"sources\": sources,",
        "detail": "ray_processing.utils",
        "documentation": {}
    },
    {
        "label": "generate_tokenized_dataset_json",
        "kind": 2,
        "importPath": "ray_processing.utils",
        "description": "ray_processing.utils",
        "peekOfCode": "def generate_tokenized_dataset_json(args, source_refs, data_key=\"json.gz\"):\n    manifest_url = os.path.join(args.output.rstrip(\"/\"), \"manifest.jsonl\")\n    dcnlp_commit_hash, dcnlp_diff = get_git_info()\n    sources = [{\"uuid\": s[\"uuid\"], \"name\": s[\"name\"]} for s in source_refs] if source_refs else []\n    # TODO: Currently I just dump the entire yaml, is this the best thing to do?\n    # Also, maybe would be nice to support automated generation of this yaml given input sources + weights\n    sampling_yaml = None\n    if args.do_sample:\n        with open(args.default_dataset_yaml, \"r\") as file:\n            sampling_yaml = yaml.safe_load(file)",
        "detail": "ray_processing.utils",
        "documentation": {}
    },
    {
        "label": "DATASET_REFS_DIR",
        "kind": 5,
        "importPath": "ray_processing.utils",
        "description": "ray_processing.utils",
        "peekOfCode": "DATASET_REFS_DIR = os.path.join(\n    pathlib.Path(__file__).parent.parent.absolute(),\n    \"exp_data\",\n    \"datasets\",\n)\ndef get_source_ref_by_key(search_value, key=\"name\", tokenized=False):\n    for f in glob.glob(f\"{DATASET_REFS_DIR}/*/*.json\"):\n        with open(f, \"r\") as file:\n            ref = json.load(open(f, \"r\"))\n            if ref.get(key, None) == search_value and ref.get(\"tokenized\", None) == tokenized:",
        "detail": "ray_processing.utils",
        "documentation": {}
    },
    {
        "label": "s3_setup_teardown",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def s3_setup_teardown():\n    with mock_s3():\n        boto3.client('s3').create_bucket(Bucket=S3_TEST_BUCKET)\n        yield\n@pytest.mark.parametrize(\"file_path\", [\"/tmp/test.jsonl\", S3_TEST_PATH])\ndef test_read_write_jsonl(file_path, s3_setup_teardown):\n    # Given\n    data_to_write = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}]\n    # When\n    write_jsonl(data_to_write, file_path)",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "test_read_write_jsonl",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def test_read_write_jsonl(file_path, s3_setup_teardown):\n    # Given\n    data_to_write = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}]\n    # When\n    write_jsonl(data_to_write, file_path)\n    # Then\n    read_data = list(read_jsonl(file_path))\n    assert read_data == data_to_write\ndef test_is_s3():\n    assert is_s3(\"s3://bucket/path\")",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "test_is_s3",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def test_is_s3():\n    assert is_s3(\"s3://bucket/path\")\n    assert not is_s3(\"/local/path/to/file\")\n    assert not is_s3(\"path/to/file\")\ndef test_makedirs_if_missing_local():\n    local_path = \"/tmp/test_dir\"\n    # Ensure directory is not present initially\n    if os.path.isdir(local_path):\n        shutil.rmtree(local_path)\n    makedirs_if_missing(local_path)",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "test_makedirs_if_missing_local",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def test_makedirs_if_missing_local():\n    local_path = \"/tmp/test_dir\"\n    # Ensure directory is not present initially\n    if os.path.isdir(local_path):\n        shutil.rmtree(local_path)\n    makedirs_if_missing(local_path)\n    assert os.path.isdir(local_path)\n    # Cleanup\n    shutil.rmtree(local_path)\ndef test_makedirs_if_missing_s3():",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "test_makedirs_if_missing_s3",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def test_makedirs_if_missing_s3():\n    s3_path = \"s3://my-bucket/path/to/file.txt\"\n    makedirs_if_missing(s3_path)\n    # For S3, we can't assert directory creation since it's a no-op.\n    # We're mainly checking that no errors are raised.\n# Mock the S3 environment\n@pytest.fixture\ndef mock_s3_bucket():\n    with mock_s3():\n        s3 = boto3.resource('s3', region_name='us-east-1')",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "mock_s3_bucket",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def mock_s3_bucket():\n    with mock_s3():\n        s3 = boto3.resource('s3', region_name='us-east-1')\n        s3.create_bucket(Bucket='test-bucket')\n        yield\ndef test_delete_local_file(tmp_path):\n    file = tmp_path / \"test.txt\"\n    file.write_text(\"Hello, World!\")\n    assert file.exists()\n    delete_file(str(file))",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "test_delete_local_file",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def test_delete_local_file(tmp_path):\n    file = tmp_path / \"test.txt\"\n    file.write_text(\"Hello, World!\")\n    assert file.exists()\n    delete_file(str(file))\n    assert not file.exists()\n@pytest.mark.usefixtures('mock_s3_bucket')\ndef test_delete_s3_file():\n    file_path = \"s3://test-bucket/test.txt\"\n    s3_path = S3Path(file_path)",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "test_delete_s3_file",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def test_delete_s3_file():\n    file_path = \"s3://test-bucket/test.txt\"\n    s3_path = S3Path(file_path)\n    s3_path.write_text(\"Hello, S3!\")\n    assert s3_path.exists()\n    delete_file(file_path)\n    assert not s3_path.exists()\n@pytest.mark.usefixtures('mock_s3_bucket')\ndef test_delete_nonexistent_s3_file():\n    file_path = \"s3://test-bucket/nonexistent.txt\"",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "test_delete_nonexistent_s3_file",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def test_delete_nonexistent_s3_file():\n    file_path = \"s3://test-bucket/nonexistent.txt\"\n    s3_path = S3Path(file_path)\n    assert not s3_path.exists()\n    with pytest.raises(FileNotFoundError):\n        delete_file(file_path)\ndef test_delete_nonexistent_local_file(tmp_path):\n    file = tmp_path / \"nonexistent.txt\"\n    assert not file.exists()\n    with pytest.raises(FileNotFoundError):",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "test_delete_nonexistent_local_file",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def test_delete_nonexistent_local_file(tmp_path):\n    file = tmp_path / \"nonexistent.txt\"\n    assert not file.exists()\n    with pytest.raises(FileNotFoundError):\n        delete_file(str(file))\ndef test_exists_local_file(tmp_path):\n    file = tmp_path / \"test.txt\"\n    file.write_text(\"Hello, World!\")\n    assert is_exists(str(file)) == True\ndef test_not_exists_local_file(tmp_path):",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "test_exists_local_file",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def test_exists_local_file(tmp_path):\n    file = tmp_path / \"test.txt\"\n    file.write_text(\"Hello, World!\")\n    assert is_exists(str(file)) == True\ndef test_not_exists_local_file(tmp_path):\n    file = tmp_path / \"nonexistent.txt\"\n    assert is_exists(str(file)) == False\n@pytest.mark.usefixtures('mock_s3_bucket')\ndef test_exists_s3_file():\n    file_path = \"s3://test-bucket/test.txt\"",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "test_not_exists_local_file",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def test_not_exists_local_file(tmp_path):\n    file = tmp_path / \"nonexistent.txt\"\n    assert is_exists(str(file)) == False\n@pytest.mark.usefixtures('mock_s3_bucket')\ndef test_exists_s3_file():\n    file_path = \"s3://test-bucket/test.txt\"\n    s3_path = S3Path(file_path)\n    s3_path.write_text(\"Hello, S3!\")\n    assert is_exists(file_path) == True\n@pytest.mark.usefixtures('mock_s3_bucket')",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "test_exists_s3_file",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def test_exists_s3_file():\n    file_path = \"s3://test-bucket/test.txt\"\n    s3_path = S3Path(file_path)\n    s3_path.write_text(\"Hello, S3!\")\n    assert is_exists(file_path) == True\n@pytest.mark.usefixtures('mock_s3_bucket')\ndef test_not_exists_s3_file():\n    file_path = \"s3://test-bucket/nonexistent.txt\"\n    assert is_exists(file_path) == False\ndef test_list_local_dir(tmp_path):",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "test_not_exists_s3_file",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def test_not_exists_s3_file():\n    file_path = \"s3://test-bucket/nonexistent.txt\"\n    assert is_exists(file_path) == False\ndef test_list_local_dir(tmp_path):\n    # Create some files, including hidden ones\n    (tmp_path / \"file1.txt\").write_text(CONTENT)\n    (tmp_path / \".hidden_file\").write_text(\"hidden content\")\n    listed_files = list_dir(str(tmp_path))\n    assert len(listed_files) == 1\n    assert \"file1.txt\" in listed_files[0]",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "test_list_local_dir",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def test_list_local_dir(tmp_path):\n    # Create some files, including hidden ones\n    (tmp_path / \"file1.txt\").write_text(CONTENT)\n    (tmp_path / \".hidden_file\").write_text(\"hidden content\")\n    listed_files = list_dir(str(tmp_path))\n    assert len(listed_files) == 1\n    assert \"file1.txt\" in listed_files[0]\n@pytest.mark.usefixtures('mock_s3_bucket')\ndef test_list_s3_dir():\n    # Create some files in S3, including hidden ones",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "test_list_s3_dir",
        "kind": 2,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "def test_list_s3_dir():\n    # Create some files in S3, including hidden ones\n    s3_dir = S3Path(\"s3://test-bucket/dir/\")\n    (s3_dir / \"file1.txt\").write_text(CONTENT)\n    (s3_dir / \".hidden_file\").write_text(\"hidden content\")\n    listed_files = list_dir(\"s3://test-bucket/dir/\")\n    print(listed_files)  # For debugging\n    assert len(listed_files) == 1\n    assert \"file1.txt\" in listed_files[0]",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "S3_TEST_BUCKET",
        "kind": 5,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "S3_TEST_BUCKET = \"test-bucket\"\nS3_TEST_PATH = f\"s3://{S3_TEST_BUCKET}/test.jsonl\"\n@pytest.fixture(scope=\"function\")\ndef s3_setup_teardown():\n    with mock_s3():\n        boto3.client('s3').create_bucket(Bucket=S3_TEST_BUCKET)\n        yield\n@pytest.mark.parametrize(\"file_path\", [\"/tmp/test.jsonl\", S3_TEST_PATH])\ndef test_read_write_jsonl(file_path, s3_setup_teardown):\n    # Given",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "S3_TEST_PATH",
        "kind": 5,
        "importPath": "tests.baselines.core.test_file_utils",
        "description": "tests.baselines.core.test_file_utils",
        "peekOfCode": "S3_TEST_PATH = f\"s3://{S3_TEST_BUCKET}/test.jsonl\"\n@pytest.fixture(scope=\"function\")\ndef s3_setup_teardown():\n    with mock_s3():\n        boto3.client('s3').create_bucket(Bucket=S3_TEST_BUCKET)\n        yield\n@pytest.mark.parametrize(\"file_path\", [\"/tmp/test.jsonl\", S3_TEST_PATH])\ndef test_read_write_jsonl(file_path, s3_setup_teardown):\n    # Given\n    data_to_write = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}]",
        "detail": "tests.baselines.core.test_file_utils",
        "documentation": {}
    },
    {
        "label": "min_character_filter",
        "kind": 2,
        "importPath": "tests.baselines.data.custom_mappers",
        "description": "tests.baselines.data.custom_mappers",
        "peekOfCode": "def min_character_filter(page: Dict, min_characters: int = 20) -> List[Dict]:\n    \"\"\"\n    Filters the input JSON object based on the number of characters in the CONTENT field.\n    This function returns a list containing the input JSON object if the number of characters\n    in the CONTENT field is greater than or equal to `min_characters`. Otherwise, it returns\n    an empty list.\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.\n    min_characters -- The minimum number of characters required for the input JSON object",
        "detail": "tests.baselines.data.custom_mappers",
        "documentation": {}
    },
    {
        "label": "mock_text_type_enricher",
        "kind": 2,
        "importPath": "tests.baselines.data.custom_mappers",
        "description": "tests.baselines.data.custom_mappers",
        "peekOfCode": "def mock_text_type_enricher(page: Dict, key: str = \"text_type\", overwrite: bool = False) -> List[Dict]:\n    \"\"\"\n    Enriches the input JSON object with a new field that indicates the type of text.\n    Arguments:\n    page -- A dictionary representing a JSON object. It should have a CONTENT field\n            that contains the text to be analyzed.\n    key -- The name of the new field to be added to the input JSON object.\n    overwrite -- Whether to overwrite the new field if it already exists in the input JSON object.\n    Returns:\n    A list containing the input JSON object with the new field added.",
        "detail": "tests.baselines.data.custom_mappers",
        "documentation": {}
    },
    {
        "label": "pattern_splitter",
        "kind": 2,
        "importPath": "tests.baselines.data.custom_mappers",
        "description": "tests.baselines.data.custom_mappers",
        "peekOfCode": "def pattern_splitter(pattern: str):\n    \"\"\"\n    Splits the input JSON object into multiple JSON objects based on a given pattern. The pattern will be included in\n    both chunks around it (i.e. they will overlap, one ending in the pattern and one starting with it).\n    \"\"\"\n    pattern = re.compile(pattern)\n    def split_by_pattern(page: Dict) -> List[Dict]:\n        new_pages = []\n        start = 0\n        if pattern.search(page[CONTENT]) is None:",
        "detail": "tests.baselines.data.custom_mappers",
        "documentation": {}
    },
    {
        "label": "test_line_counter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_enrichers",
        "description": "tests.baselines.mappers.enrichers.test_enrichers",
        "peekOfCode": "def test_line_counter():\n    # Test case 1\n    result = line_counter('Hello\\n\\nWorld and all\\nand beyond')\n    assert result == len(['Hello', 'World and all',\n                          'and beyond']), f\"Expected 3, but got {result}\"\n    # Test case 2\n    result = line_counter('Hello\\n\\nWorld and all\\nand beyond', remove_empty=False)\n    assert result == len(['Hello', '', 'World and all',\n                          'and beyond']), f\"Expected 4, but got {result}\"\n    # Test case 3",
        "detail": "tests.baselines.mappers.enrichers.test_enrichers",
        "documentation": {}
    },
    {
        "label": "test_line_counter_enricher",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_enrichers",
        "description": "tests.baselines.mappers.enrichers.test_enrichers",
        "peekOfCode": "def test_line_counter_enricher():\n    page = {CONTENT: 'Hello\\n\\nWorld and all\\nand beyond'}\n    result = line_counter_enricher(page)\n    assert result[0]['num_lines'] == 3, f\"Expected 3, but got {result[0]['num_sentences']}\"\ndef test_word_counter_enricher():\n\t# Empty page\n\tpage = {CONTENT: ''}\n\tresult = word_counter_enricher(page)\n\tassert result[0]['word_count'] == 0, f\"Expected 0, but got {result[0]['word_count']}\"\t\n\t# Several different tricky cases included in the shared example",
        "detail": "tests.baselines.mappers.enrichers.test_enrichers",
        "documentation": {}
    },
    {
        "label": "test_word_counter_enricher",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_enrichers",
        "description": "tests.baselines.mappers.enrichers.test_enrichers",
        "peekOfCode": "def test_word_counter_enricher():\n\t# Empty page\n\tpage = {CONTENT: ''}\n\tresult = word_counter_enricher(page)\n\tassert result[0]['word_count'] == 0, f\"Expected 0, but got {result[0]['word_count']}\"\t\n\t# Several different tricky cases included in the shared example\n\tpage = {CONTENT: \"CameCaseWord Line 1\\nLine 2!!! e.g.\\t  U.S.A.  \\n\\n\\nLine  ????    3\\t\"}\n\tresult = word_counter_enricher(page.copy())\n\tassert result[0]['word_count'] == 9, f\"Expected 9, but got {result[0]['word_count']}\"\t\n\tresult = word_counter_enricher(page.copy(), model='uniseg')",
        "detail": "tests.baselines.mappers.enrichers.test_enrichers",
        "documentation": {}
    },
    {
        "label": "\tpage",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_enrichers",
        "description": "tests.baselines.mappers.enrichers.test_enrichers",
        "peekOfCode": "\tpage = {CONTENT: ''}\n\tresult = word_counter_enricher(page)\n\tassert result[0]['word_count'] == 0, f\"Expected 0, but got {result[0]['word_count']}\"\t\n\t# Several different tricky cases included in the shared example\n\tpage = {CONTENT: \"CameCaseWord Line 1\\nLine 2!!! e.g.\\t  U.S.A.  \\n\\n\\nLine  ????    3\\t\"}\n\tresult = word_counter_enricher(page.copy())\n\tassert result[0]['word_count'] == 9, f\"Expected 9, but got {result[0]['word_count']}\"\t\n\tresult = word_counter_enricher(page.copy(), model='uniseg')\n\tassert result[0]['word_count'] == 9, f\"Expected 9, but got {result[0]['word_count']}\"\t\n\t# Setting ignore punctuation to False can produce non-intuitive word counts when using either model",
        "detail": "tests.baselines.mappers.enrichers.test_enrichers",
        "documentation": {}
    },
    {
        "label": "\tresult",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_enrichers",
        "description": "tests.baselines.mappers.enrichers.test_enrichers",
        "peekOfCode": "\tresult = word_counter_enricher(page)\n\tassert result[0]['word_count'] == 0, f\"Expected 0, but got {result[0]['word_count']}\"\t\n\t# Several different tricky cases included in the shared example\n\tpage = {CONTENT: \"CameCaseWord Line 1\\nLine 2!!! e.g.\\t  U.S.A.  \\n\\n\\nLine  ????    3\\t\"}\n\tresult = word_counter_enricher(page.copy())\n\tassert result[0]['word_count'] == 9, f\"Expected 9, but got {result[0]['word_count']}\"\t\n\tresult = word_counter_enricher(page.copy(), model='uniseg')\n\tassert result[0]['word_count'] == 9, f\"Expected 9, but got {result[0]['word_count']}\"\t\n\t# Setting ignore punctuation to False can produce non-intuitive word counts when using either model\n\tresult = word_counter_enricher(page.copy(), ignore_punctuation=False)",
        "detail": "tests.baselines.mappers.enrichers.test_enrichers",
        "documentation": {}
    },
    {
        "label": "\tpage",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_enrichers",
        "description": "tests.baselines.mappers.enrichers.test_enrichers",
        "peekOfCode": "\tpage = {CONTENT: \"CameCaseWord Line 1\\nLine 2!!! e.g.\\t  U.S.A.  \\n\\n\\nLine  ????    3\\t\"}\n\tresult = word_counter_enricher(page.copy())\n\tassert result[0]['word_count'] == 9, f\"Expected 9, but got {result[0]['word_count']}\"\t\n\tresult = word_counter_enricher(page.copy(), model='uniseg')\n\tassert result[0]['word_count'] == 9, f\"Expected 9, but got {result[0]['word_count']}\"\t\n\t# Setting ignore punctuation to False can produce non-intuitive word counts when using either model\n\tresult = word_counter_enricher(page.copy(), ignore_punctuation=False)\n\tassert result[0]['word_count'] == 14, f\"Expected 14, but got {result[0]['word_count']}\"\t\n\tresult = word_counter_enricher(page.copy(), model='uniseg', ignore_punctuation=False)\n\tassert result[0]['word_count'] == 18, f\"Expected 18, but got {result[0]['word_count']}\"\t",
        "detail": "tests.baselines.mappers.enrichers.test_enrichers",
        "documentation": {}
    },
    {
        "label": "\tresult",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_enrichers",
        "description": "tests.baselines.mappers.enrichers.test_enrichers",
        "peekOfCode": "\tresult = word_counter_enricher(page.copy())\n\tassert result[0]['word_count'] == 9, f\"Expected 9, but got {result[0]['word_count']}\"\t\n\tresult = word_counter_enricher(page.copy(), model='uniseg')\n\tassert result[0]['word_count'] == 9, f\"Expected 9, but got {result[0]['word_count']}\"\t\n\t# Setting ignore punctuation to False can produce non-intuitive word counts when using either model\n\tresult = word_counter_enricher(page.copy(), ignore_punctuation=False)\n\tassert result[0]['word_count'] == 14, f\"Expected 14, but got {result[0]['word_count']}\"\t\n\tresult = word_counter_enricher(page.copy(), model='uniseg', ignore_punctuation=False)\n\tassert result[0]['word_count'] == 18, f\"Expected 18, but got {result[0]['word_count']}\"\t\n\t# Setting ignore whitespace to False can produce non-intuitive word counts when using uniseg",
        "detail": "tests.baselines.mappers.enrichers.test_enrichers",
        "documentation": {}
    },
    {
        "label": "\tresult",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_enrichers",
        "description": "tests.baselines.mappers.enrichers.test_enrichers",
        "peekOfCode": "\tresult = word_counter_enricher(page.copy(), model='uniseg')\n\tassert result[0]['word_count'] == 9, f\"Expected 9, but got {result[0]['word_count']}\"\t\n\t# Setting ignore punctuation to False can produce non-intuitive word counts when using either model\n\tresult = word_counter_enricher(page.copy(), ignore_punctuation=False)\n\tassert result[0]['word_count'] == 14, f\"Expected 14, but got {result[0]['word_count']}\"\t\n\tresult = word_counter_enricher(page.copy(), model='uniseg', ignore_punctuation=False)\n\tassert result[0]['word_count'] == 18, f\"Expected 18, but got {result[0]['word_count']}\"\t\n\t# Setting ignore whitespace to False can produce non-intuitive word counts when using uniseg\n\tresult = word_counter_enricher(page.copy(), ignore_whitespace=False)\n\tassert result[0]['word_count'] == 9, f\"Expected 9, but got {result[0]['word_count']}\"\t",
        "detail": "tests.baselines.mappers.enrichers.test_enrichers",
        "documentation": {}
    },
    {
        "label": "\tresult",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_enrichers",
        "description": "tests.baselines.mappers.enrichers.test_enrichers",
        "peekOfCode": "\tresult = word_counter_enricher(page.copy(), ignore_punctuation=False)\n\tassert result[0]['word_count'] == 14, f\"Expected 14, but got {result[0]['word_count']}\"\t\n\tresult = word_counter_enricher(page.copy(), model='uniseg', ignore_punctuation=False)\n\tassert result[0]['word_count'] == 18, f\"Expected 18, but got {result[0]['word_count']}\"\t\n\t# Setting ignore whitespace to False can produce non-intuitive word counts when using uniseg\n\tresult = word_counter_enricher(page.copy(), ignore_whitespace=False)\n\tassert result[0]['word_count'] == 9, f\"Expected 9, but got {result[0]['word_count']}\"\t\n\tresult = word_counter_enricher(page.copy(), model='uniseg', ignore_whitespace=False)\n\tassert result[0]['word_count'] == 29, f\"Expected 9, but got {result[0]['word_count']}\"",
        "detail": "tests.baselines.mappers.enrichers.test_enrichers",
        "documentation": {}
    },
    {
        "label": "\tresult",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_enrichers",
        "description": "tests.baselines.mappers.enrichers.test_enrichers",
        "peekOfCode": "\tresult = word_counter_enricher(page.copy(), model='uniseg', ignore_punctuation=False)\n\tassert result[0]['word_count'] == 18, f\"Expected 18, but got {result[0]['word_count']}\"\t\n\t# Setting ignore whitespace to False can produce non-intuitive word counts when using uniseg\n\tresult = word_counter_enricher(page.copy(), ignore_whitespace=False)\n\tassert result[0]['word_count'] == 9, f\"Expected 9, but got {result[0]['word_count']}\"\t\n\tresult = word_counter_enricher(page.copy(), model='uniseg', ignore_whitespace=False)\n\tassert result[0]['word_count'] == 29, f\"Expected 9, but got {result[0]['word_count']}\"",
        "detail": "tests.baselines.mappers.enrichers.test_enrichers",
        "documentation": {}
    },
    {
        "label": "\tresult",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_enrichers",
        "description": "tests.baselines.mappers.enrichers.test_enrichers",
        "peekOfCode": "\tresult = word_counter_enricher(page.copy(), ignore_whitespace=False)\n\tassert result[0]['word_count'] == 9, f\"Expected 9, but got {result[0]['word_count']}\"\t\n\tresult = word_counter_enricher(page.copy(), model='uniseg', ignore_whitespace=False)\n\tassert result[0]['word_count'] == 29, f\"Expected 9, but got {result[0]['word_count']}\"",
        "detail": "tests.baselines.mappers.enrichers.test_enrichers",
        "documentation": {}
    },
    {
        "label": "\tresult",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_enrichers",
        "description": "tests.baselines.mappers.enrichers.test_enrichers",
        "peekOfCode": "\tresult = word_counter_enricher(page.copy(), model='uniseg', ignore_whitespace=False)\n\tassert result[0]['word_count'] == 29, f\"Expected 9, but got {result[0]['word_count']}\"",
        "detail": "tests.baselines.mappers.enrichers.test_enrichers",
        "documentation": {}
    },
    {
        "label": "test_single_line_lang_detect",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "def test_single_line_lang_detect():\n    res1 = detect_lang_paragraph_helper(text1, get_langdetect_lang_prob, tokenizer)\n    assert list(res1.keys()) == ['en']\n    assert [round(x['average_probability'], 2) for x in res1.values()] == [1.0]\n    res2 = detect_lang_paragraph_helper(text2, get_langdetect_lang_prob, tokenizer)\n    assert list(res2.keys()) == ['de']\n    assert [round(x['average_probability'], 2) for x in res2.values()] == [1.0]\n    res3 = detect_lang_paragraph_helper(text3, get_langdetect_lang_prob, tokenizer)\n    assert list(res3.keys()) == ['he']\n    assert [round(x['average_probability'], 2) for x in res3.values()] == [1]",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "test_single_line_fast_text_enricher",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "def test_single_line_fast_text_enricher():\n    res1 = detect_lang_paragraph_helper(text1, get_fasttext_lang_prob, tokenizer, fasttext_model)\n    assert list(res1.keys()) == ['en']\n    assert [round(x['average_probability'], 2) for x in res1.values()] == [0.99]\n    res2 = detect_lang_paragraph_helper(text2, get_fasttext_lang_prob, tokenizer, fasttext_model)\n    assert list(res2.keys()) == ['de']\n    assert [round(x['average_probability'], 2) for x in res2.values()] == [0.95]\n    res3 = detect_lang_paragraph_helper(text3, get_fasttext_lang_prob, tokenizer, fasttext_model)\n    assert list(res3.keys()) == ['he']\n    assert [round(x['average_probability'], 2) for x in res3.values()] == [1]",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "test_multiple_lines",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "def test_multiple_lines():\n    assert list(detect_lang_paragraph_helper(text4, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == ['en', 'de', 'he']\n    assert list(detect_lang_paragraph_helper(text4, get_langdetect_lang_prob, tokenizer).keys()) == ['en', 'de', 'he']\ndef test_empty_string():\n    assert list(detect_lang_paragraph_helper(empty_text, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == []\n    assert list(detect_lang_paragraph_helper(empty_text, get_langdetect_lang_prob, tokenizer).keys()) == []\ndef test_spaces_string():\n    assert list(detect_lang_paragraph_helper(spaces_text, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == []\n    assert list(detect_lang_paragraph_helper(spaces_text, get_langdetect_lang_prob, tokenizer).keys()) == []\ndef test_symbols_string():",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "test_empty_string",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "def test_empty_string():\n    assert list(detect_lang_paragraph_helper(empty_text, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == []\n    assert list(detect_lang_paragraph_helper(empty_text, get_langdetect_lang_prob, tokenizer).keys()) == []\ndef test_spaces_string():\n    assert list(detect_lang_paragraph_helper(spaces_text, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == []\n    assert list(detect_lang_paragraph_helper(spaces_text, get_langdetect_lang_prob, tokenizer).keys()) == []\ndef test_symbols_string():\n    assert list(detect_lang_paragraph_helper(symbols_text, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == []\n    assert list(detect_lang_paragraph_helper(symbols_text, get_langdetect_lang_prob, tokenizer).keys()) == []\ndef test_mixed_line():",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "test_spaces_string",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "def test_spaces_string():\n    assert list(detect_lang_paragraph_helper(spaces_text, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == []\n    assert list(detect_lang_paragraph_helper(spaces_text, get_langdetect_lang_prob, tokenizer).keys()) == []\ndef test_symbols_string():\n    assert list(detect_lang_paragraph_helper(symbols_text, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == []\n    assert list(detect_lang_paragraph_helper(symbols_text, get_langdetect_lang_prob, tokenizer).keys()) == []\ndef test_mixed_line():\n    assert list(detect_lang_paragraph_helper(mixed_line_text_de, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == ['en','de']\n    assert list(detect_lang_paragraph_helper(mixed_line_text_de, get_langdetect_lang_prob, tokenizer).keys()) == ['en','de']\n    assert list(detect_lang_paragraph_helper(mixed_line_text, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == ['en','de']",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "test_symbols_string",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "def test_symbols_string():\n    assert list(detect_lang_paragraph_helper(symbols_text, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == []\n    assert list(detect_lang_paragraph_helper(symbols_text, get_langdetect_lang_prob, tokenizer).keys()) == []\ndef test_mixed_line():\n    assert list(detect_lang_paragraph_helper(mixed_line_text_de, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == ['en','de']\n    assert list(detect_lang_paragraph_helper(mixed_line_text_de, get_langdetect_lang_prob, tokenizer).keys()) == ['en','de']\n    assert list(detect_lang_paragraph_helper(mixed_line_text, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == ['en','de']\n    assert list(detect_lang_paragraph_helper(mixed_line_text, get_langdetect_lang_prob, tokenizer).keys()) == ['en','de']\n    assert list(detect_lang_paragraph_helper(mixed_line_text_reversed, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == ['de','en']\n    assert list(detect_lang_paragraph_helper(mixed_line_text_reversed, get_langdetect_lang_prob, tokenizer).keys()) == ['de', 'en']",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "test_mixed_line",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "def test_mixed_line():\n    assert list(detect_lang_paragraph_helper(mixed_line_text_de, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == ['en','de']\n    assert list(detect_lang_paragraph_helper(mixed_line_text_de, get_langdetect_lang_prob, tokenizer).keys()) == ['en','de']\n    assert list(detect_lang_paragraph_helper(mixed_line_text, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == ['en','de']\n    assert list(detect_lang_paragraph_helper(mixed_line_text, get_langdetect_lang_prob, tokenizer).keys()) == ['en','de']\n    assert list(detect_lang_paragraph_helper(mixed_line_text_reversed, get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == ['de','en']\n    assert list(detect_lang_paragraph_helper(mixed_line_text_reversed, get_langdetect_lang_prob, tokenizer).keys()) == ['de', 'en']\ndef test_non_latin_scripts():\n    assert list(detect_lang_paragraph_helper(\"你好，世界\", get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == [\n        'zh']  # fasttext detects the Chinese language as \"zh",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "test_non_latin_scripts",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "def test_non_latin_scripts():\n    assert list(detect_lang_paragraph_helper(\"你好，世界\", get_fasttext_lang_prob, tokenizer, fasttext_model).keys()) == [\n        'zh']  # fasttext detects the Chinese language as \"zh\n    assert list(detect_lang_paragraph_helper(\"你好，世界\", get_langdetect_lang_prob, tokenizer).keys()) == [\n        'zh-cn']  # langdetect detects the Chinese language as \"zh-cn\ndef test_mixed_languages_in_lines():\n    assert list(detect_lang_paragraph_helper(\"\"\"This is English.\\nDies ist Deutsch.\"\"\", get_fasttext_lang_prob,\n                                             tokenizer, fasttext_model).keys()) == ['en', 'de']\n    assert list(detect_lang_paragraph_helper(\"\"\"This is English.\\nDies ist Deutsch.\"\"\", get_langdetect_lang_prob,\n                                             tokenizer).keys()) == ['en', 'de']",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "test_mixed_languages_in_lines",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "def test_mixed_languages_in_lines():\n    assert list(detect_lang_paragraph_helper(\"\"\"This is English.\\nDies ist Deutsch.\"\"\", get_fasttext_lang_prob,\n                                             tokenizer, fasttext_model).keys()) == ['en', 'de']\n    assert list(detect_lang_paragraph_helper(\"\"\"This is English.\\nDies ist Deutsch.\"\"\", get_langdetect_lang_prob,\n                                             tokenizer).keys()) == ['en', 'de']\ndef test_language_whole_page():\n    assert 'de' in list(detect_lang_whole_page_langdetect(text4).keys())\n    assert 'de' in list(detect_lang_whole_page_fasttext(fasttext_model, text4).keys())\ndef test_language_whole_page_enricher():\n     example_page = {CONTENT: text4}",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "test_language_whole_page",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "def test_language_whole_page():\n    assert 'de' in list(detect_lang_whole_page_langdetect(text4).keys())\n    assert 'de' in list(detect_lang_whole_page_fasttext(fasttext_model, text4).keys())\ndef test_language_whole_page_enricher():\n     example_page = {CONTENT: text4}\n     result = detect_lang_whole_page_enricher(LANGDETECT)(example_page)\n     res_dict = result[0]['language_id_whole_page_langdetect']\n     assert list(res_dict.keys()) == ['de']\n     assert round(list(res_dict.values())[0], 1) == 1.0\n     result = detect_lang_whole_page_enricher(FASTTEXT)(example_page)",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "test_language_whole_page_enricher",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "def test_language_whole_page_enricher():\n     example_page = {CONTENT: text4}\n     result = detect_lang_whole_page_enricher(LANGDETECT)(example_page)\n     res_dict = result[0]['language_id_whole_page_langdetect']\n     assert list(res_dict.keys()) == ['de']\n     assert round(list(res_dict.values())[0], 1) == 1.0\n     result = detect_lang_whole_page_enricher(FASTTEXT)(example_page)\n     res_dict = result[0]['language_id_whole_page_fasttext']\n     assert list(res_dict.keys()) == ['de']\n     assert round(list(res_dict.values())[0], 1) == 0.9",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "test_language_paragraph_enricher_fast_text",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "def test_language_paragraph_enricher_fast_text():\n    example_page = {CONTENT: text4}\n    model_func_fast_text = detect_lang_paragraph_enricher(FASTTEXT, tokenizer)\n    res_dict_fast_text = model_func_fast_text(example_page)\n    res_dict = res_dict_fast_text[0]['language_id_paragraph_fasttext']\n    assert list(res_dict.keys()) == ['en', 'de', 'he']\n    assert [round(x['average_probability'], 2) for x in res_dict.values()] == [0.98, 0.99, 1.0]\ndef test_language_paragraph_enricher_lang_detect():\n    example_page = {CONTENT: text4}\n    res_dict_fast_text = detect_lang_paragraph_enricher(LANGDETECT, tokenizer)(example_page)",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "test_language_paragraph_enricher_lang_detect",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "def test_language_paragraph_enricher_lang_detect():\n    example_page = {CONTENT: text4}\n    res_dict_fast_text = detect_lang_paragraph_enricher(LANGDETECT, tokenizer)(example_page)\n    res_dict = res_dict_fast_text[0]['language_id_paragraph_langdetect']\n    assert list(res_dict.keys()) == ['en', 'de', 'he']\n    assert [round(x['average_probability'], 2) for x in res_dict.values()] == [1.0, 1.0, 1.0]",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "text1",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "text1 = \"War doesn't show who's right, just who's left.\"\ntext2 = \"Ein, zwei, drei, vier\"\ntext3 = \"באתי, ראיתי, כבשתי\"\ntext4 = \"This is English.\\nDies ist Deutsch, die offizielle Sprache in Deutschland.\\nזה עברית\"\nempty_text = \"\"\nspaces_text = \"     \"\nsymbols_text = \"!@#$%^&*()_+\"\nmixed_line_text = \"This is an English text that should be long. Dies ist Deutsch.\"\nmixed_line_text_reversed = \"Dies ist Deutsch. This is an English text that should be long.\"\nmixed_line_text_de = \"English. Dies ist Deutsch.\"",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "text2",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "text2 = \"Ein, zwei, drei, vier\"\ntext3 = \"באתי, ראיתי, כבשתי\"\ntext4 = \"This is English.\\nDies ist Deutsch, die offizielle Sprache in Deutschland.\\nזה עברית\"\nempty_text = \"\"\nspaces_text = \"     \"\nsymbols_text = \"!@#$%^&*()_+\"\nmixed_line_text = \"This is an English text that should be long. Dies ist Deutsch.\"\nmixed_line_text_reversed = \"Dies ist Deutsch. This is an English text that should be long.\"\nmixed_line_text_de = \"English. Dies ist Deutsch.\"\ntokenizer = 'nltk'",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "text3",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "text3 = \"באתי, ראיתי, כבשתי\"\ntext4 = \"This is English.\\nDies ist Deutsch, die offizielle Sprache in Deutschland.\\nזה עברית\"\nempty_text = \"\"\nspaces_text = \"     \"\nsymbols_text = \"!@#$%^&*()_+\"\nmixed_line_text = \"This is an English text that should be long. Dies ist Deutsch.\"\nmixed_line_text_reversed = \"Dies ist Deutsch. This is an English text that should be long.\"\nmixed_line_text_de = \"English. Dies ist Deutsch.\"\ntokenizer = 'nltk'\nfasttext_model = load_fasttext_model()",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "text4",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "text4 = \"This is English.\\nDies ist Deutsch, die offizielle Sprache in Deutschland.\\nזה עברית\"\nempty_text = \"\"\nspaces_text = \"     \"\nsymbols_text = \"!@#$%^&*()_+\"\nmixed_line_text = \"This is an English text that should be long. Dies ist Deutsch.\"\nmixed_line_text_reversed = \"Dies ist Deutsch. This is an English text that should be long.\"\nmixed_line_text_de = \"English. Dies ist Deutsch.\"\ntokenizer = 'nltk'\nfasttext_model = load_fasttext_model()\ndef test_single_line_lang_detect():",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "empty_text",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "empty_text = \"\"\nspaces_text = \"     \"\nsymbols_text = \"!@#$%^&*()_+\"\nmixed_line_text = \"This is an English text that should be long. Dies ist Deutsch.\"\nmixed_line_text_reversed = \"Dies ist Deutsch. This is an English text that should be long.\"\nmixed_line_text_de = \"English. Dies ist Deutsch.\"\ntokenizer = 'nltk'\nfasttext_model = load_fasttext_model()\ndef test_single_line_lang_detect():\n    res1 = detect_lang_paragraph_helper(text1, get_langdetect_lang_prob, tokenizer)",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "spaces_text",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "spaces_text = \"     \"\nsymbols_text = \"!@#$%^&*()_+\"\nmixed_line_text = \"This is an English text that should be long. Dies ist Deutsch.\"\nmixed_line_text_reversed = \"Dies ist Deutsch. This is an English text that should be long.\"\nmixed_line_text_de = \"English. Dies ist Deutsch.\"\ntokenizer = 'nltk'\nfasttext_model = load_fasttext_model()\ndef test_single_line_lang_detect():\n    res1 = detect_lang_paragraph_helper(text1, get_langdetect_lang_prob, tokenizer)\n    assert list(res1.keys()) == ['en']",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "symbols_text",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "symbols_text = \"!@#$%^&*()_+\"\nmixed_line_text = \"This is an English text that should be long. Dies ist Deutsch.\"\nmixed_line_text_reversed = \"Dies ist Deutsch. This is an English text that should be long.\"\nmixed_line_text_de = \"English. Dies ist Deutsch.\"\ntokenizer = 'nltk'\nfasttext_model = load_fasttext_model()\ndef test_single_line_lang_detect():\n    res1 = detect_lang_paragraph_helper(text1, get_langdetect_lang_prob, tokenizer)\n    assert list(res1.keys()) == ['en']\n    assert [round(x['average_probability'], 2) for x in res1.values()] == [1.0]",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "mixed_line_text",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "mixed_line_text = \"This is an English text that should be long. Dies ist Deutsch.\"\nmixed_line_text_reversed = \"Dies ist Deutsch. This is an English text that should be long.\"\nmixed_line_text_de = \"English. Dies ist Deutsch.\"\ntokenizer = 'nltk'\nfasttext_model = load_fasttext_model()\ndef test_single_line_lang_detect():\n    res1 = detect_lang_paragraph_helper(text1, get_langdetect_lang_prob, tokenizer)\n    assert list(res1.keys()) == ['en']\n    assert [round(x['average_probability'], 2) for x in res1.values()] == [1.0]\n    res2 = detect_lang_paragraph_helper(text2, get_langdetect_lang_prob, tokenizer)",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "mixed_line_text_reversed",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "mixed_line_text_reversed = \"Dies ist Deutsch. This is an English text that should be long.\"\nmixed_line_text_de = \"English. Dies ist Deutsch.\"\ntokenizer = 'nltk'\nfasttext_model = load_fasttext_model()\ndef test_single_line_lang_detect():\n    res1 = detect_lang_paragraph_helper(text1, get_langdetect_lang_prob, tokenizer)\n    assert list(res1.keys()) == ['en']\n    assert [round(x['average_probability'], 2) for x in res1.values()] == [1.0]\n    res2 = detect_lang_paragraph_helper(text2, get_langdetect_lang_prob, tokenizer)\n    assert list(res2.keys()) == ['de']",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "mixed_line_text_de",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "mixed_line_text_de = \"English. Dies ist Deutsch.\"\ntokenizer = 'nltk'\nfasttext_model = load_fasttext_model()\ndef test_single_line_lang_detect():\n    res1 = detect_lang_paragraph_helper(text1, get_langdetect_lang_prob, tokenizer)\n    assert list(res1.keys()) == ['en']\n    assert [round(x['average_probability'], 2) for x in res1.values()] == [1.0]\n    res2 = detect_lang_paragraph_helper(text2, get_langdetect_lang_prob, tokenizer)\n    assert list(res2.keys()) == ['de']\n    assert [round(x['average_probability'], 2) for x in res2.values()] == [1.0]",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "tokenizer = 'nltk'\nfasttext_model = load_fasttext_model()\ndef test_single_line_lang_detect():\n    res1 = detect_lang_paragraph_helper(text1, get_langdetect_lang_prob, tokenizer)\n    assert list(res1.keys()) == ['en']\n    assert [round(x['average_probability'], 2) for x in res1.values()] == [1.0]\n    res2 = detect_lang_paragraph_helper(text2, get_langdetect_lang_prob, tokenizer)\n    assert list(res2.keys()) == ['de']\n    assert [round(x['average_probability'], 2) for x in res2.values()] == [1.0]\n    res3 = detect_lang_paragraph_helper(text3, get_langdetect_lang_prob, tokenizer)",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "fasttext_model",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "description": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "peekOfCode": "fasttext_model = load_fasttext_model()\ndef test_single_line_lang_detect():\n    res1 = detect_lang_paragraph_helper(text1, get_langdetect_lang_prob, tokenizer)\n    assert list(res1.keys()) == ['en']\n    assert [round(x['average_probability'], 2) for x in res1.values()] == [1.0]\n    res2 = detect_lang_paragraph_helper(text2, get_langdetect_lang_prob, tokenizer)\n    assert list(res2.keys()) == ['de']\n    assert [round(x['average_probability'], 2) for x in res2.values()] == [1.0]\n    res3 = detect_lang_paragraph_helper(text3, get_langdetect_lang_prob, tokenizer)\n    assert list(res3.keys()) == ['he']",
        "detail": "tests.baselines.mappers.enrichers.test_language_id_enricher",
        "documentation": {}
    },
    {
        "label": "test_classify_fasttext_hq_prob_enricher",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_fasttext",
        "description": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_fasttext",
        "peekOfCode": "def test_classify_fasttext_hq_prob_enricher():\n    wiki_probs = pd.DataFrame(index=range(1, 6), columns=range(1, 6))\n    for i in range(1, 6):  # assuming files are numbered from 1 to 5\n        with open(os.path.join(test_files_path, f'wikipedia_paragraph{i}.html'), 'r') as file:\n            wikipedia_content_page = {CONTENT: file.read()}\n        result_wiki_page = classify_fasttext_hq_prob_enricher()(wikipedia_content_page)\n        result_wiki = result_wiki_page[0]['fasttext_hq_prob']\n        for j in range(1, 6):  # comparing each Wikipedia file to all CommonCrawl files\n            with open(os.path.join(test_files_path, f'common_crawl_paragraph{j}.html'), 'r') as file:\n                commoncrawl_content_page = {CONTENT: file.read()}",
        "detail": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_fasttext",
        "documentation": {}
    },
    {
        "label": "current_directory",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_fasttext",
        "description": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_fasttext",
        "peekOfCode": "current_directory = os.path.dirname(os.path.abspath(__file__))\ntest_files_path = os.path.join(current_directory, 'enricher_test_files')\ndef test_classify_fasttext_hq_prob_enricher():\n    wiki_probs = pd.DataFrame(index=range(1, 6), columns=range(1, 6))\n    for i in range(1, 6):  # assuming files are numbered from 1 to 5\n        with open(os.path.join(test_files_path, f'wikipedia_paragraph{i}.html'), 'r') as file:\n            wikipedia_content_page = {CONTENT: file.read()}\n        result_wiki_page = classify_fasttext_hq_prob_enricher()(wikipedia_content_page)\n        result_wiki = result_wiki_page[0]['fasttext_hq_prob']\n        for j in range(1, 6):  # comparing each Wikipedia file to all CommonCrawl files",
        "detail": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_fasttext",
        "documentation": {}
    },
    {
        "label": "test_files_path",
        "kind": 5,
        "importPath": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_fasttext",
        "description": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_fasttext",
        "peekOfCode": "test_files_path = os.path.join(current_directory, 'enricher_test_files')\ndef test_classify_fasttext_hq_prob_enricher():\n    wiki_probs = pd.DataFrame(index=range(1, 6), columns=range(1, 6))\n    for i in range(1, 6):  # assuming files are numbered from 1 to 5\n        with open(os.path.join(test_files_path, f'wikipedia_paragraph{i}.html'), 'r') as file:\n            wikipedia_content_page = {CONTENT: file.read()}\n        result_wiki_page = classify_fasttext_hq_prob_enricher()(wikipedia_content_page)\n        result_wiki = result_wiki_page[0]['fasttext_hq_prob']\n        for j in range(1, 6):  # comparing each Wikipedia file to all CommonCrawl files\n            with open(os.path.join(test_files_path, f'common_crawl_paragraph{j}.html'), 'r') as file:",
        "detail": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_fasttext",
        "documentation": {}
    },
    {
        "label": "test_low_example",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_kenlm_perplexity",
        "description": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_kenlm_perplexity",
        "peekOfCode": "def test_low_example():\n    example_page = {CONTENT: \"I am very perplexed\"}\n    example_page_enriched = ken_lm_perplexity_enricher()(example_page)\n    ''' assert score around 341.3 '''\n    assert abs(example_page_enriched[0]['kenlm_perplexity'] - 341.3) <= 0.1\ndef test_high_example():\n    example_page = {CONTENT: \"im hella trippin\"}\n    example_page_enriched = ken_lm_perplexity_enricher()(example_page)\n    ''' assert score around 46793.5 '''\n    assert abs(example_page_enriched[0]['kenlm_perplexity'] - 46793.5) <= 0.1",
        "detail": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_kenlm_perplexity",
        "documentation": {}
    },
    {
        "label": "test_high_example",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_kenlm_perplexity",
        "description": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_kenlm_perplexity",
        "peekOfCode": "def test_high_example():\n    example_page = {CONTENT: \"im hella trippin\"}\n    example_page_enriched = ken_lm_perplexity_enricher()(example_page)\n    ''' assert score around 46793.5 '''\n    assert abs(example_page_enriched[0]['kenlm_perplexity'] - 46793.5) <= 0.1\ndef test_get_perplexity():\n    high_perplexity_sentences = [\n        \"Happily water you the pass can?\",\n        \"To the store to go I need.\",\n        \"The now time is what?\",",
        "detail": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_kenlm_perplexity",
        "documentation": {}
    },
    {
        "label": "test_get_perplexity",
        "kind": 2,
        "importPath": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_kenlm_perplexity",
        "description": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_kenlm_perplexity",
        "peekOfCode": "def test_get_perplexity():\n    high_perplexity_sentences = [\n        \"Happily water you the pass can?\",\n        \"To the store to go I need.\",\n        \"The now time is what?\",\n        \"Really good is weather the.\",\n        \"Today very am I happy.\"\n    ]\n    low_perplexity_sentences = [\n        \"I am very happy today.\",",
        "detail": "tests.baselines.mappers.enrichers.test_quality_prediction_enricher_kenlm_perplexity",
        "documentation": {}
    },
    {
        "label": "test_github_extension_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_github_extension_filter():\n    # Test case with an allowed file extension\n    page = {'filename': 'script.py'}\n    assert github_extension_filter(page) == [page]\n    # Check for allowed extensions that do not contain a \".\"\n    page = {'filename': 'Makefile'}\n    assert github_extension_filter(page) == [page]\n    page = {'filename': 'Dockerfile'}\n    assert github_extension_filter(page) == [page]\n    # Test case with a different key for filename",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_line_length_filters",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_line_length_filters():\n    content_max_exceeded = 'a' * 1001 + '\\n' + 'Short line'\n    page_max_exceeded = {CONTENT: content_max_exceeded}\n    assert line_length_filter(page_max_exceeded, max_length=1000, length_type='max') == []\n    # average line length exceeding limit\n    content_avg_exceeded = ('a' * 200 + '\\n') * 5\n    page_avg_exceeded = {CONTENT: content_avg_exceeded}\n    assert line_length_filter(page_avg_exceeded, max_length=100, length_type='avg') == []\n    content_within_limits = 'a' * 100 + '\\n' + 'Short line'\n    page_within_limits = {CONTENT: content_within_limits}",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_alphabetic_characters_to_tokens_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_alphabetic_characters_to_tokens_filter():\n    alpha_to_tokens_filter = alphabetic_characters_to_tokens_filter()\n    # Test with empty text\n    page = {CONTENT: ''}\n    assert alpha_to_tokens_filter(page) == []\n    # Test with all alphanumeric text and default max_alpha_ratio\n    page = {CONTENT: 'Hello123'}\n    assert alpha_to_tokens_filter(page) == [page]\n    assert alpha_to_tokens_filter(page, 0.25) == [page]\n    # Test with a text that has a low alphabetical ratio",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_alphanumeric_char_ratio_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_alphanumeric_char_ratio_filter():\n    # Test with empty text\n    page = {CONTENT: ''}\n    assert alphanumeric_char_ratio_filter(page) == []\n    # Test with all alphanumeric text\n    page = {CONTENT: 'Hello123'}\n    assert alphanumeric_char_ratio_filter(page, 0.25) == [page]\n    # Test with mixed content\n    page = {CONTENT: 'Hello 123!'}\n    assert alphanumeric_char_ratio_filter(page, 0.25) == [page]",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_repetition_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_repetition_filter():\n    page = {CONTENT: ''}\n    assert repetition_filter(page, granularity='line', max_fraction=1) == []\n    assert repetition_filter(page, granularity='paragraph', max_fraction=1) == [] \n    assert repetition_filter(page, granularity=2, max_fraction=0.99) == []\n    assert repetition_filter(page, granularity=8, max_fraction=0.99) == []\n    ### Line-wise repetition filtering ###\n    # If just one line/paragraph, then return the page by default\n    page = {CONTENT: 'Hello, World.'}\n    assert repetition_filter(page, granularity='line', max_fraction=0.0, count_characters=False) == [page]",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_massive_web_repetition_filters",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_massive_web_repetition_filters():\n    for _ in range(100):\n        page = {CONTENT: 'This is a complex sentence with unique words.\\n Here is another one.'}\n        assert massive_web_repetition_filters(page) == [page]\n        page = {CONTENT: 'Repeat repeat repeat repeat.'}\n        assert massive_web_repetition_filters(page) == []\n        page = {CONTENT: ''}\n        assert massive_web_repetition_filters(page) == []\n        page = {CONTENT: None}\n        with pytest.raises(TypeError):",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_page_length_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_page_length_filter():\n    # Test with empty text\n    page = {CONTENT: ''}\n    assert page_length_filter(page, \"word\", 1) == []\n    assert page_length_filter(page, \"sentence\", 1) == []\n    assert page_length_filter(page, \"line\", 1) == []\n    assert page_length_filter(page, \"paragraph\", 1) == []\n    assert page_length_filter(page, \"char\", 1) == []\n    # Test with a one-sentence long piece of text\n    page = {CONTENT: 'Hello world.'}",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_substring_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_substring_filter():\n    # Test basic presence/absence of the banned word\n    page = {CONTENT: \"warning: Javascript should be enabled\"}\n    assert substring_filter('javascript')(page) == []\n    assert substring_filter('{')(page) == [page]\n    # Test when multiple banned words are passed in\n    assert substring_filter(['javascript', '{'])(page) == []\n    assert substring_filter(['python', '{'])(page) == [page]\n    # Test location parameter\n    assert substring_filter('javascript', location='prefix')(page) == [page]",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_ellipsis_count_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_ellipsis_count_filter():\n    for ell in ['...', '. . .', '\\u2026']:\n        # Text with 50% of lines ending with an ellipsis\n        page = {CONTENT: f\"This text has one line that ends with an ellipsis{ell}\\n and one line that doesn't.\"}\n        assert ellipsis_count_filter(page, max_ellipsis_end_ratio=0.49) == []\n        assert ellipsis_count_filter(page, max_ellipsis_end_ratio=0.51) == [page]\n        # Text where ellipsis exists but not at end of a line\n        page = {CONTENT: f\"This text has one{ell}line that ends with an ellipsis{ell}and one line that doesn't.\"}\n        assert ellipsis_count_filter(page, max_ellipsis_end_ratio=0.01) == [page]\n        # Text where multiple ellipsis exist in a row",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_bullet_count_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_bullet_count_filter():\n    for bullet1 in ['●', '•', '*', '-']:\n        for bullet2 in ['●', '•', '*', '-']:\n            # Text with 2/3 of lines start with an ellipsis, using both acceptable forms of ellipsis\n            page = {CONTENT: f\"{bullet1} This text has two lines\\n{bullet2} that start with a bullet\\n and one line that doesn't.\"}\n            assert bullet_count_filter(page, max_bullet_start_ratio=0.66) == []\n            assert bullet_count_filter(page, max_bullet_start_ratio=0.67) == [page]\n            # Text where bullets exist but not at end of a line\n            page = {CONTENT: f\"This text has two lines{bullet2} that start with a bullet{bullet1}\\n and one line that doesn't.\"}\n            assert bullet_count_filter(page, max_bullet_start_ratio=0.01) == [page]",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_stop_word_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_stop_word_filter():\n    # Differentiate instances of stop words vs unique stop words\n    page = {CONTENT: 'The best of the best'} # the, of, the\n    assert stop_word_filter(page, False, min_stop_word=3) == [page]\n    assert stop_word_filter(page, True, min_stop_word=3) == []\n    # All stop words\n    page = {CONTENT: 'the be to of and that have with'}\n    assert stop_word_filter(page, False, min_stop_word=8) == [page]\n    assert stop_word_filter(page, True, min_stop_word=8) == [page]\n    # Substrings",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_word_length_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_word_length_filter():\n    # Regular case\n    page = {CONTENT: 'The average word length is 4.33'}\n    assert word_length_filter(page, min_length=4, max_length=5) == [page]\n    # Test short words and default value\n    page = {CONTENT: 'The cat in the hat'}\n    assert word_length_filter(page, min_length=3, max_length=10) == []\n    assert word_length_filter(page, max_length=10) == [page]\n    # Test long words and default value\n    page = {CONTENT: 'Loquaciousness verylongwords'}",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_symbol_ratio_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_symbol_ratio_filter():\n    # Regular tests\n    page = {CONTENT: 'No symbols here.'}\n    assert symbol_ratio_filter(page, max_symbol_to_word_ratio=0.0) == [page]\n    page = {CONTENT: '#hashtags #on #every #word'}\n    assert symbol_ratio_filter(page, max_symbol_to_word_ratio=1.0) == [page]\n    assert symbol_ratio_filter(page, max_symbol_to_word_ratio=0.1) == []\n    # Test all ellipses\n    page = {CONTENT: '#hashtags and ellipses both count...'}\n    assert symbol_ratio_filter(page, max_symbol_to_word_ratio=0.3) == []",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_word_removal_ratio_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_word_removal_ratio_filter():\n    # Scenario 1: No words are removed, should return the page as is.\n    page = {CONTENT: \"word 1\\nword 2\\nword 3\", 'prev_word_count': 6}\n    assert word_removal_ratio_filter(page, 'prev_word_count') == [page]\n    # Scenario 2: 33.33% of words are removed (2 out of 6), which is more than 5%.\n    page = {CONTENT: \"word 1\\nword 2\", 'prev_word_count': 6}\n    assert word_removal_ratio_filter(page, 'prev_word_count') == []\n    assert word_removal_ratio_filter(page, 'prev_word_count', max_removed_ratio=0.34) == [page]\n    # Scenario 3: Exactly 5% of words are removed (95 remain out of 100). Should return the page as is.\n    page = {CONTENT: \"word\\n\" * 95, 'prev_word_count': 100}",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_alphabetic_word_ratio_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_content_filters",
        "description": "tests.baselines.mappers.filters.test_content_filters",
        "peekOfCode": "def test_alphabetic_word_ratio_filter():\n    # Test with empty page\n    page = {CONTENT: ''}\n    assert alphabetic_word_ratio_filter(page, 0.2) == []\n    # Test with 20% non-alphabetic words and some words with a mix of alphabetic and non-alphabetic elements\n    page = {CONTENT: 'This sentence contains <1,000 non-alphabetic w0rd5 within it but >1.'}\n    assert alphabetic_word_ratio_filter(page, 0.2) == [page]\n    assert alphabetic_word_ratio_filter(page, 0.1) == []\n    # Test with 20% non-alphabetic words and some words with a mix of alphabetic and non-alphabetic elements\n    page = {CONTENT: 'This sentence contains <1,000     non-alphabetic \\nw0rd5 within it but >1.'}",
        "detail": "tests.baselines.mappers.filters.test_content_filters",
        "documentation": {}
    },
    {
        "label": "test_random_sampling_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_metadata_filters",
        "description": "tests.baselines.mappers.filters.test_metadata_filters",
        "peekOfCode": "def test_random_sampling_filter():\n    page = {CONTENT: \"content\"}\n    for seed in [1,2,3]:\n        random.seed(seed)\n        pages = [random_sampling_filter(page, 0.1) for i in range(10000)]\n        assert 900 <= sum(len(p) for p in pages) <= 1100\n    with pytest.raises(AssertionError):\n        assert random_sampling_filter(page, 1.1)\n    with pytest.raises(AssertionError):\n        assert random_sampling_filter(page, -0.1)",
        "detail": "tests.baselines.mappers.filters.test_metadata_filters",
        "documentation": {}
    },
    {
        "label": "test_url_substring_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_metadata_filters",
        "description": "tests.baselines.mappers.filters.test_metadata_filters",
        "peekOfCode": "def test_url_substring_filter():\n    # Test basic presence/absence of the banned word\n    page = {URL: 'https://www.badword.com'}\n    assert url_substring_filter(['badword', 'reallybadword'])(page) == []\n    assert url_substring_filter(['reallybadword', 'extremelybadword'])(page) == [page]\n    # Test robustness to urls that split up a bad word to bypass detectors\n    page = {URL: 'https://www.ba-dwo.rd.com'}\n    assert url_substring_filter('badword')(page) == [page]\n    assert url_substring_filter(['badword'])(page) == [page]\n    assert url_substring_filter('badword', ignore_chars=['-', '.'])(page) == []",
        "detail": "tests.baselines.mappers.filters.test_metadata_filters",
        "documentation": {}
    },
    {
        "label": "test_language_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_metadata_filters",
        "description": "tests.baselines.mappers.filters.test_metadata_filters",
        "peekOfCode": "def test_language_filter():\n    page_above_threshold = {\n        CONTENT: \"This is an example content in English.\",\n        \"language_id_whole_page_langdetect\": {\"en\": 0.81}\n    }\n    page_below_threshold = {\n        CONTENT: \"This is another example content in English.\",\n        \"language_id_whole_page_langdetect\": {\"en\": 0.79}\n    }\n    # Keeping English pages with probability above 0.8",
        "detail": "tests.baselines.mappers.filters.test_metadata_filters",
        "documentation": {}
    },
    {
        "label": "test_language_filter_v2",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_metadata_filters",
        "description": "tests.baselines.mappers.filters.test_metadata_filters",
        "peekOfCode": "def test_language_filter_v2():\n    # Test with language not in the keep_languages list\n    page = {'language_id_whole_page_langdetect': {'en': 0.9}}\n    assert language_filter(page, ['es', 'fr', 'de']) == []\n    # Test with language in the keep_languages list but below threshold\n    page = {'language_id_whole_page_langdetect': {'en': 0.5}}\n    assert language_filter(page, ['es', 'fr', 'en', 'de'], threshold=0.8) == []\n    # Test with language in the keep_languages list and above threshold\n    page = {'language_id_whole_page_langdetect': {'en': 0.9}}\n    assert language_filter(page, ['es', 'fr', 'en', 'de'], threshold=0.8) == [page]",
        "detail": "tests.baselines.mappers.filters.test_metadata_filters",
        "documentation": {}
    },
    {
        "label": "test_quality_filter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.filters.test_metadata_filters",
        "description": "tests.baselines.mappers.filters.test_metadata_filters",
        "peekOfCode": "def test_quality_filter():\n    page = {CONTENT: \"This is an example content.\", \"fasttext_hq_prob\": 0.75, \"kenlm_perplexity\": 176.1}\n    result = quality_filter(page, \"fasttext_hq_prob\", 0.74)\n    assert len(result) == 1, \"Page was not kept when threshold was equal to real quality score using 'fasttext'.\"\n    result = quality_filter(page, \"fasttext_hq_prob\", 0.75)\n    assert len(result) == 1, \"Page was not kept when threshold was equal to real quality score using 'fasttext'.\"\n    result = quality_filter(page, \"fasttext_hq_prob\", 0.9)\n    assert len(result) == 0, \"Page was kept when threshold was above real quality score using 'fasttext'.\"\n    result = quality_filter(page, \"kenlm_perplexity\", 176.0, lower_better=True)\n    assert len(result) == 0, \"Page was kept when threshold was below real quality score using 'kenlm'.\"",
        "detail": "tests.baselines.mappers.filters.test_metadata_filters",
        "documentation": {}
    },
    {
        "label": "test_key_name_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_key_name_modifier():\n    # Test when the old_key is present\n    page = {'content': 'example text'}\n    assert key_name_modifier(page, 'content', 'text') == [{'text': 'example text'}]\n    # If old_key not already present, don't do anything. \n    page = {'content': 'example text'}\n    assert key_name_modifier(page, 'page_content', 'text') == [{'content': 'example text'}]\n    # If old_key and new_key are present, do not overwrite new_key by default\n    page = {'content': 'example text', 'text': 'existing text'}\n    with pytest.warns(UserWarning, match=\"text is already in the page but allow_overwrite is set to False.\"):",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_starcoder_v2_repo_splitter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_starcoder_v2_repo_splitter():\n    input_page = {\n        'repo_name': 'sample/repo', \n        'files': [ {'filename': f'/path/to/file{i}', 'text': f'code{i}'} for i in range(30)],\n        CONTENT: 'placeholder text'\n    }\n    output_pages = [\n        {\n            'repo_name': 'sample/repo', \n            'files': [ {'filename': f'/path/to/file{i}', 'text': f'code{i}'} for i in range(10)]",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_starcoder_v2_format_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_starcoder_v2_format_modifier():\n    input_page = {\n        'repo_name': 'sample/repo', \n        'files': [\n            {'filename': '/path/to/file1', 'text': 'code1'},\n            {'filename': '/path/to/file2', 'text': 'code2'},\n            {'filename': '/path/to/file3', 'text': 'code3'},\n        ]\n    }\n    output_page = copy.deepcopy(input_page)",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_arxiv_appendix_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_arxiv_appendix_modifier():\n    modifier = arxiv_appendix_modifier()\n    # Test with \\appendix at the beginning\n    input_page = {CONTENT: \"\\\\appendix\\nAppendix\"}\n    assert modifier(input_page) == []\n    # Test with \\bibliography at the beginning\n    input_page = {CONTENT: \"\\\\bibliography{ref}\\nBibliography content\"}\n    assert modifier(input_page) == []\n    # Test with variations of \\begin{references}\n    input_page = {CONTENT: \"Content\\n\\\\begin{references}\\nReferences content\"}",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_arxiv_comment_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_arxiv_comment_modifier():\n    modifier = arxiv_comment_modifier()\n    modifier_w_multiline = arxiv_comment_modifier(remove_multiline=True)\n    # Test removing a single-line comment\n    input_page = {CONTENT: \"% This is a single line comment\"}\n    assert modifier(input_page) == []\n    # Test removing multiple single-line comments\n    input_page = {CONTENT: \"% Comment 1\\n% Comment 2\\nContent\"}\n    expected_output = {CONTENT: \"Content\"}\n    assert modifier(input_page) == [expected_output]",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_arxiv_section_strip_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_arxiv_section_strip_modifier():\n    modifier = arxiv_section_strip_modifier()\n    # Test with content before and after the first section\n    input_page = {CONTENT: \"Content before\\\\section{First Section}Content in between\\\\section{Last Section}Content after\"}\n    expected_output = {CONTENT: \"\\\\section{First Section}Content in between\\\\section{Last Section}Content after\"}\n    assert modifier(input_page) == [expected_output]\n    # Test with multiple sections and preamble\n    input_page = {CONTENT: \"Preamble\\\\section{Sec1}Content1\\\\section{Sec2}Content2\"}\n    expected_output = {CONTENT: \"\\\\section{Sec1}Content1\\\\section{Sec2}Content2\"}\n    assert modifier(input_page) == [expected_output]",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_arxiv_macro_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_arxiv_macro_modifier():\n    modifier = arxiv_macro_modifier()\n    # Test replacing a \\newcommand macro with its value\n    input_page = {CONTENT: \"\\\\newcommand{\\\\examplemacro}{ExampleValue}\\nUse of \\\\examplemacro here.\"}\n    expected_output = {CONTENT: '\\\\newcommand{ExampleValue}{ExampleValue}\\nUse of ExampleValue here.'} \n    assert modifier(input_page) == [expected_output]\n    # Test replacing multiple macros\n    input_page = {CONTENT: \"\\\\newcommand{\\\\macroOne}{FirstValue}\\n\\\\newcommand{\\\\macroTwo}{SecondValue}\\nUse of \\\\macroOne and \\\\macroTwo.\"}\n    expected_output = {CONTENT: \"\\\\newcommand{FirstValue}{FirstValue}\\n\\\\newcommand{SecondValue}{SecondValue}\\nUse of FirstValue and SecondValue.\"}\n    assert modifier(input_page) == [expected_output]",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_stackexchange_list_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_stackexchange_list_modifier():\n    page_wo_answers = {'question': {'text': 'Question 1\\n<ol>\\n\\t<li>Choice 1</li>\\n\\t<li>Choice 2</li></ol>'}}\n    stackexchange_list_modifier(page_wo_answers) == [{'question': {'text': \"Question 1\\n\\n*\\n\\t\\n*Choice 1\\n\\t\\n*Choice 2\"}}]\n    page = {\n        'question': {'text': 'Question 1\\n<ol>\\n\\t<li>Choice 1</li>\\n\\t<li>Choice 2</li></ol>'},\n        'answers': [\n            {'text': 'Choice 1'},\n            {'text': '<ol>\\n\\t<li>Choice 1 is correct</li>\\n\\t<li>Choice 2 is incorrect</li></ol>'},\n        ]\n    }",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_stackexchange_answer_sort_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_stackexchange_answer_sort_modifier():\n    page_with_scores = {\n        'question': {'text': 'Question 1'},\n        'answers': [\n            {'text': 'Answer 1', 'score': 10},\n            {'text': 'Answer 2', 'score': 5},\n            {'text': 'Answer 3', 'score': 15},\n        ]\n    }\n    sorted_page_descending = {",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_stackexchange_html_extraction_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_stackexchange_html_extraction_modifier():\n    modifier = stackexchange_html_extraction_modifier()\n    page = {\n        'question': {'text': '<!DOCTYPE html>\\n<html>\\n<head><title>Question title</title></head>\\n<body>Question body</body>\\n</html>'},\n        'answers': [\n            {'text': 'Answer 3', 'score': 15},\n            {'text': '<ol>\\n\\t<li>Here is the answer.</li>\\n\\t<li>This is why.</li></ol>', 'score': 10},\n        ]\n    }\n    output_page = {",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_stackexchange_qa_formatter",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_stackexchange_qa_formatter():\n    page = {\n        'question': {'text': 'Question 1'},\n        'answers': [\n            {'text': 'Answer 3', 'score': 15},\n            {'text': 'Answer 1', 'score': 10},\n            {'text': 'Answer 2', 'score': 5},\n        ]\n    }\n    output_page = {",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_html_content_extraction_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_html_content_extraction_modifier():\n    # Test with empty text\n    page = {CONTENT: ''}\n    assert html_content_extraction_modifier(page) == []\n    # Test with short 404 page\n    page = {\n        CONTENT: '<html><head><meta charset=\"utf-8\" /><title>404 Not Found</title><meta name=\"description\" content=\"404 Not Found\" /></head></html>'}\n    assert html_content_extraction_modifier(page) == []\n    # Test with short sample page\n    page = {",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_substring_line_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_substring_line_modifier():\n    # Test with no CONTENT in page\n    page = {}\n    with pytest.raises(KeyError):\n        substring_line_modifier('javascript')(page)\n    # Test with empty text\n    page = {CONTENT: ''}\n    assert substring_line_modifier('javascript')(page) == []\n    # Test with various lines\n    page = {CONTENT: 'JavaScript must be enabled\\n\\nThis is an article about JavaScript.\\njava script enabled '}",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_line_length_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_line_length_modifier():\n    # Test with empty text\n    page = {CONTENT: ''}\n    assert line_length_modifier(page, 2, 10) == []\n    # Test with various lines\n    page = {CONTENT: 'Short line\\n\\na b c d e f g h i j k\\n  There are exactly 10 words present in this line here\\nHi'}\n    assert line_length_modifier(page, 2, 10) == [\n        {CONTENT: 'Short line\\n\\n  There are exactly 10 words present in this line here'}]\n    # Test with unspecified max\n    page = {CONTENT: 'Short line\\n\\na b c d e f g h i j k\\n  There are exactly 10 words present in this line here\\nHi'}",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_punctuation_line_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_punctuation_line_modifier():\n    # punctuation_line_modifier is a factory function\n    modifier = punctuation_line_modifier()\n    # Test with empty text\n    page = {CONTENT: ''}\n    assert modifier(page) == []\n    # Test with various lines\n    page = {\n        CONTENT: 'Period.\\nQuestion?\\nExclamation!\\n\\n\"Quotation\"\\nNothing\\nWhitespace.  \\nEllipsis...\\nEllipsis\\u2026'}\n    assert modifier(page) == [{",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_word_length_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_word_length_modifier():\n    # Test with empty text\n    page = {CONTENT: ''}\n    assert word_length_modifier(page, 10, model='split') == []\n    # Test with various lines\n    page = {CONTENT: 'Regular line    \\n\\nVerylongword and regular words\\nBaaaaarely in the limit.'}\n    assert word_length_modifier(page, 10, ignore_punctuation=True, model='split') == [\n        {CONTENT: 'Regular line    \\n\\nBaaaaarely in the limit.'}]\n    # Test with no 'text' in page\n    page = {}",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_counter_line_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_counter_line_modifier():\n    counter_remover = counter_line_modifier()\n    # Test with empty text\n    page = {CONTENT: ''}\n    assert counter_remover(page) == []\n    # Test with various different counters and number types\n    for counter_type in [\"likes\", \"shares\", \"comments\", \"retweets\", \"reposts\", \"quotes\", \"bookmarks\", \"upvotes\",\n                         \"downvotes\", \"downloads\", \"views\", \"followers\"]:\n        page = {\n            CONTENT: f'3 {counter_type}\\n3,000 {counter_type}\\n123K {counter_type}!\\n 5.1M   {counter_type}  \\nThis line does not contain a counter.'}",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_uppercase_ratio_line_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_uppercase_ratio_line_modifier():\n    # Test with empty text\n    page = {CONTENT: ''}\n    assert uppercase_ratio_line_modifier(page) == []\n    # Test single line case\n    page = {CONTENT: 'Hello, World'}\n    assert uppercase_ratio_line_modifier(page, 1.0) == [{CONTENT: 'Hello, World'}]\n    assert uppercase_ratio_line_modifier(page, 2 / 12) == [{CONTENT: 'Hello, World'}]\n    assert uppercase_ratio_line_modifier(page, 2 / 12 - 1e-6) == []\n    assert uppercase_ratio_line_modifier(page, 0.0) == []",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_numeric_ratio_line_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_numeric_ratio_line_modifier():\n    # Test with empty text\n    page = {CONTENT: ''}\n    assert numeric_ratio_line_modifier(page) == []\n    # Test with line of all numbers (as per the RefinedWeb rule)\n    page = {CONTENT: '12345'}\n    assert numeric_ratio_line_modifier(page, 1.0) == [{CONTENT: '12345'}]\n    assert numeric_ratio_line_modifier(page, 1.0 - 1e-6) == []\n    assert numeric_ratio_line_modifier(page, 0.0) == []\n    # Test whether it removes only the incorrect line and handles extra line breaks",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_citation_removal_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_citation_removal_modifier():\n    # Test with empty text\n    page = {CONTENT: ''}\n    assert citation_removal_modifier()(page) == []\n    # Test with various lines\n    page = {\n        CONTENT: '[edit]This is what a wiki citation looks like[134].\\nStudies show it helps to remove it for LLM training[citation needed].'}\n    assert citation_removal_modifier()(page) == [\n        {CONTENT: 'This is what a wiki citation looks like.\\nStudies show it helps to remove it for LLM training.'}]\n    # Test with no 'text' in page",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_url_removal_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_url_removal_modifier():\n    url_remover = url_removal_modifier()\n    # Test with empty text\n    page = {CONTENT: ''}\n    assert url_remover(page) == []\n    # Test with various url formats (generated by chatGPT)\n    test_urls = [\n        'https://www.example.com',\n        'www.example.org',\n        'example.co.uk'",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_within_page_dedup",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_within_page_dedup():\n    # Line-level: check whether it correctly ignores whitespace and case\n    page = {\n        CONTENT: 'This line is duplicated.\\n\\nThis line is duplicated.\\n This line is duplicated.  \\n\\nthis line is duplicated.\\n This line is kept.'}\n    assert within_page_dedup(page, 'line') == [{CONTENT: 'This line is duplicated.\\n\\n\\n This line is kept.'}]\n    # Line-level: check whether it checks for whitespace and case\n    page = {\n        CONTENT: 'This line is duplicated.\\nThis line is duplicated.\\n This line is duplicated.  \\n\\nthis line is duplicated.\\nThis line is kept.'}\n    assert within_page_dedup(page, 'line', normalize=False) == [{\n        CONTENT: 'This line is duplicated.\\n This line is duplicated.  \\n\\nthis line is duplicated.\\nThis line is kept.'}]",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_newline_removal_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_newline_removal_modifier():\n    modifier = newline_removal_modifier()  # Get the actual modification function\n    modifier_single_newlines = newline_removal_modifier(max_consecutive=1)  # Get the actual modification function\n    # Page with empty CONTENT\n    empty_page = {CONTENT: ''}\n    assert modifier(empty_page) == [{CONTENT: ''}]\n    assert modifier_single_newlines(empty_page) == [{CONTENT: ''}]\n    # Page with no newlines\n    no_newline_page = {CONTENT: 'Hello, World'}\n    assert modifier(no_newline_page) == [{CONTENT: 'Hello, World'}]",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_split_lines_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_split_lines_modifier():\n    # Typical use case, when delimiter is \\n\n    page = {CONTENT: 'line1\\nline2\\n\\nline3'}\n    assert split_lines_modifier(page) == [{CONTENT: ['line1', 'line2', '', 'line3']}]\n    # Changing the delimiter to \\n\\n\n    page = {CONTENT: 'line1\\nline2\\n\\nline3'}\n    assert split_lines_modifier(page, delimiter = '\\n\\n') == [{CONTENT: ['line1\\nline2', 'line3']}]\n    # When CONTENT is already in the form of a list, do not make a modification\n    page = {CONTENT: ['line1', 'line2', 'line3']}\n    assert split_lines_modifier(page, delimiter = '\\n\\n') == [{CONTENT: ['line1', 'line2', 'line3']}]",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_join_lines_modifier",
        "kind": 2,
        "importPath": "tests.baselines.mappers.modifiers.test_modifiers",
        "description": "tests.baselines.mappers.modifiers.test_modifiers",
        "peekOfCode": "def test_join_lines_modifier():\n    # Typical use case, when delimiter is \\n\n    page = {CONTENT: ['line1', 'line2', '', 'line3']} \n    assert join_lines_modifier(page) == [{CONTENT: 'line1\\nline2\\n\\nline3'}]\n    # Changing the delimiter to \\n\\n\n    page = {CONTENT: ['line1\\nline2', 'line3']}\n    assert join_lines_modifier(page, delimiter = '\\n\\n') == [{CONTENT: 'line1\\nline2\\n\\nline3'}]\n    # When CONTENT is already in the form of a list, do not make a modification\n    page = {CONTENT: 'line1\\nline2\\n\\nline3'}\n    assert join_lines_modifier(page, delimiter = '\\n\\n') == [{CONTENT:  'line1\\nline2\\n\\nline3'}]",
        "detail": "tests.baselines.mappers.modifiers.test_modifiers",
        "documentation": {}
    },
    {
        "label": "test_split_paragraphs",
        "kind": 2,
        "importPath": "tests.baselines.mappers.test_core_utils",
        "description": "tests.baselines.mappers.test_core_utils",
        "peekOfCode": "def test_split_paragraphs():\n    # Test case 1\n    result = split_paragraphs('Hello\\n\\nWorld and all\\nand beyond')\n    assert result == ['Hello', 'World and all',\n                      'and beyond'], f\"Expected ['Hello', 'World and all', 'and beyond'], but got {result}\"\n    # Test case 2\n    result = split_paragraphs('Hello\\n\\nWorld and all\\nand beyond', remove_empty=False)\n    assert result == ['Hello', '', 'World and all',\n                      'and beyond'], f\"Expected ['Hello', '', 'World and all', 'and beyond'], but got {result}\"\n    # Test case 3",
        "detail": "tests.baselines.mappers.test_core_utils",
        "documentation": {}
    },
    {
        "label": "test_split_sentences",
        "kind": 2,
        "importPath": "tests.baselines.mappers.test_core_utils",
        "description": "tests.baselines.mappers.test_core_utils",
        "peekOfCode": "def test_split_sentences():\n    # Test case 1\n    result = split_sentences(\n        'Hello. World and all. And beyond. A sentence with an abbreviation,'\n        ' e.g. a sentence. Also one with a 1.4 floating points?')\n    expected_result = ['Hello.', 'World and all.', 'And beyond.', 'A sentence with an abbreviation, e.g. a sentence.',\n                       'Also one with a 1.4 floating points?']\n    assert result == expected_result, f\"Expected {expected_result}, but got {result}\"\n    # Additional test cases:\n    # Test with an empty string",
        "detail": "tests.baselines.mappers.test_core_utils",
        "documentation": {}
    },
    {
        "label": "test_split_words",
        "kind": 2,
        "importPath": "tests.baselines.mappers.test_core_utils",
        "description": "tests.baselines.mappers.test_core_utils",
        "peekOfCode": "def test_split_words():\n    # Test case 1\n    result = len(split_words('Hello World and all', model='uniseg', ignore_punctuation=True))\n    assert result == 4, f\"Expected 4, but got {result}\"\n    # Additional test cases:\n    # Test with different punctuation and whitespace settings\n    result = len(split_words('Also one with complications e.g., a 1.4 floating points...', model='uniseg',\n                             ignore_punctuation=True))\n    assert result == 9, f\"Expected 9, but got {result}\"\n    result = len(split_words('Also one with complications e.g., a 1.4 floating points...', model='uniseg',",
        "detail": "tests.baselines.mappers.test_core_utils",
        "documentation": {}
    },
    {
        "label": "test_join_sentences",
        "kind": 2,
        "importPath": "tests.baselines.mappers.test_core_utils",
        "description": "tests.baselines.mappers.test_core_utils",
        "peekOfCode": "def test_join_sentences():\n    # Test case 1\n    result = join_sentences(['Hello', 'World and all', 'and beyond'])\n    assert result == 'Hello World and all and beyond', f\"Expected 'Hello World and all and beyond', but got {result}\"\n    # Additional test cases:\n    # Test with an empty list\n    result = join_sentences([])\n    assert result == '', f\"Expected '', but got {result}\"\n    # Test with a list of one sentence\n    result = join_sentences(['Hello'])",
        "detail": "tests.baselines.mappers.test_core_utils",
        "documentation": {}
    },
    {
        "label": "test_join_paragraphs",
        "kind": 2,
        "importPath": "tests.baselines.mappers.test_core_utils",
        "description": "tests.baselines.mappers.test_core_utils",
        "peekOfCode": "def test_join_paragraphs():\n    # Test case 1\n    result = join_paragraphs(['Hello', 'World and all', 'and beyond'])\n    expected = 'Hello\\nWorld and all\\nand beyond'\n    assert result == expected, f\"Expected {expected}, but got {result}\"\n    # Additional test cases:\n    # Test with an empty list\n    result = join_paragraphs([])\n    assert result == '', f\"Expected '', but got {result}\"\n    # Test with a list of one paragraph",
        "detail": "tests.baselines.mappers.test_core_utils",
        "documentation": {}
    },
    {
        "label": "test_normalize_url",
        "kind": 2,
        "importPath": "tests.baselines.mappers.test_core_utils",
        "description": "tests.baselines.mappers.test_core_utils",
        "peekOfCode": "def test_normalize_url():\n    # Reference URL for testing first regex rule r\"https?:\\/\\/(www\\.)?\"\n    url_1 = 'https://www.google.com'\n    normalized_url_1 = normalize_url(url_1)\n    # http instead of https\n    url_2 = 'http://www.google.com'\n    assert normalized_url_1 == normalize_url(url_2)\n    # http instead of https\n    url_2 = 'google.com'\n    assert normalized_url_1 == normalize_url(url_2)",
        "detail": "tests.baselines.mappers.test_core_utils",
        "documentation": {}
    },
    {
        "label": "test_normalize_whitespace_and_lowercase",
        "kind": 2,
        "importPath": "tests.baselines.mappers.test_core_utils",
        "description": "tests.baselines.mappers.test_core_utils",
        "peekOfCode": "def test_normalize_whitespace_and_lowercase():\n    # Reference text\n    text_1 = 'This is a piece of sample text.  '\n    normalized_text_1 = normalize_whitespace_and_lowercase(text_1)\n    # All lowercase\n    text_2 = 'this is a piece of sample text.'\n    assert normalized_text_1 == normalize_whitespace_and_lowercase(text_2)\n    # All uppercase\n    text_2 = 'THIS IS A PIECE OF SAMPLE TEXT.'\n    assert normalized_text_1 == normalize_whitespace_and_lowercase(text_2)",
        "detail": "tests.baselines.mappers.test_core_utils",
        "documentation": {}
    },
    {
        "label": "test_normalize_timestamps",
        "kind": 2,
        "importPath": "tests.baselines.mappers.test_core_utils",
        "description": "tests.baselines.mappers.test_core_utils",
        "peekOfCode": "def test_normalize_timestamps():\n    # Reference timestamp\n    timestamp_1 = '2013-12-05T04:56:51Z'\n    normalized_timestamp_1 = normalize_timestamps(timestamp_1)\n    # Difference is in the second\n    timestamp_2 = '2013-12-05T04:56:50Z'\n    assert normalized_timestamp_1 > normalize_timestamps(timestamp_2)\n    # Difference is in the minute\n    timestamp_2 = '2013-12-05T04:55:51Z'\n    assert normalized_timestamp_1 > normalize_timestamps(timestamp_2)",
        "detail": "tests.baselines.mappers.test_core_utils",
        "documentation": {}
    },
    {
        "label": "test_ccnet_dedup_normalizer",
        "kind": 2,
        "importPath": "tests.baselines.mappers.test_core_utils",
        "description": "tests.baselines.mappers.test_core_utils",
        "peekOfCode": "def test_ccnet_dedup_normalizer():\n    weird = \"０２３´∶：\\x10 | ;012 hèllo\"\n    normalized = \"000 | ;000 hello\"\n    assert normalized == ccnet_dedup_normalizer(weird)\n    weird = \"😃 kožušček\"\n    normalized = \" kozuscek\"\n    assert normalized == ccnet_dedup_normalizer(weird)\n    weird = \"北亰 29 \\xa0\"\n    normalized = \"Bei Jing  00\"\n    assert normalized == ccnet_dedup_normalizer(weird)",
        "detail": "tests.baselines.mappers.test_core_utils",
        "documentation": {}
    },
    {
        "label": "test_percentiles",
        "kind": 2,
        "importPath": "tests.baselines.test_aggregators",
        "description": "tests.baselines.test_aggregators",
        "peekOfCode": "def test_percentiles():\n    # Test with integer values\n    values = [1, 2, 3, 4, 5]\n    result = percentiles(values)\n    assert result == {'min': 1, 'max': 5, 'median': 3, 'p25': 2, 'p75': 4, 'p90': 5, 'p95': 5, 'p99': 5, 'mean': 3}\n    # Test with floating point values\n    values = [1.0, 2.0, 3.0, 4.0, 5.0]\n    result = percentiles(values)\n    assert result == {'min': 1.0, 'max': 5.0, 'median': 3.0, 'p25': 2.0, 'p75': 4.0, 'p90': 5.0, 'p95': 5.0, 'p99': 5.0, 'mean': 3.0}\n    # Test with negative values",
        "detail": "tests.baselines.test_aggregators",
        "documentation": {}
    },
    {
        "label": "test_histogram",
        "kind": 2,
        "importPath": "tests.baselines.test_aggregators",
        "description": "tests.baselines.test_aggregators",
        "peekOfCode": "def test_histogram():\n    # Test with integer values\n    values = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]\n    result = histogram(values)\n    assert isinstance(result, dict)\n    assert len(result) == 10\n    # Test with string values\n    values = ['a', 'b', 'b', 'c', 'c', 'c']\n    result = histogram(values)\n    assert result == Counter(values)",
        "detail": "tests.baselines.test_aggregators",
        "documentation": {}
    },
    {
        "label": "file_cleanup",
        "kind": 2,
        "importPath": "tests.baselines.test_processor",
        "description": "tests.baselines.test_processor",
        "peekOfCode": "def file_cleanup():\n    os.makedirs(TMP_OUTPUT_DIR, exist_ok=True)\n    yield\n    # The cleanup code below will run regardless of the test outcome\n    if os.path.exists(TMP_OUTPUT_DIR):\n        shutil.rmtree(TMP_OUTPUT_DIR)\n@pytest.mark.parametrize(\"workers\", [1, 2],\n                         ids=[\"Sequential processing\", \"Parallel processing\"])\ndef test_main_with_args(file_cleanup, workers):\n    assert \"tests\" not in os.getcwd(), 'Please run this test from the root directory of the project'",
        "detail": "tests.baselines.test_processor",
        "documentation": {}
    },
    {
        "label": "test_main_with_args",
        "kind": 2,
        "importPath": "tests.baselines.test_processor",
        "description": "tests.baselines.test_processor",
        "peekOfCode": "def test_main_with_args(file_cleanup, workers):\n    assert \"tests\" not in os.getcwd(), 'Please run this test from the root directory of the project'\n    yaml = os.path.join(DATA_BASE_PATH, 'example_config.yaml')\n    raw_data_dirpath = os.path.join(DATA_BASE_PATH, 'example_data/root_data_dir')  # this will not be mirrored in the output dir hierarchy\n    jsonl = 'subdir1/shard1.jsonl'  # but this will\n    # Run the main function with the parsed arguments\n    output_path, stats_output_path = process_single_file(\n        config_path=yaml,\n        raw_data_dirpath=raw_data_dirpath,\n        jsonl_relpath=jsonl,",
        "detail": "tests.baselines.test_processor",
        "documentation": {}
    },
    {
        "label": "TMP_OUTPUT_DIR",
        "kind": 5,
        "importPath": "tests.baselines.test_processor",
        "description": "tests.baselines.test_processor",
        "peekOfCode": "TMP_OUTPUT_DIR = './tmpoutputs'\nDATA_BASE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data')\n@pytest.fixture\ndef file_cleanup():\n    os.makedirs(TMP_OUTPUT_DIR, exist_ok=True)\n    yield\n    # The cleanup code below will run regardless of the test outcome\n    if os.path.exists(TMP_OUTPUT_DIR):\n        shutil.rmtree(TMP_OUTPUT_DIR)\n@pytest.mark.parametrize(\"workers\", [1, 2],",
        "detail": "tests.baselines.test_processor",
        "documentation": {}
    },
    {
        "label": "DATA_BASE_PATH",
        "kind": 5,
        "importPath": "tests.baselines.test_processor",
        "description": "tests.baselines.test_processor",
        "peekOfCode": "DATA_BASE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data')\n@pytest.fixture\ndef file_cleanup():\n    os.makedirs(TMP_OUTPUT_DIR, exist_ok=True)\n    yield\n    # The cleanup code below will run regardless of the test outcome\n    if os.path.exists(TMP_OUTPUT_DIR):\n        shutil.rmtree(TMP_OUTPUT_DIR)\n@pytest.mark.parametrize(\"workers\", [1, 2],\n                         ids=[\"Sequential processing\", \"Parallel processing\"])",
        "detail": "tests.baselines.test_processor",
        "documentation": {}
    },
    {
        "label": "get_commoncrawl_paths",
        "kind": 2,
        "importPath": "tools.commoncrawl.gen_common_crawl_paths",
        "description": "tools.commoncrawl.gen_common_crawl_paths",
        "peekOfCode": "def get_commoncrawl_paths():\n    s3_client = boto3.client(\"s3\")\n    bucket_name = \"commoncrawl\"\n    base_prefix = \"crawl-data/\"\n    paths = []\n    # Create a paginator for listing objects\n    paginator = s3_client.get_paginator(\"list_objects_v2\")\n    # Use the paginator to list all the crawl directories\n    for page in paginator.paginate(Bucket=bucket_name, Prefix=base_prefix, Delimiter=\"/\"):\n        for crawl in tqdm(page.get(\"CommonPrefixes\", [])):",
        "detail": "tools.commoncrawl.gen_common_crawl_paths",
        "documentation": {}
    },
    {
        "label": "APPROX_TOKENS_PER_WARC",
        "kind": 5,
        "importPath": "tools.commoncrawl.gen_common_crawl_paths",
        "description": "tools.commoncrawl.gen_common_crawl_paths",
        "peekOfCode": "APPROX_TOKENS_PER_WARC = 90000000\ndef get_commoncrawl_paths():\n    s3_client = boto3.client(\"s3\")\n    bucket_name = \"commoncrawl\"\n    base_prefix = \"crawl-data/\"\n    paths = []\n    # Create a paginator for listing objects\n    paginator = s3_client.get_paginator(\"list_objects_v2\")\n    # Use the paginator to list all the crawl directories\n    for page in paginator.paginate(Bucket=bucket_name, Prefix=base_prefix, Delimiter=\"/\"):",
        "detail": "tools.commoncrawl.gen_common_crawl_paths",
        "documentation": {}
    },
    {
        "label": "convert_warc_to_wet",
        "kind": 2,
        "importPath": "tools.commoncrawl.process_common_crawl_source",
        "description": "tools.commoncrawl.process_common_crawl_source",
        "peekOfCode": "def convert_warc_to_wet(warc_path):\n    return warc_path.replace(\"/warc/\", \"/wet/\").replace(\".warc.gz\", \".warc.wet.gz\")\ndef open_file(path, mode=\"rb\"):\n    if path.startswith(\"s3://\"):\n        s3 = boto3.resource(\"s3\")\n        bucket_name, key = path[5:].split(\"/\", 1)\n        obj = s3.Object(bucket_name, key)\n        if mode == \"rb\":\n            return BytesIO(obj.get()[\"Body\"].read())\n        else:",
        "detail": "tools.commoncrawl.process_common_crawl_source",
        "documentation": {}
    },
    {
        "label": "open_file",
        "kind": 2,
        "importPath": "tools.commoncrawl.process_common_crawl_source",
        "description": "tools.commoncrawl.process_common_crawl_source",
        "peekOfCode": "def open_file(path, mode=\"rb\"):\n    if path.startswith(\"s3://\"):\n        s3 = boto3.resource(\"s3\")\n        bucket_name, key = path[5:].split(\"/\", 1)\n        obj = s3.Object(bucket_name, key)\n        if mode == \"rb\":\n            return BytesIO(obj.get()[\"Body\"].read())\n        else:\n            return obj\n    else:",
        "detail": "tools.commoncrawl.process_common_crawl_source",
        "documentation": {}
    },
    {
        "label": "write_output",
        "kind": 2,
        "importPath": "tools.commoncrawl.process_common_crawl_source",
        "description": "tools.commoncrawl.process_common_crawl_source",
        "peekOfCode": "def write_output(output_path, data, mode=\"wt\"):\n    if output_path.startswith(\"s3://\"):\n        with open_file(output_path, \"wb\") as f:\n            f.put(Body=gzip.compress(\"\\n\".join(data).encode(\"utf-8\")))\n    else:\n        with gzip.open(output_path, mode) as f:\n            f.write(\"\\n\".join(data))\ndef process_file(path, documents_per_jsonl, is_wet, output_dir):\n    output_file_template = os.path.join(output_dir, os.path.basename(path).replace(\".gz\", \"\") + \"_{}.jsonl.gz\")\n    with GZipStream(open_file(path)) as stream:",
        "detail": "tools.commoncrawl.process_common_crawl_source",
        "documentation": {}
    },
    {
        "label": "process_file",
        "kind": 2,
        "importPath": "tools.commoncrawl.process_common_crawl_source",
        "description": "tools.commoncrawl.process_common_crawl_source",
        "peekOfCode": "def process_file(path, documents_per_jsonl, is_wet, output_dir):\n    output_file_template = os.path.join(output_dir, os.path.basename(path).replace(\".gz\", \"\") + \"_{}.jsonl.gz\")\n    with GZipStream(open_file(path)) as stream:\n        record_type_filter = (\n            WarcRecordType.conversion if is_wet else WarcRecordType.response\n        ) | WarcRecordType.warcinfo\n        iterator = ArchiveIterator(stream, record_types=record_type_filter)\n        count = 0\n        file_index = 1\n        jsonl_content = []",
        "detail": "tools.commoncrawl.process_common_crawl_source",
        "documentation": {}
    },
    {
        "label": "load_json_file",
        "kind": 2,
        "importPath": "tools.commoncrawl.process_common_crawl_source",
        "description": "tools.commoncrawl.process_common_crawl_source",
        "peekOfCode": "def load_json_file(json_file_path):\n    with open(json_file_path, \"r\") as file:\n        data = json.load(file)\n    return data.get(\"dataset_urls\", [])\ndef main():\n    parser = argparse.ArgumentParser(description=\"Convert WARC/WET to JSONL.GZ with metadata\")\n    parser.add_argument(\"json_file_path\", help=\"Path to the JSON file containing WARC/WET paths\")\n    parser.add_argument(\n        \"--documents_per_jsonl\",\n        type=int,",
        "detail": "tools.commoncrawl.process_common_crawl_source",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.commoncrawl.process_common_crawl_source",
        "description": "tools.commoncrawl.process_common_crawl_source",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"Convert WARC/WET to JSONL.GZ with metadata\")\n    parser.add_argument(\"json_file_path\", help=\"Path to the JSON file containing WARC/WET paths\")\n    parser.add_argument(\n        \"--documents_per_jsonl\",\n        type=int,\n        default=1000,\n        help=\"Number of documents per JSONL file\",\n    )\n    parser.add_argument(\"--wet\", action=\"store_true\", help=\"Indicate if the files are WET format\")",
        "detail": "tools.commoncrawl.process_common_crawl_source",
        "documentation": {}
    },
    {
        "label": "GlobalCounter",
        "kind": 6,
        "importPath": "tools.commoncrawl.process_common_crawl_w_ray",
        "description": "tools.commoncrawl.process_common_crawl_w_ray",
        "peekOfCode": "class GlobalCounter:\n    def __init__(self):\n        self.value = 0\n        self.token_count = 0\n    def increment(self):\n        self.value += 1\n        return self.value\n    def increment_token_count(self, num_tokens):\n        self.token_count += num_tokens\n        return self.token_count",
        "detail": "tools.commoncrawl.process_common_crawl_w_ray",
        "documentation": {}
    },
    {
        "label": "convert_warc_to_wet",
        "kind": 2,
        "importPath": "tools.commoncrawl.process_common_crawl_w_ray",
        "description": "tools.commoncrawl.process_common_crawl_w_ray",
        "peekOfCode": "def convert_warc_to_wet(warc_path): \n    return warc_path.replace(\"/warc/\", \"/wet/\").replace(\".warc.gz\", \".warc.wet.gz\")\ndef open_file(path, mode=\"rb\"):\n    if path.startswith(\"s3://\"):\n        s3 = boto3.resource(\"s3\")\n        bucket_name, key = path[5:].split(\"/\", 1)\n        obj = s3.Object(bucket_name, key)\n        if mode == \"rb\":\n            return BytesIO(obj.get()[\"Body\"].read())\n        else:",
        "detail": "tools.commoncrawl.process_common_crawl_w_ray",
        "documentation": {}
    },
    {
        "label": "open_file",
        "kind": 2,
        "importPath": "tools.commoncrawl.process_common_crawl_w_ray",
        "description": "tools.commoncrawl.process_common_crawl_w_ray",
        "peekOfCode": "def open_file(path, mode=\"rb\"):\n    if path.startswith(\"s3://\"):\n        s3 = boto3.resource(\"s3\")\n        bucket_name, key = path[5:].split(\"/\", 1)\n        obj = s3.Object(bucket_name, key)\n        if mode == \"rb\":\n            return BytesIO(obj.get()[\"Body\"].read())\n        else:\n            return obj\n    else:",
        "detail": "tools.commoncrawl.process_common_crawl_w_ray",
        "documentation": {}
    },
    {
        "label": "write_output",
        "kind": 2,
        "importPath": "tools.commoncrawl.process_common_crawl_w_ray",
        "description": "tools.commoncrawl.process_common_crawl_w_ray",
        "peekOfCode": "def write_output(output_path, data, mode=\"wt\"):\n    if output_path.startswith(\"s3://\"):\n        f = open_file(output_path, \"wb\")\n        f.put(Body=gzip.compress(\"\\n\".join(data).encode(\"utf-8\")))\n    else:\n        with gzip.open(output_path, mode) as f:\n            f.write(\"\\n\".join(data))\n@ray.remote\nclass GlobalCounter:\n    def __init__(self):",
        "detail": "tools.commoncrawl.process_common_crawl_w_ray",
        "documentation": {}
    },
    {
        "label": "process_file_batch",
        "kind": 2,
        "importPath": "tools.commoncrawl.process_common_crawl_w_ray",
        "description": "tools.commoncrawl.process_common_crawl_w_ray",
        "peekOfCode": "def process_file_batch(path, documents_per_jsonl, is_wet, output_dir, counter):\n    jitter = random.uniform(1, 30)\n    time.sleep(jitter)\n    s3 = boto3.resource('s3')\n    rets = []\n    for p in path[\"path\"]:\n        ret = process_file(s3, p, documents_per_jsonl, is_wet, output_dir, counter)\n        rets.append(ret)\n    return {\"data\": rets}\ndef process_file(s3, path, documents_per_jsonl, is_wet, output_dir, counter):",
        "detail": "tools.commoncrawl.process_common_crawl_w_ray",
        "documentation": {}
    },
    {
        "label": "process_file",
        "kind": 2,
        "importPath": "tools.commoncrawl.process_common_crawl_w_ray",
        "description": "tools.commoncrawl.process_common_crawl_w_ray",
        "peekOfCode": "def process_file(s3, path, documents_per_jsonl, is_wet, output_dir, counter):\n    if args.wet:\n        path = convert_warc_to_wet(path)\n    s = time.time()\n    # basename alone has collisions\n    short_md5 = hashlib.md5(path.encode()).hexdigest()[:7]\n    assert path.endswith(\".gz\")\n    hash_path = path[:-3] + f\"_{short_md5}\" + \".gz\"\n    output_file_check = os.path.join(\n        output_dir.rstrip('/') + \"_check\", os.path.basename(hash_path).replace(\".gz\", \"\") + \".stat\"",
        "detail": "tools.commoncrawl.process_common_crawl_w_ray",
        "documentation": {}
    },
    {
        "label": "load_json_file",
        "kind": 2,
        "importPath": "tools.commoncrawl.process_common_crawl_w_ray",
        "description": "tools.commoncrawl.process_common_crawl_w_ray",
        "peekOfCode": "def load_json_file(json_file_path):\n    with open(json_file_path, \"r\") as file:\n        data = json.load(file)\n    return data.get(\"dataset_urls\", [])\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Convert WARC/WET to JSONL.GZ with metadata\")\n    parser.add_argument(\n        \"--json_file_path\", type=str, required=True, help=\"Path to the JSON file containing WARC/WET paths\"\n    )\n    parser.add_argument(",
        "detail": "tools.commoncrawl.process_common_crawl_w_ray",
        "documentation": {}
    },
    {
        "label": "read_keys_from_file",
        "kind": 2,
        "importPath": "tools.commoncrawl.sample_source_keys",
        "description": "tools.commoncrawl.sample_source_keys",
        "peekOfCode": "def read_keys_from_file(file_path, regex_pattern):\n    try:\n        compiled_pattern = re.compile(regex_pattern)\n    except re.error as e:\n        print(f\"Error compiling regex pattern: {e}\")\n        return []\n    keys = []\n    with fsspec.open(file_path, \"rb\") as file:\n        if file_path.endswith(\".gz\"):\n            with gzip.GzipFile(fileobj=io.BytesIO(file.read())) as gz:",
        "detail": "tools.commoncrawl.sample_source_keys",
        "documentation": {}
    },
    {
        "label": "list_matching_s3_keys",
        "kind": 2,
        "importPath": "tools.commoncrawl.sample_source_keys",
        "description": "tools.commoncrawl.sample_source_keys",
        "peekOfCode": "def list_matching_s3_keys(bucket, prefix=\"\", regex_pattern=\"\"):\n    \"\"\"List keys in an S3 bucket that match the given regex pattern.\"\"\"\n    s3_client = boto3.client(\"s3\")\n    paginator = s3_client.get_paginator(\"list_objects_v2\")\n    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n        for obj in tqdm(page.get(\"Contents\", [])):\n            key = obj[\"Key\"]\n            if re.match(regex_pattern, key):\n                yield key\ndef sample_keys(keys, subset_size, seed):",
        "detail": "tools.commoncrawl.sample_source_keys",
        "documentation": {}
    },
    {
        "label": "sample_keys",
        "kind": 2,
        "importPath": "tools.commoncrawl.sample_source_keys",
        "description": "tools.commoncrawl.sample_source_keys",
        "peekOfCode": "def sample_keys(keys, subset_size, seed):\n    \"\"\"Randomly sample a subset of keys.\"\"\"\n    random.seed(seed)\n    return random.sample(keys, min(subset_size, len(keys)))\ndef get_git_info():\n    \"\"\"Get the current git commit hash and diff.\"\"\"\n    commit_hash = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).strip().decode()\n    git_diff = subprocess.check_output([\"git\", \"diff\"]).strip().decode()\n    return commit_hash, git_diff\ndef generate_dataset_json(sampled_keys, name, file_path, bucket, prefix, seed):",
        "detail": "tools.commoncrawl.sample_source_keys",
        "documentation": {}
    },
    {
        "label": "get_git_info",
        "kind": 2,
        "importPath": "tools.commoncrawl.sample_source_keys",
        "description": "tools.commoncrawl.sample_source_keys",
        "peekOfCode": "def get_git_info():\n    \"\"\"Get the current git commit hash and diff.\"\"\"\n    commit_hash = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).strip().decode()\n    git_diff = subprocess.check_output([\"git\", \"diff\"]).strip().decode()\n    return commit_hash, git_diff\ndef generate_dataset_json(sampled_keys, name, file_path, bucket, prefix, seed):\n    \"\"\"Generate dataset JSON with mandatory keys.\"\"\"\n    if file_path:\n        source = file_path\n    else:",
        "detail": "tools.commoncrawl.sample_source_keys",
        "documentation": {}
    },
    {
        "label": "generate_dataset_json",
        "kind": 2,
        "importPath": "tools.commoncrawl.sample_source_keys",
        "description": "tools.commoncrawl.sample_source_keys",
        "peekOfCode": "def generate_dataset_json(sampled_keys, name, file_path, bucket, prefix, seed):\n    \"\"\"Generate dataset JSON with mandatory keys.\"\"\"\n    if file_path:\n        source = file_path\n    else:\n        source = f\"{bucket}/{prefix}\"\n    git_info = get_git_info()\n    dataset = {\n        \"uuid\": str(uuid.uuid4()),\n        \"name\": name,",
        "detail": "tools.commoncrawl.sample_source_keys",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.commoncrawl.sample_source_keys",
        "description": "tools.commoncrawl.sample_source_keys",
        "peekOfCode": "def main(\n    file_path, bucket, prefix, regex_pattern, seed, subset_size, name, exp_data_path\n):  # pylint: disable=too-many-arguments\n    \"\"\"\n    Main function to generate dataset json\n    \"\"\"\n    if file_path:\n        all_keys = read_keys_from_file(file_path, regex_pattern)\n    else:\n        all_keys = list(list_matching_s3_keys(bucket, prefix, regex_pattern))",
        "detail": "tools.commoncrawl.sample_source_keys",
        "documentation": {}
    },
    {
        "label": "modify_json",
        "kind": 2,
        "importPath": "tools.add_params_txt",
        "description": "tools.add_params_txt",
        "peekOfCode": "def modify_json(file_path):\n    # Read the JSON file\n    with open(file_path, \"r\") as file:\n        data = json.load(file)\n    # Extract the checkpoint_url\n    checkpoint_url = data.get(\"checkpoint_url\")\n    if checkpoint_url:\n        # Create the new params_url\n        params_url = \"/\".join(checkpoint_url.split(\"/\")[:-2]) + \"/params.txt\"\n        data[\"params_url\"] = params_url",
        "detail": "tools.add_params_txt",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.add_params_txt",
        "description": "tools.add_params_txt",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"Modify a JSON file to add a params_url key.\")\n    parser.add_argument(\"file_path\", help=\"Path to the JSON file\")\n    args = parser.parse_args()\n    modify_json(args.file_path)\nif __name__ == \"__main__\":\n    main()",
        "detail": "tools.add_params_txt",
        "documentation": {}
    },
    {
        "label": "helper",
        "kind": 2,
        "importPath": "tools.download_fineweb",
        "description": "tools.download_fineweb",
        "peekOfCode": "def helper(args):\n    idx, split_dataset = args\n    fname = f\"/tmp2/fineweb-edu-sample-350BT/fineweb-edu-sample-350BT-{idx:06}.jsonl.gz\"\n    split_dataset.to_json(fname, compression=\"gzip\")\n    return 0\nif __name__ == \"__main__\":\n    splits, examples = 50000, 339347842  # for sample-350BT\n    # splits, examples = 1000, 9672101 # for sample-10BT\n    bs = math.ceil(examples / splits)\n    fw = load_dataset(",
        "detail": "tools.download_fineweb",
        "documentation": {}
    },
    {
        "label": "tri_copy_model_via_hop",
        "kind": 2,
        "importPath": "tools.eval_expdb",
        "description": "tools.eval_expdb",
        "peekOfCode": "def tri_copy_model_via_hop(src, dst, profile):\n    if profile in [None, \"\"]:\n        profile_arg = \"\"\n    else:\n        profile_arg = f\"--profile {profile}\"\n    src_split = src.split(\"/\")\n    name_idx = -1\n    if src_split[name_idx].startswith(\"epoch_\") or src_split[name_idx].startswith(\"params\"):\n        name_idx -= 1\n    if src_split[name_idx].startswith(\"checkpoints\"):",
        "detail": "tools.eval_expdb",
        "documentation": {}
    },
    {
        "label": "load_models",
        "kind": 2,
        "importPath": "tools.eval_expdb",
        "description": "tools.eval_expdb",
        "peekOfCode": "def load_models(database_path, table, start_idx, end_idx, filters):\n    table_dfs = build_table_dfs(database_path, table)\n    merged_dfs = merge_uuid_references(table_dfs)\n    df = merged_dfs[table]  # Assuming 'models' is the table name\n    # Apply filters to the DataFrame\n    if filters:\n        df = filter_df(df, filters)\n    print(df)\n    return df.iloc[start_idx:end_idx]\ndef replace_prefix(s3_url, prefix_replacement):",
        "detail": "tools.eval_expdb",
        "documentation": {}
    },
    {
        "label": "replace_prefix",
        "kind": 2,
        "importPath": "tools.eval_expdb",
        "description": "tools.eval_expdb",
        "peekOfCode": "def replace_prefix(s3_url, prefix_replacement):\n    old_prefix, new_prefix = prefix_replacement.split(\"=\")\n    if s3_url.startswith(old_prefix):\n        return s3_url.replace(old_prefix, new_prefix, 1)\n    return s3_url\ndef download_from_s3(s3_url, output_dir, prefix_replacement=None, profile=None):\n    if prefix_replacement:\n        s3_url = replace_prefix(s3_url, prefix_replacement)\n    if profile is not None:\n        profile = f\"--profile {profile}\"",
        "detail": "tools.eval_expdb",
        "documentation": {}
    },
    {
        "label": "download_from_s3",
        "kind": 2,
        "importPath": "tools.eval_expdb",
        "description": "tools.eval_expdb",
        "peekOfCode": "def download_from_s3(s3_url, output_dir, prefix_replacement=None, profile=None):\n    if prefix_replacement:\n        s3_url = replace_prefix(s3_url, prefix_replacement)\n    if profile is not None:\n        profile = f\"--profile {profile}\"\n    else:\n        profile = \"\"\n    try:\n        local_filename = os.path.join(output_dir, s3_url.split(\"/\")[-1])\n        print(f\"Downloading {s3_url} to {local_filename}\")",
        "detail": "tools.eval_expdb",
        "documentation": {}
    },
    {
        "label": "download_checkpoint",
        "kind": 2,
        "importPath": "tools.eval_expdb",
        "description": "tools.eval_expdb",
        "peekOfCode": "def download_checkpoint(\n    model_row,\n    output_dir,\n    prefix_replacement,\n    tri_s3_path=None,\n    local_download=True,\n    profile=None,\n    checkpoint_replacement=None,\n):\n    checkpoint_url = model_row[\"checkpoint_url\"]",
        "detail": "tools.eval_expdb",
        "documentation": {}
    },
    {
        "label": "download_params",
        "kind": 2,
        "importPath": "tools.eval_expdb",
        "description": "tools.eval_expdb",
        "peekOfCode": "def download_params(model_row, output_dir, prefix_replacement, tri_s3_path=None, local_download=True, profile=None):\n    if tri_s3_path is not None:\n        params_url = tri_copy_model_via_hop(model_row[\"params_url\"], tri_s3_path, profile=profile)\n    else:\n        params_url = model_row[\"params_url\"]\n    if local_download and params_url.startswith(\"s3://\"):\n        return download_from_s3(params_url, output_dir, prefix_replacement, profile=profile)\n    else:\n        return params_url\ndef run_eval(",
        "detail": "tools.eval_expdb",
        "documentation": {}
    },
    {
        "label": "run_eval",
        "kind": 2,
        "importPath": "tools.eval_expdb",
        "description": "tools.eval_expdb",
        "peekOfCode": "def run_eval(\n    eval_script,\n    model_checkpoint,\n    tokenizer,\n    model_config,\n    eval_yaml,\n    params_file,\n    output_dir,\n    skip_perplexity,\n    averager_name,",
        "detail": "tools.eval_expdb",
        "documentation": {}
    },
    {
        "label": "modify_and_save_evaluation_json",
        "kind": 2,
        "importPath": "tools.eval_expdb",
        "description": "tools.eval_expdb",
        "peekOfCode": "def modify_and_save_evaluation_json(eval_data, model_uuid, output_dir, evaluation_name):\n    eval_data[\"model_uuid\"] = model_uuid\n    destination = os.path.join(output_dir, evaluation_name)\n    print(f\"Saving json to {destination}\")\n    with fsspec.open(destination, \"w\") as f:\n        json.dump(eval_data, f, indent=4)\ndef check_path_exists(path):\n    # Determine the file system type based on the path prefix\n    if path.startswith(\"s3://\"):\n        # S3 file system",
        "detail": "tools.eval_expdb",
        "documentation": {}
    },
    {
        "label": "check_path_exists",
        "kind": 2,
        "importPath": "tools.eval_expdb",
        "description": "tools.eval_expdb",
        "peekOfCode": "def check_path_exists(path):\n    # Determine the file system type based on the path prefix\n    if path.startswith(\"s3://\"):\n        # S3 file system\n        # Note: You need to have s3fs installed and AWS credentials set up\n        fs = fsspec.filesystem(\"s3\", anon=False)\n    else:\n        # Local file system\n        fs = fsspec.filesystem(\"file\")\n    # Check if the path exists",
        "detail": "tools.eval_expdb",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.eval_expdb",
        "description": "tools.eval_expdb",
        "peekOfCode": "def main(\n    database_path,\n    tri_s3_path,\n    table,\n    filters,\n    start_idx,\n    end_idx,\n    output_dir,\n    skip_perplexity,\n    averager_name,",
        "detail": "tools.eval_expdb",
        "documentation": {}
    },
    {
        "label": "load_smart_html",
        "kind": 2,
        "importPath": "tools.expdb",
        "description": "tools.expdb",
        "peekOfCode": "def load_smart_html(df):\n    # Specify fixed columns\n    fixed_columns = [\"_source_json\", \"uuid\", \"dataset_name\", \"model\", \"params\", \"tokens\", \"low_variance_datasets\"]\n    df.rename(columns={\"hyperparameters.params\": \"params\", \"hyperparameters.tokens\": \"tokens\"}, inplace=True)\n    # Generate column definitions for ag-Grid\n    column_defs = [\n        {\n            \"headerName\": col.replace(\"eval_metrics.icl\", \"icl\").replace(\"eval_metrics.downstream_perpexity\", \"ppl\"),\n            \"field\": col,\n        }",
        "detail": "tools.expdb",
        "documentation": {}
    },
    {
        "label": "load_json_to_df",
        "kind": 2,
        "importPath": "tools.expdb",
        "description": "tools.expdb",
        "peekOfCode": "def load_json_to_df(file_path, partition_keys):\n    with open(file_path, \"r\") as file:\n        df = pd.json_normalize(json.load(file))\n        df[\"_source_json\"] = file_path\n        for idx, key in enumerate(partition_keys, start=1):\n            df[f\"partition_key_{idx}\"] = key\n        return df\ndef build_table_dfs(database_path, table=None):\n    table_dfs = {}\n    for table_name in os.listdir(database_path):",
        "detail": "tools.expdb",
        "documentation": {}
    },
    {
        "label": "build_table_dfs",
        "kind": 2,
        "importPath": "tools.expdb",
        "description": "tools.expdb",
        "peekOfCode": "def build_table_dfs(database_path, table=None):\n    table_dfs = {}\n    for table_name in os.listdir(database_path):\n        if table is not None and table_name != table:\n            continue  # save redundant loading time\n        table_path = os.path.join(database_path, table_name)\n        if os.path.isdir(table_path):\n            dataframes = []\n            for root, dirs, files in os.walk(table_path):\n                partition_keys = os.path.relpath(root, table_path).split(os.sep)",
        "detail": "tools.expdb",
        "documentation": {}
    },
    {
        "label": "merge_uuid_references",
        "kind": 2,
        "importPath": "tools.expdb",
        "description": "tools.expdb",
        "peekOfCode": "def merge_uuid_references(table_dfs):\n    for table_name, df in table_dfs.items():\n        uuid_cols = [col for col in df.columns if col.endswith(\"_uuid\")]\n        for col in uuid_cols:\n            # Adjust for singular to plural mapping\n            ref_table_name = col.rsplit(\"_\", 1)[0] + \"s\"\n            if ref_table_name in table_dfs:\n                ref_df = table_dfs[ref_table_name]\n                if \"name\" in ref_df.columns:\n                    # Merge and keep only the name from the reference table",
        "detail": "tools.expdb",
        "documentation": {}
    },
    {
        "label": "convert_shorthand_to_number",
        "kind": 2,
        "importPath": "tools.expdb",
        "description": "tools.expdb",
        "peekOfCode": "def convert_shorthand_to_number(s):\n    multipliers = {\"K\": 1e3, \"M\": 1e6, \"B\": 1e9, \"T\": 1e12}\n    if s[-1] in multipliers:\n        return float(s[:-1]) * multipliers[s[-1]]\n    return float(s)\ndef filter_df(df, filter_str):\n    for f in filter_str:\n        if \"=\" in f and \">\" not in f and \"<\" not in f:\n            parts = f.split(\"=\", 1)\n            column_name, pattern = parts",
        "detail": "tools.expdb",
        "documentation": {}
    },
    {
        "label": "filter_df",
        "kind": 2,
        "importPath": "tools.expdb",
        "description": "tools.expdb",
        "peekOfCode": "def filter_df(df, filter_str):\n    for f in filter_str:\n        if \"=\" in f and \">\" not in f and \"<\" not in f:\n            parts = f.split(\"=\", 1)\n            column_name, pattern = parts\n            print(df)\n            print(column_name, pattern)\n            df = df[df[column_name].astype(str).str.contains(pattern, na=False)]\n        else:\n            parts = re.split(\"(>=|<=|>|<)\", f, maxsplit=1)",
        "detail": "tools.expdb",
        "documentation": {}
    },
    {
        "label": "enrich_evals",
        "kind": 2,
        "importPath": "tools.expdb",
        "description": "tools.expdb",
        "peekOfCode": "def enrich_evals(table_dfs, additional_model_keys=None):\n    # Enrich table with some extra information for evals.\n    #   - # tokens, # params\n    #   - chinchilla multiplier\n    #   - datasets\n    datasets_df = table_dfs[\"datasets\"].set_index(\"uuid\")\n    models_df = table_dfs[\"models\"].set_index(\"uuid\")\n    evals_df = table_dfs[\"evals\"].set_index(\"uuid\")\n    model_keys = [\"hyperparameters.tokens\", \"hyperparameters.params\", \"dataset_name\", \"dataset_uuid\"]\n    if additional_model_keys:",
        "detail": "tools.expdb",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.expdb",
        "description": "tools.expdb",
        "peekOfCode": "def main(\n    database_path=\"exp_data\",\n    table=\"models\",\n    columns=None,\n    leading_columns=None,\n    max_rows=None,\n    max_colwidth=50,\n    filter=None,\n    output_html=None,\n    output_csv=None,",
        "detail": "tools.expdb",
        "documentation": {}
    },
    {
        "label": "HTML_TEMPLATE",
        "kind": 5,
        "importPath": "tools.expdb",
        "description": "tools.expdb",
        "peekOfCode": "HTML_TEMPLATE = \"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"UTF-8\">\n<title>Data Table</title>\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.datatables.net/1.10.20/css/jquery.dataTables.css\">\n<script type=\"text/javascript\" charset=\"utf8\" src=\"https://code.jquery.com/jquery-3.3.1.js\"></script>\n<script type=\"text/javascript\" charset=\"utf8\" src=\"https://cdn.datatables.net/1.10.20/js/jquery.dataTables.js\"></script>\n</head>",
        "detail": "tools.expdb",
        "documentation": {}
    },
    {
        "label": "object_exists",
        "kind": 2,
        "importPath": "tools.migrate_expdb_s3_keys",
        "description": "tools.migrate_expdb_s3_keys",
        "peekOfCode": "def object_exists(bucket, key, aws_profile):\n    env = os.environ.copy()\n    env[\"AWS_PROFILE\"] = aws_profile\n    result = subprocess.run([\"aws\", \"s3\", \"ls\", f\"s3://{bucket}/{key}\"], capture_output=True, env=env)\n    return result.returncode == 0\ndef migrate_object(s3_key, source_profile, dest_profile, tmp_bucket, dest_bucket, tmp_prefix, dest_prefix, sync):\n    parsed_url = urlparse(s3_key)\n    source_bucket = parsed_url.netloc\n    object_key = parsed_url.path.lstrip(\"/\")\n    tmp_key = f\"s3://{tmp_bucket}/{tmp_prefix}/{object_key}\"",
        "detail": "tools.migrate_expdb_s3_keys",
        "documentation": {}
    },
    {
        "label": "migrate_object",
        "kind": 2,
        "importPath": "tools.migrate_expdb_s3_keys",
        "description": "tools.migrate_expdb_s3_keys",
        "peekOfCode": "def migrate_object(s3_key, source_profile, dest_profile, tmp_bucket, dest_bucket, tmp_prefix, dest_prefix, sync):\n    parsed_url = urlparse(s3_key)\n    source_bucket = parsed_url.netloc\n    object_key = parsed_url.path.lstrip(\"/\")\n    tmp_key = f\"s3://{tmp_bucket}/{tmp_prefix}/{object_key}\"\n    dest_key = f\"s3://{dest_bucket}/{dest_prefix}/{object_key}\"\n    # Check if object exists in destination\n    cmd = \"sync\" if sync else \"cp\"\n    if not sync and object_exists(dest_bucket, f\"{dest_prefix}/{object_key}\", dest_profile):\n        logger.info(f\"Object {dest_key} already exists in destination. Skipping migration.\")",
        "detail": "tools.migrate_expdb_s3_keys",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.migrate_expdb_s3_keys",
        "description": "tools.migrate_expdb_s3_keys",
        "peekOfCode": "def main(\n    source_profile, dest_profile, tmp_location, dest_location, table, column, database_path, filter, update_json, sync\n):\n    source_session = boto3.Session(profile_name=source_profile)\n    dest_session = boto3.Session(profile_name=dest_profile)\n    tmp_bucket = tmp_location.replace(\"s3://\", \"\").split(\"/\", 1)[0]\n    dest_bucket = dest_location.replace(\"s3://\", \"\").split(\"/\", 1)[0]\n    tmp_prefix = tmp_location.replace(f\"s3://{tmp_bucket}/\", \"\")\n    dest_prefix = dest_location.replace(f\"s3://{dest_bucket}/\", \"\")\n    num_cores = os.cpu_count()",
        "detail": "tools.migrate_expdb_s3_keys",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "tools.plot_decontamination",
        "description": "tools.plot_decontamination",
        "peekOfCode": "def parse_args(args):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--json-file-1\", type=str)\n    parser.add_argument(\"--json-file-2\", type=str)\n    parser.add_argument(\"--contamination-json\", type=str)\n    parser.add_argument(\"--contamination-summary\", type=str)\n    parser.add_argument(\"--output-file\", type=str)\n    parser.add_argument(\"--thresh\", type=float, default=0.8)\n    parser.add_argument(\"--eval-metadata\", type=str, default=\"eval/eval_meta_data.csv\")\n    args = parser.parse_args(args)",
        "detail": "tools.plot_decontamination",
        "documentation": {}
    },
    {
        "label": "get_aggregated_results",
        "kind": 2,
        "importPath": "tools.plot_decontamination",
        "description": "tools.plot_decontamination",
        "peekOfCode": "def get_aggregated_results(data, data_control, eval_metadata):\n    eval_metadata[\"results\"] = eval_metadata[\"Eval Task\"].map(data[\"eval_metrics\"][\"icl\"])\n    eval_metadata[\"results control\"] = eval_metadata[\"Eval Task\"].map(data_control[\"eval_metrics\"][\"icl\"])\n    eval_metadata[\"centered results\"] = (\n        eval_metadata[\"results\"].astype(float) - 0.01 * eval_metadata[\"Random baseline\"].astype(float)\n    ) / (1.0 - 0.01 * eval_metadata[\"Random baseline\"].astype(float))\n    eval_metadata[\"centered results control\"] = (\n        eval_metadata[\"results control\"].astype(float) - 0.01 * eval_metadata[\"Random baseline\"].astype(float)\n    ) / (1.0 - 0.01 * eval_metadata[\"Random baseline\"].astype(float))\n    eval_metadata[\"centered results diff\"] = (",
        "detail": "tools.plot_decontamination",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.plot_decontamination",
        "description": "tools.plot_decontamination",
        "peekOfCode": "def main(args):\n    args = parse_args(args)\n    with open(args.json_file_1, \"r\") as f:\n        data_1 = json.load(f)\n    with open(args.json_file_2, \"r\") as f:\n        data_2 = json.load(f)\n    df = pd.read_json(args.contamination_json, lines=True)\n    with open(args.contamination_summary, \"r\") as f:\n        overlaps_summary = json.load(f)\n    with open(\"eval/additional_aggregation.json\", \"r\") as f:",
        "detail": "tools.plot_decontamination",
        "documentation": {}
    },
    {
        "label": "upload_file_to_s3",
        "kind": 2,
        "importPath": "tools.push_JSC_model_remotely",
        "description": "tools.push_JSC_model_remotely",
        "peekOfCode": "def upload_file_to_s3(file_name, bucket, object_name=None):\n    \"\"\"\n    Upload a file to an S3 bucket\n    :param file_name: File to upload\n    :param bucket: Bucket to upload to\n    :param object_name: S3 object name. If not specified, file_name is used\n    :return: True if file was uploaded, else False\n    \"\"\"\n    if object_name is None:\n        object_name = file_name",
        "detail": "tools.push_JSC_model_remotely",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "tools.push_JSC_model_remotely",
        "description": "tools.push_JSC_model_remotely",
        "peekOfCode": "parser = argparse.ArgumentParser(description=\"JSC push models and update json\")\n# Add an argument for the JSON file path\nparser.add_argument(\"--json_path\", type=str, help=\"Path to the JSON file\")\nparser.add_argument(\"--s3_path\", type=str, help=\"Path to the s3 bucket for model\")\nargs = parser.parse_args()\nBUCKET_PATH = args.s3_path  # s3://dcnlp-west/dsir_wiki/experiments/models/CC_v3_random_resampled_3B-open_lm_160m-1.0/\nLOCAL_PATH = (\n    args.json_path\n)  # /p/home/jusers/garg4/juwels/garg4/scratch/code/dcnlp/exp_data/models/CC_v3_random_resampled_3B-open_lm_160m-1.0.json\nwith open(LOCAL_PATH, \"r\") as f:",
        "detail": "tools.push_JSC_model_remotely",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "tools.push_JSC_model_remotely",
        "description": "tools.push_JSC_model_remotely",
        "peekOfCode": "args = parser.parse_args()\nBUCKET_PATH = args.s3_path  # s3://dcnlp-west/dsir_wiki/experiments/models/CC_v3_random_resampled_3B-open_lm_160m-1.0/\nLOCAL_PATH = (\n    args.json_path\n)  # /p/home/jusers/garg4/juwels/garg4/scratch/code/dcnlp/exp_data/models/CC_v3_random_resampled_3B-open_lm_160m-1.0.json\nwith open(LOCAL_PATH, \"r\") as f:\n    data = json.load(f)\nlocal_model_path = Path(data[\"checkpoint_url\"])\nlocal_param_path = Path(data[\"params_url\"])\nbucket_name = BUCKET_PATH.split(\"/\")[2]",
        "detail": "tools.push_JSC_model_remotely",
        "documentation": {}
    },
    {
        "label": "BUCKET_PATH",
        "kind": 5,
        "importPath": "tools.push_JSC_model_remotely",
        "description": "tools.push_JSC_model_remotely",
        "peekOfCode": "BUCKET_PATH = args.s3_path  # s3://dcnlp-west/dsir_wiki/experiments/models/CC_v3_random_resampled_3B-open_lm_160m-1.0/\nLOCAL_PATH = (\n    args.json_path\n)  # /p/home/jusers/garg4/juwels/garg4/scratch/code/dcnlp/exp_data/models/CC_v3_random_resampled_3B-open_lm_160m-1.0.json\nwith open(LOCAL_PATH, \"r\") as f:\n    data = json.load(f)\nlocal_model_path = Path(data[\"checkpoint_url\"])\nlocal_param_path = Path(data[\"params_url\"])\nbucket_name = BUCKET_PATH.split(\"/\")[2]\nremote_model_path = Path(\"/\".join(BUCKET_PATH.split(\"/\")[3:])) / local_model_path.name",
        "detail": "tools.push_JSC_model_remotely",
        "documentation": {}
    },
    {
        "label": "LOCAL_PATH",
        "kind": 5,
        "importPath": "tools.push_JSC_model_remotely",
        "description": "tools.push_JSC_model_remotely",
        "peekOfCode": "LOCAL_PATH = (\n    args.json_path\n)  # /p/home/jusers/garg4/juwels/garg4/scratch/code/dcnlp/exp_data/models/CC_v3_random_resampled_3B-open_lm_160m-1.0.json\nwith open(LOCAL_PATH, \"r\") as f:\n    data = json.load(f)\nlocal_model_path = Path(data[\"checkpoint_url\"])\nlocal_param_path = Path(data[\"params_url\"])\nbucket_name = BUCKET_PATH.split(\"/\")[2]\nremote_model_path = Path(\"/\".join(BUCKET_PATH.split(\"/\")[3:])) / local_model_path.name\nremote_param_path = Path(\"/\".join(BUCKET_PATH.split(\"/\")[3:])) / local_param_path.name",
        "detail": "tools.push_JSC_model_remotely",
        "documentation": {}
    },
    {
        "label": "local_model_path",
        "kind": 5,
        "importPath": "tools.push_JSC_model_remotely",
        "description": "tools.push_JSC_model_remotely",
        "peekOfCode": "local_model_path = Path(data[\"checkpoint_url\"])\nlocal_param_path = Path(data[\"params_url\"])\nbucket_name = BUCKET_PATH.split(\"/\")[2]\nremote_model_path = Path(\"/\".join(BUCKET_PATH.split(\"/\")[3:])) / local_model_path.name\nremote_param_path = Path(\"/\".join(BUCKET_PATH.split(\"/\")[3:])) / local_param_path.name\n# Example usage\nupload_file_to_s3(local_model_path, bucket_name, str(remote_model_path))\nupload_file_to_s3(local_param_path, bucket_name, str(remote_param_path))\ndata[\"checkpoint_url\"] = \"s3://\" + str(f\"{bucket_name}\" / remote_model_path)\ndata[\"params_url\"] = \"s3://\" + str(f\"{bucket_name}\" / remote_param_path)",
        "detail": "tools.push_JSC_model_remotely",
        "documentation": {}
    },
    {
        "label": "local_param_path",
        "kind": 5,
        "importPath": "tools.push_JSC_model_remotely",
        "description": "tools.push_JSC_model_remotely",
        "peekOfCode": "local_param_path = Path(data[\"params_url\"])\nbucket_name = BUCKET_PATH.split(\"/\")[2]\nremote_model_path = Path(\"/\".join(BUCKET_PATH.split(\"/\")[3:])) / local_model_path.name\nremote_param_path = Path(\"/\".join(BUCKET_PATH.split(\"/\")[3:])) / local_param_path.name\n# Example usage\nupload_file_to_s3(local_model_path, bucket_name, str(remote_model_path))\nupload_file_to_s3(local_param_path, bucket_name, str(remote_param_path))\ndata[\"checkpoint_url\"] = \"s3://\" + str(f\"{bucket_name}\" / remote_model_path)\ndata[\"params_url\"] = \"s3://\" + str(f\"{bucket_name}\" / remote_param_path)\nwith open(LOCAL_PATH, \"w\") as f:",
        "detail": "tools.push_JSC_model_remotely",
        "documentation": {}
    },
    {
        "label": "bucket_name",
        "kind": 5,
        "importPath": "tools.push_JSC_model_remotely",
        "description": "tools.push_JSC_model_remotely",
        "peekOfCode": "bucket_name = BUCKET_PATH.split(\"/\")[2]\nremote_model_path = Path(\"/\".join(BUCKET_PATH.split(\"/\")[3:])) / local_model_path.name\nremote_param_path = Path(\"/\".join(BUCKET_PATH.split(\"/\")[3:])) / local_param_path.name\n# Example usage\nupload_file_to_s3(local_model_path, bucket_name, str(remote_model_path))\nupload_file_to_s3(local_param_path, bucket_name, str(remote_param_path))\ndata[\"checkpoint_url\"] = \"s3://\" + str(f\"{bucket_name}\" / remote_model_path)\ndata[\"params_url\"] = \"s3://\" + str(f\"{bucket_name}\" / remote_param_path)\nwith open(LOCAL_PATH, \"w\") as f:\n    json.dump(data, f, indent=4)",
        "detail": "tools.push_JSC_model_remotely",
        "documentation": {}
    },
    {
        "label": "remote_model_path",
        "kind": 5,
        "importPath": "tools.push_JSC_model_remotely",
        "description": "tools.push_JSC_model_remotely",
        "peekOfCode": "remote_model_path = Path(\"/\".join(BUCKET_PATH.split(\"/\")[3:])) / local_model_path.name\nremote_param_path = Path(\"/\".join(BUCKET_PATH.split(\"/\")[3:])) / local_param_path.name\n# Example usage\nupload_file_to_s3(local_model_path, bucket_name, str(remote_model_path))\nupload_file_to_s3(local_param_path, bucket_name, str(remote_param_path))\ndata[\"checkpoint_url\"] = \"s3://\" + str(f\"{bucket_name}\" / remote_model_path)\ndata[\"params_url\"] = \"s3://\" + str(f\"{bucket_name}\" / remote_param_path)\nwith open(LOCAL_PATH, \"w\") as f:\n    json.dump(data, f, indent=4)",
        "detail": "tools.push_JSC_model_remotely",
        "documentation": {}
    },
    {
        "label": "remote_param_path",
        "kind": 5,
        "importPath": "tools.push_JSC_model_remotely",
        "description": "tools.push_JSC_model_remotely",
        "peekOfCode": "remote_param_path = Path(\"/\".join(BUCKET_PATH.split(\"/\")[3:])) / local_param_path.name\n# Example usage\nupload_file_to_s3(local_model_path, bucket_name, str(remote_model_path))\nupload_file_to_s3(local_param_path, bucket_name, str(remote_param_path))\ndata[\"checkpoint_url\"] = \"s3://\" + str(f\"{bucket_name}\" / remote_model_path)\ndata[\"params_url\"] = \"s3://\" + str(f\"{bucket_name}\" / remote_param_path)\nwith open(LOCAL_PATH, \"w\") as f:\n    json.dump(data, f, indent=4)",
        "detail": "tools.push_JSC_model_remotely",
        "documentation": {}
    },
    {
        "label": "data[\"checkpoint_url\"]",
        "kind": 5,
        "importPath": "tools.push_JSC_model_remotely",
        "description": "tools.push_JSC_model_remotely",
        "peekOfCode": "data[\"checkpoint_url\"] = \"s3://\" + str(f\"{bucket_name}\" / remote_model_path)\ndata[\"params_url\"] = \"s3://\" + str(f\"{bucket_name}\" / remote_param_path)\nwith open(LOCAL_PATH, \"w\") as f:\n    json.dump(data, f, indent=4)",
        "detail": "tools.push_JSC_model_remotely",
        "documentation": {}
    },
    {
        "label": "data[\"params_url\"]",
        "kind": 5,
        "importPath": "tools.push_JSC_model_remotely",
        "description": "tools.push_JSC_model_remotely",
        "peekOfCode": "data[\"params_url\"] = \"s3://\" + str(f\"{bucket_name}\" / remote_param_path)\nwith open(LOCAL_PATH, \"w\") as f:\n    json.dump(data, f, indent=4)",
        "detail": "tools.push_JSC_model_remotely",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "tools.sync_aws_hf2",
        "description": "tools.sync_aws_hf2",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Sync S3 files to Hugging Face repository\")\n    parser.add_argument(\"--s3_bucket\", type=str, default=\"***REMOVED***\", help=\"S3 bucket name\")\n    parser.add_argument(\n        \"--s3_prefix\",\n        type=str,\n        default=\"users/vaishaal/mlr/hero-run-fasttext/filtered/OH_eli5_vs_rw_v2_bigram_200k_train/fasttext_openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train/processed_data/\",\n        help=\"S3 prefix\",\n    )\n    parser.add_argument(",
        "detail": "tools.sync_aws_hf2",
        "documentation": {}
    },
    {
        "label": "list_s3_objects",
        "kind": 2,
        "importPath": "tools.sync_aws_hf2",
        "description": "tools.sync_aws_hf2",
        "peekOfCode": "def list_s3_objects(s3, bucket, prefix):\n    logger.info(f\"Listing objects in S3 bucket: {bucket} with prefix: {prefix}\")\n    objects = []\n    paginator = s3.get_paginator(\"list_objects_v2\")\n    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n        if \"Contents\" in page:\n            objects.extend(page[\"Contents\"])\n    return objects\ndef list_hf_files(api, repo_id):\n    logger.info(f\"Listing files in Hugging Face repository: {repo_id}\")",
        "detail": "tools.sync_aws_hf2",
        "documentation": {}
    },
    {
        "label": "list_hf_files",
        "kind": 2,
        "importPath": "tools.sync_aws_hf2",
        "description": "tools.sync_aws_hf2",
        "peekOfCode": "def list_hf_files(api, repo_id):\n    logger.info(f\"Listing files in Hugging Face repository: {repo_id}\")\n    hf_files = api.list_repo_files(repo_id, repo_type=\"dataset\")\n    return set(hf_files)\ndef download_worker(s3, bucket, local_dir, download_queue, upload_queue):\n    while not download_queue.empty():\n        s3_key = download_queue.get()\n        local_file_path = os.path.join(local_dir, os.path.basename(s3_key))\n        logger.info(f\"Downloading {s3_key} to {local_file_path}\")\n        s3.download_file(bucket, s3_key, local_file_path)",
        "detail": "tools.sync_aws_hf2",
        "documentation": {}
    },
    {
        "label": "download_worker",
        "kind": 2,
        "importPath": "tools.sync_aws_hf2",
        "description": "tools.sync_aws_hf2",
        "peekOfCode": "def download_worker(s3, bucket, local_dir, download_queue, upload_queue):\n    while not download_queue.empty():\n        s3_key = download_queue.get()\n        local_file_path = os.path.join(local_dir, os.path.basename(s3_key))\n        logger.info(f\"Downloading {s3_key} to {local_file_path}\")\n        s3.download_file(bucket, s3_key, local_file_path)\n        upload_queue.put((local_file_path, s3_key))\n        download_queue.task_done()\ndef upload_worker(api, repo_id, s3_prefix, upload_queue, batch_size):\n    batch = []",
        "detail": "tools.sync_aws_hf2",
        "documentation": {}
    },
    {
        "label": "upload_worker",
        "kind": 2,
        "importPath": "tools.sync_aws_hf2",
        "description": "tools.sync_aws_hf2",
        "peekOfCode": "def upload_worker(api, repo_id, s3_prefix, upload_queue, batch_size):\n    batch = []\n    local_file_paths = []\n    while True:\n        try:\n            local_file_path, s3_key = upload_queue.get(timeout=300)\n            logger.info(f\"{local_file_path}/{s3_key}\")\n            hf_repo_path = s3_key[len(s3_prefix) :]\n            logger.info(f\"Adding {local_file_path} to Hugging Face at commit {hf_repo_path}\")\n            batch.append(CommitOperationAdd(path_in_repo=hf_repo_path, path_or_fileobj=local_file_path))",
        "detail": "tools.sync_aws_hf2",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.sync_aws_hf2",
        "description": "tools.sync_aws_hf2",
        "peekOfCode": "def main():\n    args = parse_args()\n    # Initialize S3 client\n    s3 = boto3.client(\"s3\")\n    # Initialize Hugging Face API\n    api = HfApi()\n    hf_folder = HfFolder()\n    hf_folder.save_token(args.hf_token)\n    # Create a local directory to store S3 files temporarily\n    os.makedirs(args.local_dir, exist_ok=True)",
        "detail": "tools.sync_aws_hf2",
        "documentation": {}
    },
    {
        "label": "DatasetReference",
        "kind": 6,
        "importPath": "training.dataset_reference",
        "description": "training.dataset_reference",
        "peekOfCode": "class DatasetReference:\n    name: str\n    sources: Union[str, List[str]]\n    tokenized: bool\n    num_tokens: int\n    size: int\n    dataset_url: str\n    manifest_url: str\n    dcnlp_commit_hash: str\n    dcnlp_diff: str",
        "detail": "training.dataset_reference",
        "documentation": {}
    },
    {
        "label": "replace_prefix",
        "kind": 2,
        "importPath": "training.dataset_reference",
        "description": "training.dataset_reference",
        "peekOfCode": "def replace_prefix(s3_url, prefix_replacement):\n    if not prefix_replacement:\n        return s3_url\n    old_prefix, new_prefix = prefix_replacement.split(\"=\")\n    if s3_url.startswith(old_prefix):\n        return s3_url.replace(old_prefix, new_prefix, 1)\n    return s3_url\n@dataclass\nclass DatasetReference:\n    name: str",
        "detail": "training.dataset_reference",
        "documentation": {}
    },
    {
        "label": "get_downstream_task_name",
        "kind": 2,
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "peekOfCode": "def get_downstream_task_name(task):\n    return \".\".join(task[\"dataset_uri\"].split(\"/\")[-1].split(\".\")[:-1])\ndef load_ppl_yaml(size=\"heavy\"):\n    task_yml = os.path.join(\n        Path(__file__).parent.parent.absolute(),\n        \"eval\",\n        f\"{size}_ppl.yaml\",\n    )\n    tasks = None\n    with open(task_yml, \"r\") as stream:",
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "load_ppl_yaml",
        "kind": 2,
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "peekOfCode": "def load_ppl_yaml(size=\"heavy\"):\n    task_yml = os.path.join(\n        Path(__file__).parent.parent.absolute(),\n        \"eval\",\n        f\"{size}_ppl.yaml\",\n    )\n    tasks = None\n    with open(task_yml, \"r\") as stream:\n        try:\n            tasks = yaml.safe_load(stream)[\"icl_tasks\"]",
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "natural_key",
        "kind": 2,
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "peekOfCode": "def natural_key(string_):\n    \"\"\"See http://www.codinghorror.com/blog/archives/001018.html\"\"\"\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_.lower())]\ndef download_val_data(name, root=Path(__file__).parent / f\"eval_data/\", skip_download=False):\n    # modified from oai _download clip function\n    if root is None:\n        raise RuntimeError(f\"{root} must not be None\")\n    cloud_checkpoints = {\n        \"open_lm_val\": {\n            \"passable_name\": os.path.join(root, name, \"shard_00000000.tar\"),",
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "download_val_data",
        "kind": 2,
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "peekOfCode": "def download_val_data(name, root=Path(__file__).parent / f\"eval_data/\", skip_download=False):\n    # modified from oai _download clip function\n    if root is None:\n        raise RuntimeError(f\"{root} must not be None\")\n    cloud_checkpoints = {\n        \"open_lm_val\": {\n            \"passable_name\": os.path.join(root, name, \"shard_00000000.tar\"),\n            \"downloads\": [\n                {\n                    \"url\": \"https://huggingface.co/datasets/mlfoundations/open_lm_example_data/resolve/main/validation_data/shard_00000000.tar\",",
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "setup_logger",
        "kind": 2,
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "peekOfCode": "def setup_logger(name=__name__):\n    logger = logging.getLogger(name)\n    # Set the logging level\n    logger.setLevel(logging.DEBUG)  # For example, set level to DEBUG\n    # Create a StreamHandler for stdout\n    stdout_handler = logging.StreamHandler()\n    stdout_handler.setLevel(logging.INFO)  # Optionally set a different level for stdout\n    # Create a formatter and set it for the handler\n    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n    stdout_handler.setFormatter(formatter)",
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "maybe_save_model_json",
        "kind": 2,
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "peekOfCode": "def maybe_save_model_json(partial_model_dir, exp_root, name, data, hparams, open_lm_args, latest_ckpt):\n    logging.info(\"Checking for partial model.\")\n    fs, path = fsspec.core.url_to_fs(exp_root)\n    if exp_root.startswith(\"s3://\"):\n        result = subprocess.run(\n            [\"aws\", \"s3\", \"ls\", os.path.join(exp_root, \"checkpoints\"), \"--recursive\"],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        files = [l.strip().split(\" \")[-1] for l in result.stdout.decode().splitlines()]",
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "keep_running_model_json",
        "kind": 2,
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "peekOfCode": "def keep_running_model_json(partial_model_dir, exp_root, name, data, hparams, open_lm_args, wait_secs):\n    logging.info(\"Starting partial model saving process.\")\n    latest_ckpt = 0\n    while True:\n        time.sleep(wait_secs)\n        latest_ckpt = maybe_save_model_json(partial_model_dir, exp_root, name, data, hparams, open_lm_args, latest_ckpt)\ndef start_partial_model_process(partial_model_dir, exp_root, name, data, hparams, open_lm_args, wait_secs=600):\n    p = mp.Process(\n        target=keep_running_model_json,\n        args=(partial_model_dir, exp_root, name, data, hparams, open_lm_args, wait_secs),",
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "start_partial_model_process",
        "kind": 2,
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "peekOfCode": "def start_partial_model_process(partial_model_dir, exp_root, name, data, hparams, open_lm_args, wait_secs=600):\n    p = mp.Process(\n        target=keep_running_model_json,\n        args=(partial_model_dir, exp_root, name, data, hparams, open_lm_args, wait_secs),\n    )\n    return p\ndef terminate_partial_model_process(p: mp.Process):\n    if p is not None and p.is_alive():\n        logging.info(f\"Terminating remote sync process.\")\n        p.terminate()",
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "terminate_partial_model_process",
        "kind": 2,
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "peekOfCode": "def terminate_partial_model_process(p: mp.Process):\n    if p is not None and p.is_alive():\n        logging.info(f\"Terminating remote sync process.\")\n        p.terminate()",
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "tok_mult_paths",
        "kind": 5,
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "peekOfCode": "tok_mult_paths = [\n    \"training/eval_data/val_tok_mult/openlm/shard_00000000.tar\",\n    \"training/eval_data/c4_val/shard-{0000000..0000010}.tar\",\n    # \"training/eval_data/paloma_val/{00000001..00000004}.tar\",\n    # \"training/eval_data/val_tok_mult/en-de_mix_shards/val_en-de_000.tar\",\n    # \"training/eval_data/val_tok_mult/en-de_mix_shards/val_en-de_010.tar\",\n    # \"training/eval_data/val_tok_mult/en-de_mix_shards/val_en-de_020.tar\",\n    # \"training/eval_data/val_tok_mult/en-de_mix_shards/val_en-de_030.tar\",\n    # \"training/eval_data/val_tok_mult/en-de_mix_shards/val_en-de_040.tar\",\n    # \"training/eval_data/val_tok_mult/en-de_mix_shards/val_en-de_050.tar\",",
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "DOWNSTREAM_SHARD_HASHES",
        "kind": 5,
        "importPath": "training.file_utils",
        "description": "training.file_utils",
        "peekOfCode": "DOWNSTREAM_SHARD_HASHES = {\n    \"agi_eval_lsat_ar\": \"f74c0426e8bd08e1e9bcf758ccc1353ca45415a672fa1c763294c3436fee1037\",\n    \"agi_eval_sat_math\": \"50c64867548c1e76a65051652fca2a2e30e6cbfe6b55eafe00b1c463536565f4\",\n    \"aqua\": \"7d6823fad372f1e42cd7b43f344d7d1947d0c97a3f9a0f574b22160f107dadca\",\n    \"bigbench_cs_algorithms\": \"64ed919e3a3ea0ea526af105c173ddd3ba1267c2f6bda54755a2198850bc645a\",\n    \"bigbench_dyck_languages\": \"77f5eacf426a26b63411e1b33e7e1d4429cf6f6433c4e4f7cf0dcd2aac477520\",\n    \"bigbench_elementary_math_qa\": \"0e45b1bb172f0b1b0062f0dfd59ab554a43dc7cf876c10f2276387f0809c1b60\",\n    \"bigbench_logical_deduction\": \"81deb91ab888ef542ce28ac2d63a7fa1a258b981cf0dc6f4fff44fe335ba89e3\",\n    \"bigbench_operators\": \"79fe39b2d1027394ac0122bb3862e0db7b7f3bc9e267b996077ce2f9319f0e8f\",\n    \"bigbench_repeat_copy_logic\": \"72fb7fb2a14c9e32103ef9dd62cd8f9c59fb407c9ee69b3b663782b84e661952\",",
        "detail": "training.file_utils",
        "documentation": {}
    },
    {
        "label": "Hyperparameters",
        "kind": 6,
        "importPath": "training.hyperparameters",
        "description": "training.hyperparameters",
        "peekOfCode": "class Hyperparameters:\n    model: str\n    tokens: int\n    warmup: int\n    lr: float\n    wd: float\n    cd: float\n    global_bs: int\n    acc: int\n    qk_norm: bool",
        "detail": "training.hyperparameters",
        "documentation": {}
    },
    {
        "label": "sanitize_for_fs",
        "kind": 2,
        "importPath": "training.hyperparameters",
        "description": "training.hyperparameters",
        "peekOfCode": "def sanitize_for_fs(s):\n    return str(s).replace(\".\", \"p\").replace(\"/\", \"-\")\n@dataclass\nclass Hyperparameters:\n    model: str\n    tokens: int\n    warmup: int\n    lr: float\n    wd: float\n    cd: float",
        "detail": "training.hyperparameters",
        "documentation": {}
    },
    {
        "label": "available_scales",
        "kind": 2,
        "importPath": "training.hyperparameters",
        "description": "training.hyperparameters",
        "peekOfCode": "def available_scales(simple_names=False):\n    return sorted(list(SCALE_CONFIGS.keys()))\ndef get_scale_config(scale):\n    if scale not in SCALE_CONFIGS:\n        raise ValueError(f\"Unknown scale: {scale}. Please use one of {available_scales()}\")\n    return SCALE_CONFIGS[scale]",
        "detail": "training.hyperparameters",
        "documentation": {}
    },
    {
        "label": "get_scale_config",
        "kind": 2,
        "importPath": "training.hyperparameters",
        "description": "training.hyperparameters",
        "peekOfCode": "def get_scale_config(scale):\n    if scale not in SCALE_CONFIGS:\n        raise ValueError(f\"Unknown scale: {scale}. Please use one of {available_scales()}\")\n    return SCALE_CONFIGS[scale]",
        "detail": "training.hyperparameters",
        "documentation": {}
    },
    {
        "label": "SCALE_CONFIG_PATHS",
        "kind": 5,
        "importPath": "training.hyperparameters",
        "description": "training.hyperparameters",
        "peekOfCode": "SCALE_CONFIG_PATHS = [\n    Path(__file__).parent / f\"configs/\",\n    Path(__file__).parent / f\"configs_ppl_filtering/\",\n    Path(__file__).parent / f\"configs_grid/\",\n]\nSCALE_CONFIGS = {}\nfor s in SCALE_CONFIG_PATHS:\n    if not os.path.isdir(s):\n        continue\n    for p in os.listdir(s):",
        "detail": "training.hyperparameters",
        "documentation": {}
    },
    {
        "label": "SCALE_CONFIGS",
        "kind": 5,
        "importPath": "training.hyperparameters",
        "description": "training.hyperparameters",
        "peekOfCode": "SCALE_CONFIGS = {}\nfor s in SCALE_CONFIG_PATHS:\n    if not os.path.isdir(s):\n        continue\n    for p in os.listdir(s):\n        with open(os.path.join(s, p), \"r\") as f:\n            SCALE_CONFIGS[Path(p).stem] = Hyperparameters(**json.load(f))\ndef available_scales(simple_names=False):\n    return sorted(list(SCALE_CONFIGS.keys()))\ndef get_scale_config(scale):",
        "detail": "training.hyperparameters",
        "documentation": {}
    },
    {
        "label": "ModelReference",
        "kind": 6,
        "importPath": "training.model_reference",
        "description": "training.model_reference",
        "peekOfCode": "class ModelReference:\n    name: str\n    dataset_name: str\n    dataset_uuid: str\n    hyperparameters: Hyperparameters\n    checkpoint_url: str\n    open_lm_version: str\n    open_lm_args: str\n    results: List[Any]\n    params_url: str",
        "detail": "training.model_reference",
        "documentation": {}
    },
    {
        "label": "add_dcnlp_args",
        "kind": 2,
        "importPath": "training.params",
        "description": "training.params",
        "peekOfCode": "def add_dcnlp_args(parser):\n    parser.add_argument(\n        \"--scale\",\n        type=str,\n        required=False,\n        default=None,\n        choices=available_scales(),\n        help=\"Competition scale.\",\n    )\n    parser.add_argument(",
        "detail": "training.params",
        "documentation": {}
    },
    {
        "label": "parse_dcnlp_args",
        "kind": 2,
        "importPath": "training.params",
        "description": "training.params",
        "peekOfCode": "def parse_dcnlp_args():\n    parser = argparse.ArgumentParser()\n    add_dcnlp_args(parser)\n    args = parser.parse_args()\n    if args.re_evaluate is not None:\n        # in the case of re-evaluation set based on the input model.json\n        model_json = None\n        with open(args.re_evaluate, \"r\") as f:\n            model_json = json.load(f)\n        args.scale = Path(model_json[\"hyperparameters\"][\"model\"]).stem",
        "detail": "training.params",
        "documentation": {}
    },
    {
        "label": "get_open_lm_args",
        "kind": 2,
        "importPath": "training.params",
        "description": "training.params",
        "peekOfCode": "def get_open_lm_args(args, hparams, dr):\n    if args.manifest_prefix_override is not None:\n        assert args.prefix_replacement == \"\"\n        manifest_name = Path(dr.manifest_url).name\n        dr.manifest_url = os.path.join(args.manifest_prefix_override, f\"{manifest_name}\")\n    if args.mirror:\n        dr.update_for_mirror(args.mirror)\n    if args.prefix_replacement:\n        assert args.manifest_prefix_override is None\n        dr.replace_prefix(args.prefix_replacement)",
        "detail": "training.params",
        "documentation": {}
    },
    {
        "label": "process_dcnlp_args",
        "kind": 2,
        "importPath": "training.train",
        "description": "training.train",
        "peekOfCode": "def process_dcnlp_args(args):\n    \"\"\"Helper script for setting up data reference, hparams, and name.\n    Note: The reason this is a function is because it is used by other scripts (e.g. Sagemaker) to get the name from an\n    args object.\n    \"\"\"\n    data = None\n    with open(args.data_config, \"r\") as f:\n        data = DatasetReference(**json.load(f))\n    # modify num tokens by multiplier\n    hparams = None",
        "detail": "training.train",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "training.train",
        "description": "training.train",
        "peekOfCode": "logger = setup_logger(__name__)\ndef process_dcnlp_args(args):\n    \"\"\"Helper script for setting up data reference, hparams, and name.\n    Note: The reason this is a function is because it is used by other scripts (e.g. Sagemaker) to get the name from an\n    args object.\n    \"\"\"\n    data = None\n    with open(args.data_config, \"r\") as f:\n        data = DatasetReference(**json.load(f))\n    # modify num tokens by multiplier",
        "detail": "training.train",
        "documentation": {}
    },
    {
        "label": "BaselineInstall",
        "kind": 6,
        "importPath": "setup",
        "description": "setup",
        "peekOfCode": "class BaselineInstall(install):\n    def run(self):\n        # Perform the standard installation\n        install.run(self)\n        # Custom setup for baseline\n        print(\"Setting up baseline requirements...\")\n        nltk.download('punkt')\nclass TrainingInstall(install):\n    def run(self):\n        install.run(self)",
        "detail": "setup",
        "documentation": {}
    },
    {
        "label": "TrainingInstall",
        "kind": 6,
        "importPath": "setup",
        "description": "setup",
        "peekOfCode": "class TrainingInstall(install):\n    def run(self):\n        install.run(self)\n        # Download and prepare training data\n        print(\"Downloading training data...\")\n        nltk.download('punkt')\nclass EvalInstall(install):\n    def run(self):\n        install.run(self)\n        print(\"Preparing evaluation setup...\")",
        "detail": "setup",
        "documentation": {}
    },
    {
        "label": "EvalInstall",
        "kind": 6,
        "importPath": "setup",
        "description": "setup",
        "peekOfCode": "class EvalInstall(install):\n    def run(self):\n        install.run(self)\n        print(\"Preparing evaluation setup...\")\nclass DevInstall(install):\n    def run(self):\n        install.run(self)\n        print(\"Setting up development environment...\")\nclass DownloadAssetsCommand(install):\n    description = 'download and set up larger assets (e.g., models, banlists) after installation'",
        "detail": "setup",
        "documentation": {}
    },
    {
        "label": "DevInstall",
        "kind": 6,
        "importPath": "setup",
        "description": "setup",
        "peekOfCode": "class DevInstall(install):\n    def run(self):\n        install.run(self)\n        print(\"Setting up development environment...\")\nclass DownloadAssetsCommand(install):\n    description = 'download and set up larger assets (e.g., models, banlists) after installation'\n    user_options = install.user_options + [\n        ('skip-downloads=', 's', \"whether to skip all downloads\"),\n        ('skip-model-downloads=', None, \"whether to skip model downloads\"),\n        ('skip-banlist-downloads=', None, \"whether to skip banlist downloads\"),",
        "detail": "setup",
        "documentation": {}
    },
    {
        "label": "DownloadAssetsCommand",
        "kind": 6,
        "importPath": "setup",
        "description": "setup",
        "peekOfCode": "class DownloadAssetsCommand(install):\n    description = 'download and set up larger assets (e.g., models, banlists) after installation'\n    user_options = install.user_options + [\n        ('skip-downloads=', 's', \"whether to skip all downloads\"),\n        ('skip-model-downloads=', None, \"whether to skip model downloads\"),\n        ('skip-banlist-downloads=', None, \"whether to skip banlist downloads\"),\n        ('rw-banlist-type=', None, \"whether to use the curated banlist or the uncurated banlist\")\n    ]\n    def initialize_options(self):\n        install.initialize_options(self)",
        "detail": "setup",
        "documentation": {}
    },
    {
        "label": "PROJECT_ROOT",
        "kind": 5,
        "importPath": "setup",
        "description": "setup",
        "peekOfCode": "PROJECT_ROOT = os.path.dirname(__file__)\nclass BaselineInstall(install):\n    def run(self):\n        # Perform the standard installation\n        install.run(self)\n        # Custom setup for baseline\n        print(\"Setting up baseline requirements...\")\n        nltk.download('punkt')\nclass TrainingInstall(install):\n    def run(self):",
        "detail": "setup",
        "documentation": {}
    }
]