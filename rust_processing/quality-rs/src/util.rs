use anyhow::Error;
use color_eyre::eyre::Result;
use ngrams::Ngram;
use once_cell::sync::Lazy;
use regex::Regex;
use serde_json::Value;
use std::collections::{HashMap, HashSet};
use vtext::tokenize::{Tokenizer, VTextTokenizerParams};

pub const WORDS_KEY: &str = "words";
pub const TEXT_KEY: &str = "text";
static PARAGRAPH_RE: Lazy<Regex> = Lazy::new(|| Regex::new(r"\n{2,}").unwrap());
static LINE_RE: Lazy<Regex> = Lazy::new(|| Regex::new(r"\n").unwrap());
pub static STOP_WORDS: [&str; 8] = ["the", "be", "to", "of", "and", "that", "have", "with"];

pub static PUNCTUATION: Lazy<String> = Lazy::new(|| {
    // åŸå§‹æ ‡ç‚¹å­—ç¬¦ä¸²
    let mut s = String::from(
        "!/â€”â€:ï¼…ï¼‘ã€ˆ&(ã€â”\\ã€#%ã€Œã€ï¼Œã€‘ï¼›+^]~â€œã€Šâ€';â€™{|âˆ¶Â´[=-`*ï¼ï¼ˆâ€“ï¼Ÿï¼ï¼š$ï½Â«ã€‰,><ã€‹)?ï¼‰ã€‚â€¦@_.\"}â–ºÂ»",
    );

    // åŠ å…¥æ§åˆ¶å­—ç¬¦åŒºé—´: 0-8, 11-12, 13-31, 127-159
    for &(start, end) in &[(0, 9), (11, 13), (13, 32), (127, 160)] {
        for code in start..end {
            if let Some(ch) = std::char::from_u32(code) {
                s.push(ch);
            }
        }
    }
    s
});

pub static PUNCTUATION_SET: Lazy<HashSet<char>> = Lazy::new(|| {
    // ä» PUNCTUATION å­—ç¬¦ä¸²æ”¶é›†æ‰€æœ‰å­—ç¬¦
    let mut set: HashSet<char> = PUNCTUATION.chars().collect();

    // æŠŠ TERMINAL_PUNCTUATION é‡Œçš„å„å­—ç¬¦ä¹ŸåŠ å…¥
    for &s in TERMINAL_PUNCTUATION.iter() {
        for ch in s.chars() {
            set.insert(ch);
        }
    }

    set
});

pub static BULLET_POINT_SYMBOLS: [&str; 14] = [
    "\u{2022}", // bullet point
    "\u{2023}", // triangular bullet point
    "\u{25B6}", // black right pointing triangle
    "\u{25C0}", // left pointing triangle
    "\u{25E6}", // bullet point
    "\u{25A0}", // square
    "\u{25A1}", // square
    "\u{25AA}", // small square
    "\u{25AB}", // small square
    "\u{2013}", // dash
    "-", "â€“", "â€¢", "â—",
];

pub static TERMINAL_PUNCTUATION: [&str; 159] = [
    "áª©",
    "ï¼Ÿ",
    "âˆ",
    "ğ‘©‚",
    "ï¼",
    "ê©",
    "ğ‘…ƒ",
    "ï¹—",
    "ğ‘‚¾",
    "\u{1B7D}",
    "á§",
    "ğ‘…‚",
    "ê¡¶",
    "ê˜",
    "â‰",
    "à ¾",
    "áª¨",
    "ğ‘Š©",
    "ğ‘±‚",
    "á±¿",
    "ğ–©®",
    "á¥…",
    "\u{11F43}",
    "\u{11F44}",
    "ï¹’",
    "ğ‘ˆ¹",
    "ğ‘ˆ¸",
    "á¢",
    "Ü‚",
    "Ø",
    "ê›³",
    "\u{10F88}",
    "ğ‘—",
    "ğ©–",
    "ğ‘™‚",
    "\u{061D}",
    "ê©Ÿ",
    "á ‰",
    "\u{1B7E}",
    "ğ‘——",
    "á°¼",
    "ğ‘»¸",
    "ï¼Ÿ",
    "ğ‘ªœ",
    "ê§‰",
    "ğ‘—‰",
    "ğ½™",
    "ğ–«µ",
    "ğ–¬·",
    "Ü€",
    "ê“¿",
    "áœµ",
    "ğ‘—",
    "ğ‘‡",
    "ğ‘—“",
    "ğ‘¥„",
    "áŸ–",
    "ğ‘¥†",
    "ğ‘—‘",
    "ğ‘—’",
    "ê¯«",
    "Û”",
    "ğ©—",
    "\u{10F86}",
    "ê¡·",
    "\u{2E54}",
    "ï½¡",
    "áŸ•",
    "ß¹",
    "â¸®",
    ".",
    "ğ‘‡…",
    "à ¹",
    "ğ›²Ÿ",
    "ê«°",
    "ê¤¯",
    "ğ½—",
    "á­",
    "ğ‘œ¼",
    "á¨",
    "ğ‘ƒ",
    "ê£",
    "ğ‘‡Ÿ",
    "ğ–¬¸",
    "ğ‘ª›",
    "ğ‘œ¾",
    "à ·",
    "ğªˆ",
    "?",
    "ğ‘ƒ€",
    "ğ‘—ƒ",
    "ï¼",
    "Ö‰",
    "ê£",
    "à¥¥",
    "ğ‘—–",
    "á­›",
    "á ƒ",
    "!",
    "áŠ",
    "ğ–º˜",
    "â‡",
    "ğ‘—Œ",
    "ğ‘‘‹",
    "ğ–­„",
    "á­Ÿ",
    "ğ‘…",
    "ğ‘™",
    "â¸¼",
    "ê©",
    "ğ‘—‹",
    "ã€‚",
    "ê§ˆ",
    "ê«±",
    "ğ‘œ½",
    "ğ½–",
    "ğ‘‚¿",
    "á™®",
    "áŸ”",
    "ê›·",
    "\u{10F89}",
    "áŸš",
    "á¥„",
    "ğ‘—•",
    "ğ‘—",
    "áªª",
    "á­š",
    "à ½",
    "ğ‘‡",
    "ğ‘—Š",
    "ğ½˜",
    "\u{2E53}",
    "ğ‘—”",
    "ğ–©¯",
    "ğ‘‡",
    "ğ‘»·",
    "ğ½•",
    "ğ‘©ƒ",
    "à¥¤",
    "ğ‘—‚",
    "ğ‘‡†",
    "ğ‘ˆ",
    "á‹",
    "á±¾",
    "ğ‘±",
    "ê˜",
    "Ü",
    "áœ¶",
    "â€¼",
    "ğ‘ˆ»",
    "â€½",
    "áª«",
    "ï¹–",
    "ğ‘‘Œ",
    "ğ‘ˆ¼",
    "\u{10F87}",
    "ğ‘—",
    "áŸ™",
    "á°»",
];

static COUNTER_RE: Lazy<Regex> = Lazy::new(|| {
    Regex::new(
        r"(?xi)           # å¿½ç•¥å¤§å°å†™ + æ”¯æŒç©ºç™½æ³¨é‡Š
        ^\W*              # å¯é€‰éå•è¯å­—ç¬¦å¼€å¤´ï¼ˆç©ºæ ¼ã€æ ‡ç‚¹ï¼‰
        \d                # ä¸€ä½æ•°å­—å¼€å¤´
        (?:[,\.\d])*      # åé¢å¯è·Ÿé€—å·ã€å°æ•°ç‚¹æˆ–æ›´å¤šæ•°å­—
        (?:[KMBkmb])?     # å¯é€‰å•ä½ K/M/B
        \s+               # è‡³å°‘ä¸€ä¸ªç©ºæ ¼åˆ†éš”
        (?:likes|shares|comments|retweets|reposts|quotes|bookmarks|upvotes|downvotes|downloads|views|followers)
                          # æ”¯æŒçš„åŠ¨ä½œè¯
        \W*$              # å¯é€‰éå•è¯å­—ç¬¦ç»“å°¾
    "
    )
    .unwrap()
});

pub fn is_counter(input: &str) -> bool {
    COUNTER_RE.is_match(input)
}

#[allow(dead_code)]
pub fn find_duplicates(x: &[&str]) -> (usize, usize) {
    let mut unique = HashSet::with_capacity(x.len());
    let mut dup_cnt = 0;
    let mut dup_chars = 0;

    for &e in x {
        if !unique.insert(e) {
            dup_cnt += 1;
            dup_chars += e.len();
        }
    }
    (dup_cnt, dup_chars)
}

#[allow(dead_code)]
pub fn find_top_duplicate(x: &[String]) -> usize {
    let mut counter = HashMap::with_capacity(x.len());

    // åªåšä¸€æ¬¡å“ˆå¸ŒæŸ¥æ‰¾
    for s in x {
        *counter.entry(s.as_str()).or_insert(0) += 1;
    }

    // æŸ¥æ‰¾ max
    counter
        .into_iter()
        .map(|(word, cnt)| word.len() * cnt)
        .max()
        .unwrap_or(0)
}

#[allow(dead_code)]
pub fn find_all_duplicate_fast(words: &[String], n: usize) -> usize {
    let m = words.len();
    if m < n || n == 0 {
        return 0;
    }

    // 1. å…ˆæŠŠæ¯ä¸ª word å“ˆå¸Œæˆ u64
    let mut word_hashes: Vec<u64> = Vec::with_capacity(m);
    for w in words {
        let mut h: u64 = 0;
        for &b in w.as_bytes() {
            // è¿™é‡Œç»™å¸¸é‡åŠ ä¸Š u64 åç¼€
            h = h.wrapping_mul(1315423911u64).wrapping_add(b as u64);
        }
        word_hashes.push(h);
    }

    // 2. é¢„è®¡ç®— base^iï¼Œä¹Ÿéƒ½æ˜¯ u64
    let mut pow: Vec<u64> = Vec::with_capacity(m + 1);
    pow.push(1u64); // è¿™é‡Œçš„ 1u64
    for i in 1..=m {
        // è¿™é‡Œçš„ wrapping_mul ä¹Ÿæ˜¯ u64
        pow.push(pow[i - 1].wrapping_mul(1315423911u64));
    }

    // 3. å‰ç¼€å“ˆå¸Œï¼Œç±»å‹éƒ½æ˜¯ u64
    let mut pref: Vec<u64> = Vec::with_capacity(m + 1);
    pref.push(0u64); // æ˜ç¡®æ˜¯ u64
    for i in 0..m {
        pref.push(
            pref[i]
                .wrapping_mul(1315423911u64)
                .wrapping_add(word_hashes[i]),
        );
    }

    // 4. é¢„å­˜æ¯ä¸ª word é•¿åº¦
    let lens: Vec<usize> = words.iter().map(|w| w.len()).collect();

    // 5. æ»‘çª—æ£€æµ‹
    let mut seen = HashSet::with_capacity(m);
    let mut dup_chars = 0;
    let mut i = 0;

    while i + n <= m {
        // è®¡ç®— window [i, i+n) çš„å“ˆå¸Œ
        // è¿™é‡Œ wrapping_sub ä¹Ÿæ˜¯ u64
        let h = pref[i + n].wrapping_sub(pref[i].wrapping_mul(pow[n]));

        if !seen.insert(h) {
            // åªæœ‰é‡å¤æ—¶æ‰è®¡ç®—é•¿åº¦
            let window_len: usize = lens[i..i + n].iter().sum();
            dup_chars += window_len;
            i += n;
        } else {
            i += 1;
        }
    }

    dup_chars
}

#[allow(dead_code)]
pub fn find_all_duplicate(words: &[String], n: usize) -> usize {
    let mut unique = HashSet::new();
    let mut repeated_chars = 0;
    let mut idx = 0;
    let n_words = words.len();

    while idx + n <= n_words {
        let n_gram = words[idx..idx + n].join("");

        if unique.contains(&n_gram) {
            repeated_chars += n_gram.len();
            idx += n;
        } else {
            unique.insert(n_gram);
            idx += 1;
        }
    }

    let total_chars: usize = words.iter().map(|w| w.len()).sum();
    assert!(repeated_chars <= total_chars);

    repeated_chars
}

pub fn split_words(
    text: &str,
    data: Option<&Value>,
    lang: &str,
    ignore_punctuation: bool,
    ignore_whitespace: bool,
) -> Result<Vec<String>, Error> {
    let mut tokens: Vec<String> = match data {
        Some(page) => page
            .get(WORDS_KEY)
            .and_then(Value::as_array)
            .map(|arr| {
                arr.iter()
                    .filter_map(Value::as_str)
                    .map(str::to_string)
                    .collect()
            })
            .unwrap_or_default(),
        None => {
            vec![]
        }
    };

    if tokens.is_empty() {
        let tok = VTextTokenizerParams::default().lang(lang).build()?;
        tokens = tok.tokenize(text).map(|s| s.to_string()).collect();
    }

    if ignore_whitespace {
        tokens = tokens.iter().map(|w| w.trim().to_string()).collect();
    }
    if ignore_punctuation {
        tokens = tokens
            .into_iter()
            .filter(|w| {
                w.chars()
                    .next()
                    .map(|c| c.is_alphanumeric() || (!ignore_whitespace && c.is_whitespace()))
                    .unwrap_or(false)
            })
            .collect();
    }

    Ok(tokens)
}

pub fn split_paragraphs(text: &str) -> Vec<&str> {
    PARAGRAPH_RE.split(text).map(|p| p.trim()).collect()
}

pub fn split_lines(text: &str) -> Vec<&str> {
    LINE_RE.split(text).collect()
}

pub fn clear_key(data: &mut Value, key: &str) {
    if let Value::Object(ref mut map) = data {
        map.remove(key);
    }
}

pub fn get_n_grams(words: &[String], n: usize) -> Vec<Vec<&str>> {
    if words.len() < n {
        return vec![];
    }
    words.iter().map(String::as_str).ngrams(n).collect()
}

pub fn print_banner() {
    println!(
        r"
     _____          ___                         ___
    /  /::\        /  /\                       /__/\
   /  /:/\:\      /  /:/                      |  |::\
  /  /:/  \:\    /  /:/       ___     ___     |  |:|:\
 /__/:/ \__\:|  /  /:/  ___  /__/\   /  /\  __|__|:|\:\
 \  \:\ /  /:/ /__/:/  /  /\ \  \:\ /  /:/ /__/::::| \:\
  \  \:\  /:/  \  \:\ /  /:/  \  \:\  /:/  \  \:\~~\__\/
   \  \:\/:/    \  \:\  /:/    \  \:\/:/    \  \:\
    \  \::/      \  \:\/:/      \  \::/      \  \:\
     \__\/        \  \::/        \__\/        \  \:\
                   \__\/                       \__\/     			
        ",
    );
}
