{
    "uuid": "67db6b77-c7c4-48ae-b431-57254587ed43",
    "name": "rpj_original",
    "creation_date": "2024_01_13-23_46_59",
    "dataset_url": "s3://***REMOVED***/openlm/dcnlp/datasets/rpj_original/",
    "manifest_url": "s3://***REMOVED***/openlm/dcnlp/datasets/rpj_original/manifest.jsonl",
    "sources": [
        {
            "uuid": "15701e36-c0bb-4bfa-bf52-d3419dbbd8a1",
            "name": "rpj_original_cc"
        },
        {
            "uuid": "a8f160b4-c1c9-409f-a747-a08b7a17d453",
            "name": "c4_original"
        },
        {
            "uuid": "edd67f24-49ae-4915-8c3a-dd4bcc62b9d8",
            "name": "rpj_original_github"
        },
        {
            "uuid": "c8b17a9b-6bd8-441a-8b9f-dbf486edf574",
            "name": "rpj_original_arxiv"
        },
        {
            "uuid": "d017c1fe-c9df-4e06-aa8f-d92b1097283b",
            "name": "rpj_original_books"
        },
        {
            "uuid": "3b25b18c-e724-4071-8c7a-d69c5e1aaeac",
            "name": "rpj_original_stackexchange"
        },
        {
            "uuid": "050bc436-8d61-4d73-b931-0306a4b26727",
            "name": "rpj_original_wiki"
        }
    ],
    "tokenized": true,
    "tokenizer": "EleutherAI/gpt-neox-20b",
    "num_tokens": 1210563340149,
    "size": 3145963378720,
    "dcnlp_commit_hash": "ab8f8ee7fff1c112fb4e3d9736bcfd126b97864f",
    "dcnlp_diff": "diff --git a/exp_data/datasets/untokenized/c4_original.json b/exp_data/datasets/untokenized/c4_original.json\nindex 966680d..f4f0c14 100644\n--- a/exp_data/datasets/untokenized/c4_original.json\n+++ b/exp_data/datasets/untokenized/c4_original.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"a8f160b4-c1c9-409f-a747-a08b7a17d453\",\n     \"name\": \"c4_original\",\n     \"creation_date\": \"2023_12_31-14_21_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/c4/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/c4/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/c4_wo_dedup.json b/exp_data/datasets/untokenized/c4_wo_dedup.json\nindex fd6fbbd..c5af54a 100644\n--- a/exp_data/datasets/untokenized/c4_wo_dedup.json\n+++ b/exp_data/datasets/untokenized/c4_wo_dedup.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"5431063a-bcdb-4c9e-83df-b5b08243ab1d\",\n     \"name\": \"c4_wo_dedup\",\n     \"creation_date\": \"2023_12_20-17_59_20\",\n-    \"dataset_url\": \"s3://dcnlp-west/cc_wet_2019_april_baselines/c4_wo_dedup/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/cc_wet_2019_april_baselines/c4_wo_dedup/\",\n     \"manifest_url\": null,\n     \"sources\": [\n         {\ndiff --git a/exp_data/datasets/untokenized/rpj_original.json b/exp_data/datasets/untokenized/rpj_original.json\nindex 817a094..d60f561 100644\n--- a/exp_data/datasets/untokenized/rpj_original.json\n+++ b/exp_data/datasets/untokenized/rpj_original.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"a49a6b1a-d357-475e-96a5-7a559ad927ef\",\n     \"name\": \"rpj_original\",\n     \"creation_date\": \"2024_01_05-10_38_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rpj_original_arxiv.json b/exp_data/datasets/untokenized/rpj_original_arxiv.json\nindex aea173a..21d27a8 100644\n--- a/exp_data/datasets/untokenized/rpj_original_arxiv.json\n+++ b/exp_data/datasets/untokenized/rpj_original_arxiv.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"c8b17a9b-6bd8-441a-8b9f-dbf486edf574\",\n     \"name\": \"rpj_original_arxiv\",\n     \"creation_date\": \"2023_12_31-14_21_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/arxiv/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/arxiv/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rpj_original_books.json b/exp_data/datasets/untokenized/rpj_original_books.json\nindex de40689..51f5c75 100644\n--- a/exp_data/datasets/untokenized/rpj_original_books.json\n+++ b/exp_data/datasets/untokenized/rpj_original_books.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"d017c1fe-c9df-4e06-aa8f-d92b1097283b\",\n     \"name\": \"rpj_original_books\",\n     \"creation_date\": \"2023_12_31-14_21_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/books_were_too_long_for_vaishaal_to_read/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/books_were_too_long_for_vaishaal_to_read/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rpj_original_cc.json b/exp_data/datasets/untokenized/rpj_original_cc.json\nindex 4a322df..e538171 100644\n--- a/exp_data/datasets/untokenized/rpj_original_cc.json\n+++ b/exp_data/datasets/untokenized/rpj_original_cc.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"15701e36-c0bb-4bfa-bf52-d3419dbbd8a1\",\n     \"name\": \"rpj_original_cc\",\n     \"creation_date\": \"2024_01_05-10_38_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/common_crawl/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/common_crawl/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rpj_original_github.json b/exp_data/datasets/untokenized/rpj_original_github.json\nindex 1380c00..d7546f7 100644\n--- a/exp_data/datasets/untokenized/rpj_original_github.json\n+++ b/exp_data/datasets/untokenized/rpj_original_github.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"edd67f24-49ae-4915-8c3a-dd4bcc62b9d8\",\n     \"name\": \"rpj_original_github\",\n     \"creation_date\": \"2023_12_31-14_21_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/github/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/github/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rpj_original_non_CC.json b/exp_data/datasets/untokenized/rpj_original_non_CC.json\nindex 181fbe5..bade67c 100644\n--- a/exp_data/datasets/untokenized/rpj_original_non_CC.json\n+++ b/exp_data/datasets/untokenized/rpj_original_non_CC.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"807c9277-7b10-4133-882d-09e22369587b\",\n     \"name\": \"rpj_original_non_CC\",\n     \"creation_date\": \"2023_12_31-14_21_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rpj_original_stackexchange.json b/exp_data/datasets/untokenized/rpj_original_stackexchange.json\nindex 12290b1..f337d4c 100644\n--- a/exp_data/datasets/untokenized/rpj_original_stackexchange.json\n+++ b/exp_data/datasets/untokenized/rpj_original_stackexchange.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"3b25b18c-e724-4071-8c7a-d69c5e1aaeac\",\n     \"name\": \"rpj_original_stackexchange\",\n     \"creation_date\": \"2023_12_31-14_21_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/stackexchange/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/stackexchange/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rpj_original_wiki.json b/exp_data/datasets/untokenized/rpj_original_wiki.json\nindex d98f66b..b7f70b0 100644\n--- a/exp_data/datasets/untokenized/rpj_original_wiki.json\n+++ b/exp_data/datasets/untokenized/rpj_original_wiki.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"050bc436-8d61-4d73-b931-0306a4b26727\",\n     \"name\": \"rpj_original_wiki\",\n     \"creation_date\": \"2023_12_31-14_21_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/wiki/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/wiki/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rw_original.json b/exp_data/datasets/untokenized/rw_original.json\nindex 3cc566d..aa35e58 100644\n--- a/exp_data/datasets/untokenized/rw_original.json\n+++ b/exp_data/datasets/untokenized/rw_original.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"df16a14e-0f67-4623-933a-805522653f22\",\n     \"name\": \"rw_original\",\n     \"creation_date\": \"2023_11_22-12_31_00\",\n-    \"dataset_url\": \"s3://dcnlp-west/refinedweb_raw_jsonl_keyfix/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/refinedweb_raw_jsonl_keyfix/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/ray_processing/__init__.py b/ray_processing/__init__.py\nindex 5e1b41d..014c770 100644\n--- a/ray_processing/__init__.py\n+++ b/ray_processing/__init__.py\n@@ -1,4 +1,4 @@\n-from dedup_jsonl import dedup_jsonl\n+from ray_processing.dedup_jsonl import dedup_jsonl\n from baselines.core.constants import GLOBAL_FUNCTIONS\n \n-GLOBAL_FUNCTIONS['exact_dedup'] = dedup_jsonl\n\\ No newline at end of file\n+GLOBAL_FUNCTIONS['exact_dedup'] = dedup_jsonl\ndiff --git a/ray_processing/cluster_tri_tokenize_shuffle.yaml b/ray_processing/cluster_tri_tokenize_shuffle.yaml\nnew file mode 100644\nindex 0000000..fbd2f5e\n--- /dev/null\n+++ b/ray_processing/cluster_tri_tokenize_shuffle.yaml\n@@ -0,0 +1,59 @@\n+# An unique identifier for the head node and workers of this cluster.\n+cluster_name: tri-ray-shuffle-tokenize\n+max_workers: 64\n+upscaling_speed: 0.0\n+available_node_types:\n+    ray.head.default:\n+        resources: {}\n+        node_config:\n+            SubnetIds: [subnet-07bf42d7c9cb929e4, subnet-0f72615fd9bd3c717, subnet-0a29e4f1a47443e28, subnet-06e0db77592be2b36]\n+            ImageId: ami-0fc5d935ebf8bc3bc # ray us-east-1\n+            InstanceType: i4i.4xlarge\n+            IamInstanceProfile:\n+                Arn: arn:aws:iam::124224456861:instance-profile/ray-autoscaler-v1\n+    ray.worker.default:\n+        min_workers: 64\n+        max_workers: 64\n+        node_config:\n+            SubnetIds: [subnet-07bf42d7c9cb929e4, subnet-0f72615fd9bd3c717, subnet-0a29e4f1a47443e28, subnet-06e0db77592be2b36]\n+            ImageId: ami-0fc5d935ebf8bc3bc # ray us-east-1\n+            InstanceType: i4i.4xlarge\n+            IamInstanceProfile:\n+                Arn: arn:aws:iam::124224456861:instance-profile/ray-autoscaler-v1\n+\n+# Cloud-provider specific configuration.\n+provider:\n+    type: aws\n+    region: us-east-1\n+    cache_stopped_nodes: False\n+    use_internal_ips: True\n+\n+# Mount local copy of DCNLP instead of cloning\n+file_mounts: {\n+    \"/home/ubuntu/dcnlp\": \"../\",\n+}\n+\n+# Add any paths you don't want to copy from your dcnlp repo.\n+rsync_exclude:\n+    - '**/venv'\n+    - 'training/eval_data/'\n+\n+setup_commands:\n+    # - sudo apt-get update -y\n+    - wget https://repo.anaconda.com/miniconda/Miniconda3-py310_23.3.1-0-Linux-x86_64.sh -O miniconda.sh\n+    - sudo mkfs -t xfs /dev/nvme1n1\n+    - sudo mount /dev/nvme1n1 /tmp\n+    - sudo chown -R $USER /tmp\n+    # NOTE: This seems to be necessary at TRI AWS due to some permissions issue.\n+    - sudo chmod 1777 /tmp\n+    - bash ~/miniconda.sh -f -b -p /tmp/miniconda3/\n+    - echo 'export PATH=\"/tmp/miniconda3/bin/:$PATH\"' >> ~/.bashrc\n+    - pip install --upgrade pip setuptools wheel\n+    - pip install -U \"ray[default] @ https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp310-cp310-manylinux2014_x86_64.whl\"\n+    - pip install boto3==1.26.90\n+    - pip install s3fs==2022.11.0\n+    - pip install psutil\n+    - pip install pyarrow\n+    # TEMPORARY: Due to dependency issues, pinning to a known working branch of open_lm for now. Will change later when better solution is found. \n+    - pip install git+https://github.com/mlfoundations/open_lm.git@achal/tmp-ray-20240108\n+\ndiff --git a/ray_processing/tokenize_shuffle.py b/ray_processing/tokenize_shuffle.py\nindex ba2ac32..14d4125 100644\n--- a/ray_processing/tokenize_shuffle.py\n+++ b/ray_processing/tokenize_shuffle.py\n@@ -53,7 +53,9 @@ if __name__ == \"__main__\":\n         assert all(s is not None for s in source_refs), \"Not all source reference jsons could be found.\"\n \n     # Collect args for tokenization and pass them into tokenize_shuffle\n-    tokenize_shuffle_args = [str(i) for k,v in vars(args).items() for i in [f\"--{k}\", v] if k not in DCNLP_ARGS and v]\n+    tokenize_shuffle_args = [str(i) for k,v in vars(args).items() for i in [f\"--{k}\", v] if k not in DCNLP_ARGS and k != \"do_sample\" and v]\n+    if args.do_sample:\n+        tokenize_shuffle_args += [\"--do_sample\"]\n     tokenize_shuffle.main(tokenize_shuffle_args)\n \n     dataset_json = generate_tokenized_dataset_json(args, source_refs)\ndiff --git a/setup.py b/setup.py\ndeleted file mode 100644\nindex 96e9a02..0000000\n--- a/setup.py\n+++ /dev/null\n@@ -1,172 +0,0 @@\n-from __future__ import annotations\n-import os\n-import urllib.request\n-import tarfile\n-import shutil\n-import argparse\n-from setuptools.command.install import install\n-from setuptools import setup, find_packages\n-from retrie.retrie import Blacklist\n-import pickle\n-import re\n-import nltk\n-\n-PROJECT_ROOT = os.path.dirname(__file__)\n-\n-class DownloadAssetsCommand(install):\n-    description = 'download and set up larger assets (e.g., models, banlists) after installation'\n-\n-    user_options = install.user_options + [\n-        ('skip-downloads=', 's', \"whether to skip all downloads\"),\n-        ('skip-model-downloads=', None, \"whether to skip model downloads\"),\n-        ('skip-banlist-downloads=', None, \"whether to skip banlist downloads\")\n-    ]\n-\n-    def initialize_options(self):\n-        install.initialize_options(self)\n-        self.skip_downloads = None\n-        self.skip_model_downloads = None\n-        self.skip_banlist_downloads = None\n-\n-    def finalize_options(self):\n-        install.finalize_options(self)\n-\n-        assert self.skip_downloads in [None, 'y', 'yes', '1', 't', 'true']\n-        assert self.skip_model_downloads in [None, 'y', 'yes', '1', 't', 'true']\n-        assert self.skip_banlist_downloads in [None, 'y', 'yes', '1', 't', 'true']\n-\n-        if self.skip_downloads:\n-            self.skip_model_downloads = 'yes'\n-            self.skip_banlist_downloads = 'yes'\n-        \n-\n-    def run(self):\n-        # Call the parent class to perform the installation\n-        super().run()\n-\n-        # Download punkt which is necessary for some mappers\n-        nltk.download('punkt')\n-\n-        if not self.skip_model_downloads:\n-            # Download the models\n-            print(\"\\n\\nReached model downloads\\n\\n\")\n-            self._download_fasttext_model()\n-            self._download_quality_models()\n-\n-        # Download the RefinedWeb banlists\n-        if not self.skip_banlist_downloads:\n-            print(\"\\n\\nReached banlist downloads\\n\\n\")\n-            self._create_refinedweb_banlists()\n-\n-    def _download_fasttext_model(self):\n-        url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n-        MODEL_SUBDIRECTORY = \"baselines/mappers/enrichers/language_id_enrichment_models\"\n-        MODEL_FILENAME = \"lid.176.bin\"\n-        destination = os.path.join(PROJECT_ROOT, MODEL_SUBDIRECTORY, MODEL_FILENAME)\n-\n-        if not os.path.exists(destination):\n-            os.makedirs(os.path.dirname(destination), exist_ok=True)\n-            print(f'Downloading {url} to {destination}')\n-            urllib.request.urlretrieve(url, destination)\n-            print(f\"Finsihed downloading {url} to {destination}\")\n-        else:\n-            print(f'File {destination} already exists')\n-\n-    def _download_quality_models(self):\n-        MODEL_SUBDIRECTORY = \"baselines/mappers/enrichers/quality_prediction_enrichment_models\"\n-\n-        # Models and their URLs\n-        models = {\n-            \"model.bin\": \"https://wmtis.s3.eu-west-1.amazonaws.com/quality_prediction_model/model.bin\",\n-            \"en.arpa.bin\": \"https://huggingface.co/edugp/kenlm/resolve/main/wikipedia/en.arpa.bin\",\n-            \"en.sp.model\": \"https://huggingface.co/edugp/kenlm/resolve/main/wikipedia/en.sp.model\"\n-        }\n-\n-        for MODEL_FILENAME, url in models.items():\n-            destination = os.path.join(PROJECT_ROOT, MODEL_SUBDIRECTORY, MODEL_FILENAME)\n-\n-            if not os.path.exists(destination):\n-                print(f\"Downloading {MODEL_FILENAME} to {destination}...\")\n-                os.makedirs(os.path.dirname(destination), exist_ok=True)\n-                urllib.request.urlretrieve(url, destination)\n-                print(f\"Finished downloading {MODEL_FILENAME} to {destination}\")\n-            else:\n-                print(f\"File {destination} already exists\")\n-\n-    def _create_refinedweb_banlists(self):\n-        UNCURATED_BANLISTS_URL = \"ftp://ftp.ut-capitole.fr/pub/reseau/cache/squidguard_contrib/blacklists.tar.gz\"\n-        BANLIST_OUTPUT_DIR = \"baselines/mappers/banlists\"\n-        BANNED_CATEGORIES = [\n-            'adult', \n-            'phishing', \n-            'dating', \n-            'gambling', \n-            'filehosting',\n-            'ddos', \n-            'agressif', \n-            'chat', \n-            'mixed_adult', \n-            'arjel'\n-        ]       \n-\n-        if not os.path.exists(f\"{BANLIST_OUTPUT_DIR}/refinedweb_banned_domains_and_urls.txt\"):\n-            print(f\"Downloading {UNCURATED_BANLISTS_URL}...\")\n-            urllib.request.urlretrieve(UNCURATED_BANLISTS_URL, f\"{BANLIST_OUTPUT_DIR}/blacklists.tar.gz\")\n-\n-            print(\"Extracting banlists...\")\n-            with tarfile.open(f\"{BANLIST_OUTPUT_DIR}/blacklists.tar.gz\") as file:\n-                file.extractall(f\"{BANLIST_OUTPUT_DIR}\")\n-\n-            print(\"Building banlist from target categories...\")\n-            banned_domains = []\n-            banned_urls = []\n-            for category in BANNED_CATEGORIES:\n-                if os.path.exists(f\"{BANLIST_OUTPUT_DIR}/blacklists/{category}/domains\"):\n-                    with open(f\"{BANLIST_OUTPUT_DIR}/blacklists/{category}/domains\", \"r\") as file:\n-                            banned_domains.extend(file.read().splitlines())\n-\n-                if os.path.exists(f\"{BANLIST_OUTPUT_DIR}/blacklists/{category}/urls\"):\n-                    with open(f\"{BANLIST_OUTPUT_DIR}/blacklists/{category}/urls\", \"r\") as file:\n-                            banned_urls.extend(file.read().splitlines())\n-            banlist = banned_domains + banned_urls\n-\n-            # Removes the raw downloads (with all the different categories)\n-            os.remove(f\"{BANLIST_OUTPUT_DIR}/blacklists.tar.gz\")\n-            shutil.rmtree(f'{BANLIST_OUTPUT_DIR}/blacklists')\n-\n-\n-            print(\"Writing banlists to files...\")\n-            with open(f\"{BANLIST_OUTPUT_DIR}/refinedweb_banned_domains.txt\", \"w\") as file:\n-                for item in banned_domains:\n-                    file.write(f\"{item}\\n\")\n-\n-            with open(f\"{BANLIST_OUTPUT_DIR}/refinedweb_banned_urls.txt\", \"w\") as file:\n-                for item in banned_urls:\n-                    file.write(f\"{item}\\n\")\n-\n-            with open(f\"{BANLIST_OUTPUT_DIR}/refinedweb_banned_domains_and_urls.txt\", \"w\") as file:\n-                for item in banlist:\n-                    file.write(f\"{item}\\n\")\n-\n-            banlist = [b.lower() for b in banlist]\n-            pattern = re.compile(Blacklist(banlist, match_substrings=True).compiled)\n-            with open(f\"{BANLIST_OUTPUT_DIR}/refinedweb_banned_domains_and_urls_regex.pkl\", \"wb\") as file:\n-                pickle.dump(pattern, file)\n-\n-        else:\n-            print(f\"File {f'{BANLIST_OUTPUT_DIR}/refinedweb_banned_domains_and_urls.txt'} already exists\")\n-\n-\n-with open('requirements.txt') as f:\n-    required = [r for r in f.read().splitlines() if 'github' not in r]\n-\n-setup(\n-    name='baselines',  # Change this to your package name\n-    version='0.0.1',  # Change this to your package version\n-    description='Description of your package',  # Add a brief description\n-    packages=find_packages(),\n-    install_requires=required,\n-    cmdclass={\n-        'install': DownloadAssetsCommand,\n-    },\n-)\ndiff --git a/training/configs/11m_1x.json b/training/configs/11m_1x.json\nindex 3cd6916..4455d89 100644\n--- a/training/configs/11m_1x.json\n+++ b/training/configs/11m_1x.json\n@@ -18,4 +18,4 @@\n         \"--fsdp-limit-all-gathers\"\n     ],\n     \"chinchilla_multiplier\": 1\n-}\n\\ No newline at end of file\n+}\ndiff --git a/training/configs/1b_1x.json b/training/configs/1b_1x.json\nindex bd0a40b..45b4656 100644\n--- a/training/configs/1b_1x.json\n+++ b/training/configs/1b_1x.json\n@@ -8,7 +8,7 @@\n     \"wd\": 0.033,\n     \"cd\": 3e-5,\n     \"global_bs\": 256,\n-    \"acc\": 2,\n+    \"acc\": 1,\n     \"qk_norm\": true,\n     \"z_loss\": 1e-4,\n     \"grad_checkpointing\": false,\n@@ -18,4 +18,4 @@\n         \"--fsdp-limit-all-gathers\"\n     ],\n     \"chinchilla_multiplier\": 1\n-}\n\\ No newline at end of file\n+}\ndiff --git a/training/configs/3b_1x.json b/training/configs/3b_1x.json\nindex d77a4d4..2e9e15b 100644\n--- a/training/configs/3b_1x.json\n+++ b/training/configs/3b_1x.json\n@@ -8,7 +8,7 @@\n     \"wd\": 0.33,\n     \"cd\": 3e-05,\n     \"global_bs\": 2048,\n-    \"acc\": 2,\n+    \"acc\": 4,\n     \"qk_norm\": true,\n     \"z_loss\": 1e-4,\n     \"grad_checkpointing\": false,\ndiff --git a/training/train_scripts/docker/Dockerfile_update b/training/train_scripts/docker/Dockerfile_update\nindex b46252b..2559a85 100644\n--- a/training/train_scripts/docker/Dockerfile_update\n+++ b/training/train_scripts/docker/Dockerfile_update\n@@ -10,5 +10,6 @@ COPY . /opt/ml/code/\n \n # # Prevent sagemaker from installing requirements again.\n RUN rm /opt/ml/code/requirements.txt\n+RUN pip install --upgrade s3fs\n \n ENV SAGEMAKER_PROGRAM training/train.py",
    "data_key": "json.gz",
    "sampling_yaml": {
        "sources": [
            {
                "source": "COMMON_CRAWL",
                "markers": [
                    "common_crawl/"
                ]
            },
            {
                "source": "C4",
                "markers": [
                    "c4/"
                ]
            },
            {
                "source": "GITHUB",
                "markers": [
                    "github"
                ]
            },
            {
                "source": "WIKIPEDIA",
                "markers": [
                    "wiki"
                ]
            },
            {
                "source": "BOOKS",
                "markers": [
                    "book"
                ]
            },
            {
                "source": "ARXIV",
                "markers": [
                    "arxiv"
                ]
            },
            {
                "source": "STACKEXCHANGE",
                "markers": [
                    "stackexchange"
                ]
            },
            {
                "source": "UNKNOWN",
                "markers": []
            }
        ],
        "sampling_frequencies": {
            "COMMON_CRAWL": 0.927137103,
            "C4": 1.031133101,
            "GITHUB": 0.917404247,
            "WIKIPEDIA": 2.176546915,
            "BOOKS": 2.077507187,
            "ARXIV": 1.068963936,
            "STACKEXCHANGE": 1.169580328
        }
    }
}