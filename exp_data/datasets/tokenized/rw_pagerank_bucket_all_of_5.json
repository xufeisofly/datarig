{
    "uuid": "f983c83d-69b4-481d-8536-ff2be9ae5b96",
    "name": "rw_pagerank_bucket_all_of_5",
    "creation_date": "2024_01_30-19_47_04",
    "dataset_url": "s3://***REMOVED***/openlm/dcnlp/datasets/rw_pagerank_buckets/all_of_5/",
    "manifest_url": "s3://***REMOVED***/openlm/dcnlp/datasets/rw_pagerank_buckets/all_of_5/manifest.jsonl",
    "sources": [
        {
            "uuid": "d99b1c9f-e611-4f8d-bf50-1edea98748ab",
            "name": "refinedweb-pagerank-bucket-0-of-5"
        },
        {
            "uuid": "52bbaaf2-faec-44f7-9c77-627e3632e9f4",
            "name": "refinedweb-pagerank-bucket-1-of-5"
        },
        {
            "uuid": "21979427-e6ed-4404-9ee7-1f3b41239eeb",
            "name": "refinedweb-pagerank-bucket-2-of-5"
        },
        {
            "uuid": "720df564-0dbd-45e4-81bc-15f52f58c7c9",
            "name": "refinedweb-pagerank-bucket-3-of-5"
        },
        {
            "uuid": "caada354-f18d-4738-ae30-6a7b7808d2ca",
            "name": "refinedweb-pagerank-bucket-4-of-5"
        }
    ],
    "tokenized": true,
    "tokenizer": "EleutherAI/gpt-neox-20b",
    "num_tokens": 237114761163,
    "size": 644756088035,
    "dcnlp_commit_hash": "5d198623e22d689b06a0eb41eeb4dbdf622eca99",
    "dcnlp_diff": "diff --git a/exp_data/datasets/tokenized/c4_original.json b/exp_data/datasets/tokenized/c4_original.json\nindex b2f9f97..9f0a4b8 100644\n--- a/exp_data/datasets/tokenized/c4_original.json\n+++ b/exp_data/datasets/tokenized/c4_original.json\n@@ -4,8 +4,8 @@\n     \"tokenized\": true,\n     \"num_tokens\": 174605508363,\n     \"size\": 1123288754203,\n-    \"dataset_url\": \"s3://***REMOVED***/original_c4/\",\n-    \"manifest_url\": \"s3://***REMOVED***/original_c4/manifest.jsonl\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/datasets/original_c4/\",\n+    \"manifest_url\": \"s3://***REMOVED***/openlm/dcnlp/datasets/original_c4/manifest.jsonl\",\n     \"dcnlp_commit_hash\": \"\",\n     \"dcnlp_diff\": \"\",\n     \"uuid\": \"7e0f5507-aa36-4d8c-9026-d049f885adf1\",\n@@ -13,4 +13,4 @@\n     \"tokenizer\": \"EleutherAI/gpt-neox-20b\",\n     \"data_key\": \"txt\",\n     \"sampling_yaml\": null\n-}\n\\ No newline at end of file\n+}\ndiff --git a/exp_data/datasets/tokenized/c4_wo_dedup_trafilatura_subset_15k.json b/exp_data/datasets/tokenized/c4_wo_dedup_trafilatura_subset_15k.json\nindex 790c3d7..b1c6229 100644\n--- a/exp_data/datasets/tokenized/c4_wo_dedup_trafilatura_subset_15k.json\n+++ b/exp_data/datasets/tokenized/c4_wo_dedup_trafilatura_subset_15k.json\n@@ -2,8 +2,8 @@\n     \"uuid\": \"9ab4fea8-aa36-4657-8fc5-dd625c9ec0eb\",\n     \"name\": \"c4_wo_dedup_trafilatura_subset_15k\",\n     \"creation_date\": \"2024_01_23-17_24_08\",\n-    \"dataset_url\": \"s3://dcnlp-west/c4_wo_dedup_trafilatura_subset_15k_tokenized\",\n-    \"manifest_url\": \"s3://dcnlp-west/c4_wo_dedup_trafilatura_subset_15k_tokenized/manifest.jsonl\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/datasets/c4_wo_dedup_trafilatura_subset_15k_tokenized\",\n+    \"manifest_url\": \"s3://***REMOVED***/openlm/dcnlp/datasets/c4_wo_dedup_trafilatura_subset_15k_tokenized/manifest.jsonl\",\n     \"sources\": [\n         {\n             \"uuid\": \"bcecbb58-c05b-4c7b-b992-17c83cb14002\",\n@@ -18,4 +18,4 @@\n     \"dcnlp_diff\": \"diff --git a/exp_data/datasets/untokenized/c4_wo_dedup_trafilatura.json b/exp_data/datasets/untokenized/c4_wo_dedup_trafilatura.json\\nindex 8e33d11..02ac959 100644\\n--- a/exp_data/datasets/untokenized/c4_wo_dedup_trafilatura.json\\n+++ b/exp_data/datasets/untokenized/c4_wo_dedup_trafilatura.json\\n@@ -2,7 +2,7 @@\\n     \\\"uuid\\\": \\\"bcecbb58-c05b-4c7b-b992-17c83cb14002\\\",\\n     \\\"name\\\": \\\"c4_wo_dedup_trafilatura\\\",\\n     \\\"creation_date\\\": \\\"2024_01_17-18_23_01\\\",\\n-    \\\"dataset_url\\\": \\\"s3://dcnlp-west/c4_wo_dedup_trafilatura/c4_wo_dedup/crawl-data/\\\",\\n+    \\\"dataset_url\\\": \\\"s3://dcnlp-west/c4_wo_dedup_trafilatura_v2/c4_wo_dedup/crawl-data/\\\",\\n     \\\"manifest_url\\\": null,\\n     \\\"sources\\\": [\\n         {\\ndiff --git a/training/train_scripts/one_node_tacc.sh b/training/train_scripts/one_node_tacc.sh\\nindex 9b4aa31..2c5129d 100644\\n--- a/training/train_scripts/one_node_tacc.sh\\n+++ b/training/train_scripts/one_node_tacc.sh\\n@@ -1,8 +1,8 @@\\n #!/bin/bash\\n #SBATCH --partition=gpu-a100\\n #SBATCH --job-name=dclm\\n-#SBATCH --nodes 1\\n-#SBATCH --ntasks-per-node=3\\n+#SBATCH --nodes 8\\n+#SBATCH --ntasks-per-node=2\\n #SBATCH --cpus-per-task=42\\n #SBATCH -t 48:00:00\\n #SBATCH --output=%x_%j.out\\n@@ -27,8 +27,8 @@ export FI_PROVIDER=efa\\n export FI_EFA_TX_MIN_CREDITS=64\\n export NCCL_TREE_THRESHOLD=0\\n \\n-export PATH=\\\"$WORK/conda/miniconda3/condabin:$PATH\\\"\\n-source $WORK/conda/miniconda3/etc/profile.d/conda.sh\\n+export PATH=\\\"/work/08002/gsmyrnis/frontera/conda/miniconda3/condabin:$PATH\\\"\\n+source /work/08002/gsmyrnis/frontera/conda/miniconda3/etc/profile.d/conda.sh\\n conda activate dcnlp  # install according to tng/tools/environment.yml\\n \\n SCALE=$1 \",\n     \"data_key\": \"json.gz\",\n     \"sampling_yaml\": null\n-}\n\\ No newline at end of file\n+}\ndiff --git a/exp_data/datasets/tokenized/rw_original.json b/exp_data/datasets/tokenized/rw_original.json\nindex bed3824..d30b02d 100644\n--- a/exp_data/datasets/tokenized/rw_original.json\n+++ b/exp_data/datasets/tokenized/rw_original.json\n@@ -4,8 +4,8 @@\n     \"tokenized\": true,\n     \"num_tokens\": 579578773317,\n     \"size\": 1565888774322,\n-    \"dataset_url\": \"s3://***REMOVED***/refined_web_tokenized/\",\n-    \"manifest_url\": \"s3://***REMOVED***/refined_web_tokenized/manifest.jsonl\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/datasets/refined_web_tokenized/\",\n+    \"manifest_url\": \"s3://***REMOVED***/openlm/dcnlp/datasets/refined_web_tokenized/manifest.jsonl\",\n     \"dcnlp_commit_hash\": \"\",\n     \"dcnlp_diff\": \"\",\n     \"uuid\": \"7e0f5507-aa36-4d8c-9026-d049f885adf7\",\n@@ -13,4 +13,4 @@\n     \"tokenizer\": \"EleutherAI/gpt-neox-20b\",\n     \"data_key\": \"json.gz\",\n     \"sampling_yaml\": null\n-}\n\\ No newline at end of file\n+}\ndiff --git a/exp_data/datasets/untokenized/rpj_original.json b/exp_data/datasets/untokenized/rpj_original.json\nindex 817a094..d60f561 100644\n--- a/exp_data/datasets/untokenized/rpj_original.json\n+++ b/exp_data/datasets/untokenized/rpj_original.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"a49a6b1a-d357-475e-96a5-7a559ad927ef\",\n     \"name\": \"rpj_original\",\n     \"creation_date\": \"2024_01_05-10_38_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rpj_original_arxiv.json b/exp_data/datasets/untokenized/rpj_original_arxiv.json\nindex aea173a..21d27a8 100644\n--- a/exp_data/datasets/untokenized/rpj_original_arxiv.json\n+++ b/exp_data/datasets/untokenized/rpj_original_arxiv.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"c8b17a9b-6bd8-441a-8b9f-dbf486edf574\",\n     \"name\": \"rpj_original_arxiv\",\n     \"creation_date\": \"2023_12_31-14_21_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/arxiv/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/arxiv/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rpj_original_books.json b/exp_data/datasets/untokenized/rpj_original_books.json\nindex de40689..51f5c75 100644\n--- a/exp_data/datasets/untokenized/rpj_original_books.json\n+++ b/exp_data/datasets/untokenized/rpj_original_books.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"d017c1fe-c9df-4e06-aa8f-d92b1097283b\",\n     \"name\": \"rpj_original_books\",\n     \"creation_date\": \"2023_12_31-14_21_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/books_were_too_long_for_vaishaal_to_read/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/books_were_too_long_for_vaishaal_to_read/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rpj_original_cc.json b/exp_data/datasets/untokenized/rpj_original_cc.json\nindex 4a322df..e538171 100644\n--- a/exp_data/datasets/untokenized/rpj_original_cc.json\n+++ b/exp_data/datasets/untokenized/rpj_original_cc.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"15701e36-c0bb-4bfa-bf52-d3419dbbd8a1\",\n     \"name\": \"rpj_original_cc\",\n     \"creation_date\": \"2024_01_05-10_38_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/common_crawl/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/common_crawl/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rpj_original_github.json b/exp_data/datasets/untokenized/rpj_original_github.json\nindex 1380c00..d7546f7 100644\n--- a/exp_data/datasets/untokenized/rpj_original_github.json\n+++ b/exp_data/datasets/untokenized/rpj_original_github.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"edd67f24-49ae-4915-8c3a-dd4bcc62b9d8\",\n     \"name\": \"rpj_original_github\",\n     \"creation_date\": \"2023_12_31-14_21_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/github/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/github/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rpj_original_non_CC.json b/exp_data/datasets/untokenized/rpj_original_non_CC.json\nindex 181fbe5..bade67c 100644\n--- a/exp_data/datasets/untokenized/rpj_original_non_CC.json\n+++ b/exp_data/datasets/untokenized/rpj_original_non_CC.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"807c9277-7b10-4133-882d-09e22369587b\",\n     \"name\": \"rpj_original_non_CC\",\n     \"creation_date\": \"2023_12_31-14_21_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rpj_original_stackexchange.json b/exp_data/datasets/untokenized/rpj_original_stackexchange.json\nindex 12290b1..f337d4c 100644\n--- a/exp_data/datasets/untokenized/rpj_original_stackexchange.json\n+++ b/exp_data/datasets/untokenized/rpj_original_stackexchange.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"3b25b18c-e724-4071-8c7a-d69c5e1aaeac\",\n     \"name\": \"rpj_original_stackexchange\",\n     \"creation_date\": \"2023_12_31-14_21_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/stackexchange/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/stackexchange/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rpj_original_wiki.json b/exp_data/datasets/untokenized/rpj_original_wiki.json\nindex d98f66b..b7f70b0 100644\n--- a/exp_data/datasets/untokenized/rpj_original_wiki.json\n+++ b/exp_data/datasets/untokenized/rpj_original_wiki.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"050bc436-8d61-4d73-b931-0306a4b26727\",\n     \"name\": \"rpj_original_wiki\",\n     \"creation_date\": \"2023_12_31-14_21_45\",\n-    \"dataset_url\": \"s3://dcnlp-west/redpajama-real/wiki/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/redpajama_raw/wiki/\",\n     \"manifest_url\": null,\n     \"sources\": [],\n     \"tokenized\": false,\ndiff --git a/exp_data/datasets/untokenized/rw_v2_wo_dedup.json b/exp_data/datasets/untokenized/rw_v2_wo_dedup.json\nindex 568a040..b8b181a 100644\n--- a/exp_data/datasets/untokenized/rw_v2_wo_dedup.json\n+++ b/exp_data/datasets/untokenized/rw_v2_wo_dedup.json\n@@ -2,7 +2,7 @@\n     \"uuid\": \"a3fc0223-4592-4fab-9c80-a838a9daeb93\",\n     \"name\": \"rw_v2_wo_dedup\",\n     \"creation_date\": \"2023_11_17-02_45_21\",\n-    \"dataset_url\": \"s3://dcnlp-west/cc_trafilatura_v2-baselines/refinedweb_V2/\",\n+    \"dataset_url\": \"s3://***REMOVED***/openlm/dcnlp/raw_datasets/cc_trafilatura_v2-baselines/refinedweb_V2/\",\n     \"manifest_url\": null,\n     \"sources\": [\n         {\n@@ -17,4 +17,4 @@\n     \"dcnlp_commit_hash\": \"685edcfe5cc34755b0be89eb03e053434dc12f4b\",\n     \"dcnlp_diff\": null,\n     \"data_key\": \"jsonl.zstd\"\n-}\n\\ No newline at end of file\n+}\ndiff --git a/ray_processing/cluster_tri_tokenize_shuffle.yaml b/ray_processing/cluster_tri_tokenize_shuffle.yaml\nindex 689c458..22d7fc9 100644\n--- a/ray_processing/cluster_tri_tokenize_shuffle.yaml\n+++ b/ray_processing/cluster_tri_tokenize_shuffle.yaml\n@@ -55,5 +55,6 @@ setup_commands:\n     - pip install 'pandas==2.1.4'\n     - pip install psutil\n     - pip install pyarrow\n-    - pip install git+https://github.com/mlfoundations/open_lm.git\n+    - pip install llm-foundry==0.4.0\n+    - pip install git+https://github.com/mlfoundations/open_lm.git@3133be1215e5e4b8dc66109a7daaa9849930973d\n \ndiff --git a/requirements.txt b/requirements.txt\nindex ef0711b..d4445cb 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -24,11 +24,11 @@ moto\n git+https://github.com/mlfoundations/open_lm.git\n scipy\n fire\n-s3fs\n+s3fs>=2023.6.0\n braceexpand\n pysimdjson\n gitpython\n Unidecode\n beautifulsoup4\n zstandard\n-git+https://github.com/mosaicml/llm-foundry.git\n\\ No newline at end of file\n+git+https://github.com/mosaicml/llm-foundry.git\ndiff --git a/setup.py b/setup.py\ndeleted file mode 100644\nindex 00f0adc..0000000\n--- a/setup.py\n+++ /dev/null\n@@ -1,188 +0,0 @@\n-from __future__ import annotations\n-import os\n-import urllib.request\n-import tarfile\n-import shutil\n-import argparse\n-from setuptools.command.install import install\n-from setuptools import setup, find_packages\n-from retrie.retrie import Blacklist\n-import pickle\n-import re\n-import nltk\n-import boto3\n-\n-PROJECT_ROOT = os.path.dirname(__file__)\n-\n-class DownloadAssetsCommand(install):\n-    description = 'download and set up larger assets (e.g., models, banlists) after installation'\n-\n-    user_options = install.user_options + [\n-        ('skip-downloads=', 's', \"whether to skip all downloads\"),\n-        ('skip-model-downloads=', None, \"whether to skip model downloads\"),\n-        ('skip-banlist-downloads=', None, \"whether to skip banlist downloads\"),\n-        ('rw-banlist-type=', None, \"whether to skip banlist downloads\")\n-    ]\n-\n-    def initialize_options(self):\n-        install.initialize_options(self)\n-        self.skip_downloads = None\n-        self.skip_model_downloads = None\n-        self.skip_banlist_downloads = None\n-        self.rw_banlist_type = 'curated'\n-\n-    def finalize_options(self):\n-        install.finalize_options(self)\n-\n-        assert self.skip_downloads in [None, 'y', 'yes', '1', 't', 'true']\n-        assert self.skip_model_downloads in [None, 'y', 'yes', '1', 't', 'true']\n-        assert self.skip_banlist_downloads in [None, 'y', 'yes', '1', 't', 'true']\n-        assert self.rw_banlist_type in ['curated', 'uncurated']\n-\n-        if self.skip_downloads:\n-            self.skip_model_downloads = 'yes'\n-            self.skip_banlist_downloads = 'yes'\n-        \n-\n-    def run(self):\n-        # Call the parent class to perform the installation\n-        super().run()\n-\n-        # Download punkt which is necessary for some mappers\n-        nltk.download('punkt')\n-\n-        if not self.skip_model_downloads:\n-            # Download the models\n-            print(\"\\n\\nReached model downloads\\n\\n\")\n-            self._download_fasttext_model()\n-            self._download_quality_models()\n-\n-        # Download the RefinedWeb banlists\n-        if not self.skip_banlist_downloads:\n-            print(\"\\n\\nReached banlist downloads\\n\\n\")\n-            if self.rw_banlist_type == 'curated':\n-                self._download_curated_refinedweb_banlists()\n-            elif self.rw_banlist_type == 'uncurated':\n-                self._create_refinedweb_banlists()\n-\n-    def _download_fasttext_model(self):\n-        url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n-        MODEL_SUBDIRECTORY = \"baselines/mappers/enrichers/language_id_enrichment_models\"\n-        MODEL_FILENAME = \"lid.176.bin\"\n-        destination = os.path.join(PROJECT_ROOT, MODEL_SUBDIRECTORY, MODEL_FILENAME)\n-\n-        if not os.path.exists(destination):\n-            os.makedirs(os.path.dirname(destination), exist_ok=True)\n-            print(f'Downloading {url} to {destination}')\n-            urllib.request.urlretrieve(url, destination)\n-            print(f\"Finsihed downloading {url} to {destination}\")\n-        else:\n-            print(f'File {destination} already exists')\n-\n-    def _download_quality_models(self):\n-        MODEL_SUBDIRECTORY = \"baselines/mappers/enrichers/quality_prediction_enrichment_models\"\n-\n-        # Models and their URLs\n-        models = {\n-            \"model.bin\": \"https://wmtis.s3.eu-west-1.amazonaws.com/quality_prediction_model/model.bin\",\n-            \"en.arpa.bin\": \"https://huggingface.co/edugp/kenlm/resolve/main/wikipedia/en.arpa.bin\",\n-            \"en.sp.model\": \"https://huggingface.co/edugp/kenlm/resolve/main/wikipedia/en.sp.model\"\n-        }\n-\n-        for MODEL_FILENAME, url in models.items():\n-            destination = os.path.join(PROJECT_ROOT, MODEL_SUBDIRECTORY, MODEL_FILENAME)\n-\n-            if not os.path.exists(destination):\n-                print(f\"Downloading {MODEL_FILENAME} to {destination}...\")\n-                os.makedirs(os.path.dirname(destination), exist_ok=True)\n-                urllib.request.urlretrieve(url, destination)\n-                print(f\"Finished downloading {MODEL_FILENAME} to {destination}\")\n-            else:\n-                print(f\"File {destination} already exists\")\n-\n-    def _download_curated_refinedweb_banlists(self):\n-        CURATED_BANLIST_PATH = \"baselines/mappers/banlists/refinedweb_banned_domains_curated.txt\"\n-        s3 = boto3.client('s3')\n-        print(\"Downloading curated banlist\")\n-        if not os.path.exists(CURATED_BANLIST_PATH):\n-            s3.download_file('dcnlp-west', 'refinedweb_url_banlists/refinedweb_banned_domains_curated.txt', CURATED_BANLIST_PATH)  \n-        else:\n-            print(f\"Curated banlist for refinedweb already exists at {CURATED_BANLIST_PATH}\")\n-\n-    def _create_refinedweb_banlists(self):\n-        UNCURATED_BANLISTS_URL = \"ftp://ftp.ut-capitole.fr/pub/reseau/cache/squidguard_contrib/blacklists.tar.gz\"\n-        BANLIST_OUTPUT_DIR = \"baselines/mappers/banlists\"\n-        BANNED_CATEGORIES = [\n-            'adult', \n-            'phishing', \n-            'dating', \n-            'gambling', \n-            'filehosting',\n-            'ddos', \n-            'agressif', \n-            'chat', \n-            'mixed_adult', \n-            'arjel'\n-        ]       \n-\n-        if not os.path.exists(f\"{BANLIST_OUTPUT_DIR}/refinedweb_banned_domains_and_urls.txt\"):\n-            print(f\"Downloading {UNCURATED_BANLISTS_URL}...\")\n-            urllib.request.urlretrieve(UNCURATED_BANLISTS_URL, f\"{BANLIST_OUTPUT_DIR}/blacklists.tar.gz\")\n-\n-            print(\"Extracting banlists...\")\n-            with tarfile.open(f\"{BANLIST_OUTPUT_DIR}/blacklists.tar.gz\") as file:\n-                file.extractall(f\"{BANLIST_OUTPUT_DIR}\")\n-\n-            print(\"Building banlist from target categories...\")\n-            banned_domains = []\n-            banned_urls = []\n-            for category in BANNED_CATEGORIES:\n-                if os.path.exists(f\"{BANLIST_OUTPUT_DIR}/blacklists/{category}/domains\"):\n-                    with open(f\"{BANLIST_OUTPUT_DIR}/blacklists/{category}/domains\", \"r\") as file:\n-                            banned_domains.extend(file.read().splitlines())\n-\n-                if os.path.exists(f\"{BANLIST_OUTPUT_DIR}/blacklists/{category}/urls\"):\n-                    with open(f\"{BANLIST_OUTPUT_DIR}/blacklists/{category}/urls\", \"r\") as file:\n-                            banned_urls.extend(file.read().splitlines())\n-            banlist = banned_domains + banned_urls\n-\n-            # Removes the raw downloads (with all the different categories)\n-            os.remove(f\"{BANLIST_OUTPUT_DIR}/blacklists.tar.gz\")\n-            shutil.rmtree(f'{BANLIST_OUTPUT_DIR}/blacklists')\n-\n-\n-            print(\"Writing banlists to files...\")\n-            with open(f\"{BANLIST_OUTPUT_DIR}/refinedweb_banned_domains.txt\", \"w\") as file:\n-                for item in banned_domains:\n-                    file.write(f\"{item}\\n\")\n-\n-            with open(f\"{BANLIST_OUTPUT_DIR}/refinedweb_banned_urls.txt\", \"w\") as file:\n-                for item in banned_urls:\n-                    file.write(f\"{item}\\n\")\n-\n-            with open(f\"{BANLIST_OUTPUT_DIR}/refinedweb_banned_domains_and_urls.txt\", \"w\") as file:\n-                for item in banlist:\n-                    file.write(f\"{item}\\n\")\n-\n-            banlist = [b.lower() for b in banlist]\n-            pattern = re.compile(Blacklist(banlist, match_substrings=True).compiled)\n-            with open(f\"{BANLIST_OUTPUT_DIR}/refinedweb_banned_domains_and_urls_regex.pkl\", \"wb\") as file:\n-                pickle.dump(pattern, file)\n-\n-        else:\n-            print(f\"File {f'{BANLIST_OUTPUT_DIR}/refinedweb_banned_domains_and_urls.txt'} already exists\")\n-\n-\n-with open('requirements.txt') as f:\n-    required = [r for r in f.read().splitlines() if 'github' not in r]\n-\n-setup(\n-    name='baselines',  # Change this to your package name\n-    version='0.0.1',  # Change this to your package version\n-    description='Description of your package',  # Add a brief description\n-    packages=find_packages(),\n-    install_requires=required,\n-    cmdclass={\n-        'install': DownloadAssetsCommand,\n-    },\n-)\ndiff --git a/training/configs/154m_1x.json b/training/configs/154m_1x.json\nindex 6ee80b2..69fbf34 100644\n--- a/training/configs/154m_1x.json\n+++ b/training/configs/154m_1x.json\n@@ -18,4 +18,4 @@\n         \"--fsdp-limit-all-gathers\"\n     ],\n     \"chinchilla_multiplier\": 1\n-}\n\\ No newline at end of file\n+}\ndiff --git a/training/configs/1b_1x.json b/training/configs/1b_1x.json\nindex bd0a40b..536d02f 100644\n--- a/training/configs/1b_1x.json\n+++ b/training/configs/1b_1x.json\n@@ -8,7 +8,7 @@\n     \"wd\": 0.033,\n     \"cd\": 3e-5,\n     \"global_bs\": 256,\n-    \"acc\": 2,\n+    \"acc\": 1,\n     \"qk_norm\": true,\n     \"z_loss\": 1e-4,\n     \"grad_checkpointing\": false,\ndiff --git a/training/configs/3b_1x.json b/training/configs/3b_1x.json\nindex d77a4d4..2e9e15b 100644\n--- a/training/configs/3b_1x.json\n+++ b/training/configs/3b_1x.json\n@@ -8,7 +8,7 @@\n     \"wd\": 0.33,\n     \"cd\": 3e-05,\n     \"global_bs\": 2048,\n-    \"acc\": 2,\n+    \"acc\": 4,\n     \"qk_norm\": true,\n     \"z_loss\": 1e-4,\n     \"grad_checkpointing\": false,\ndiff --git a/training/configs/411m_1x.json b/training/configs/411m_1x.json\nindex 85a7d1e..b3ddb28 100644\n--- a/training/configs/411m_1x.json\n+++ b/training/configs/411m_1x.json\n@@ -8,7 +8,7 @@\n     \"wd\": 0.033,\n     \"cd\": 3e-05,\n     \"global_bs\": 512,\n-    \"acc\": 8,\n+    \"acc\": 2,\n     \"qk_norm\": true,\n     \"z_loss\": 1e-4,\n     \"grad_checkpointing\": false,\ndiff --git a/training/params.py b/training/params.py\nindex d5c719b..e9eefc5 100644\n--- a/training/params.py\n+++ b/training/params.py\n@@ -104,6 +104,11 @@ def parse_dcnlp_args():\n         default=None,\n         help=\"pass a model json\",\n     )\n+    parser.add_argument(\n+        \"--skip-eval\",\n+        action=\"store_true\",\n+        help=\"Whether to skip evaluation\",\n+    )\n     parser.add_argument(\n         \"--downstream-eval\",\n         required=False,\n@@ -160,6 +165,12 @@ def parse_dcnlp_args():\n         default=None,\n         help=\"gradient acc steps\",\n     )\n+    parser.add_argument(\n+        \"--skip-train\",\n+        action=\"store_true\",\n+        help=\"If true, skip training. Useful for creating a model json.\"\n+    )\n+    parser.add_argument(\"--multiple-data-passes\", action=\"store_true\")\n \n     args = parser.parse_args()\n \n@@ -231,6 +242,8 @@ def get_open_lm_args(args, hparams, dr):\n         \"--logs\",\n         f\"{args.logs}\",\n     ]\n+    if args.multiple_data_passes:\n+        open_lm_args.append(\"--multiple-data-passes\")\n \n     name = None\n     if args.re_evaluate is None:\n@@ -266,11 +279,14 @@ def get_open_lm_args(args, hparams, dr):\n             ]\n         )\n \n-    openlm_val_data = download_val_data(\"open_lm_val\", skip_download=local_rank != 0)\n-    c4_val_data = download_val_data(\"c4_val\", skip_download=local_rank != 0)\n-    paloma_val_data = download_val_data(\"paloma_val\", skip_download=local_rank != 0)\n+    if not args.skip_eval:\n+        openlm_val_data = download_val_data(\"open_lm_val\", skip_download=local_rank != 0)\n+        c4_val_data = download_val_data(\"c4_val\", skip_download=local_rank != 0)\n+        paloma_val_data = download_val_data(\"paloma_val\", skip_download=local_rank != 0)\n \n-    if args.downstream_eval:\n+    if args.skip_eval:\n+        pass\n+    elif args.downstream_eval:\n         tasks = load_ppl_yaml()\n         downstream_datas = [download_val_data(task_name, skip_download=local_rank != 0) for task_name in tasks]\n \ndiff --git a/training/train.py b/training/train.py\nindex a36fa3f..ab662c5 100644\n--- a/training/train.py\n+++ b/training/train.py\n@@ -74,14 +74,15 @@ if __name__ == \"__main__\":\n         # create this dir to prevent sync'ing errors\n         os.makedirs(os.path.join(args.logs, name), exist_ok=True)\n \n-    print(f\"Running with args:\\n{open_lm_args}\")\n+    if not args.skip_train:\n+        print(f\"Running with args:\\n{open_lm_args}\")\n \n-    try:\n-        main(open_lm_args)\n-    except Exception as e:\n-        if rank == 0:\n-            logger.error(e)\n-            logger.error(traceback.format_exc())\n+        try:\n+            main(open_lm_args)\n+        except Exception as e:\n+            if rank == 0:\n+                logger.error(e)\n+                logger.error(traceback.format_exc())\n \n     if rank == 0:\n         time.sleep(10)\n@@ -92,7 +93,7 @@ if __name__ == \"__main__\":\n             fs, exp_root = fsspec.core.url_to_fs(os.path.join(args.logs, name))\n \n         stats_glob = os.path.join(exp_root, \"checkpoints\", \"stats_*.pt\")\n-        results_jsonl = os.path.join(exp_root, \"checkpoints\", \"results.jsonl\")\n+        # results_jsonl = os.path.join(exp_root, \"checkpoints\", \"results.jsonl\")\n \n         stats = fs.glob(stats_glob)\n         stats = sorted(stats, key=natural_key)\n@@ -111,7 +112,7 @@ if __name__ == \"__main__\":\n \n         assert fs.exists(final_checkpoint), \"final checkpoint does not exist\"\n         assert fs.exists(params_txt), \"params.txt does not exist\"\n-        assert fs.exists(results_jsonl), \"results.jsonl does not exist\"\n+        # assert fs.exists(results_jsonl), \"results.jsonl does not exist\"\n \n         logger.info(\"Done training.\")\n \n@@ -124,14 +125,14 @@ if __name__ == \"__main__\":\n             model = ModelReference(**read_data)\n \n             # override results\n-            # model.results = stats[\"evaluation_metrics\"]\n-\n-            # get most up to date eval from the results.jsonl\n-            results = None\n-            with fs.open(results_jsonl, \"r\") as f:\n-                logger.info(\"loading results.jsonl\")\n-                results = json.loads(list(f)[-1])\n-            model.results = results\n+            model.results = stats[\"evaluation_metrics\"]\n+\n+            # # get most up to date eval from the results.jsonl\n+            # results = None\n+            # with fs.open(results_jsonl, \"r\") as f:\n+            #     logger.info(\"loading results.jsonl\")\n+            #     results = json.loads(list(f)[-1])\n+            # model.results = results\n \n         else:\n             model = ModelReference(\ndiff --git a/training/train_scripts/docker/Dockerfile.p5 b/training/train_scripts/docker/Dockerfile.p5\nindex eb9d237..6ab9203 100644\n--- a/training/train_scripts/docker/Dockerfile.p5\n+++ b/training/train_scripts/docker/Dockerfile.p5\n@@ -86,6 +86,7 @@ RUN pip install -r /opt/ml/code/requirements.txt\n # # Prevent sagemaker from installing requirements again.\n # RUN rm /opt/ml/code/setup.py\n RUN rm /opt/ml/code/requirements.txt\n+RUN pip install --upgrade s3fs\n \n # Defines a script entrypoint \n ENV SAGEMAKER_PROGRAM training/train.py\ndiff --git a/training/train_scripts/train_sagemaker.py b/training/train_scripts/train_sagemaker.py\nindex cca62e3..6d45c3e 100644\n--- a/training/train_scripts/train_sagemaker.py\n+++ b/training/train_scripts/train_sagemaker.py\n@@ -86,6 +86,8 @@ def main():\n     parser.add_argument(\"--data-config\", required=True)\n     parser.add_argument(\"--remote-sync\", required=True, help=\"S3 path to sync to\")\n     parser.add_argument(\"--chinchilla-multiplier\", required=False, type=float)\n+    parser.add_argument(\"--skip-eval\", action=\"store_true\")\n+    parser.add_argument(\"--multiple-data-passes\", action=\"store_true\")\n \n     # Docker / AWS args\n     parser.add_argument(\"--docker-dir\", type=Path, default=Path(__file__).parent / \"docker\")\n@@ -168,6 +170,10 @@ def main_after_setup_move(args):\n     }\n     if args.chinchilla_multiplier:\n         train_args[\"chinchilla-multiplier\"] = args.chinchilla_multiplier\n+    if args.skip_eval:\n+        train_args[\"skip-eval\"] = \"\"\n+    if args.multiple_data_passes:\n+        train_args[\"multiple-data-passes\"] = \"\"\n \n     estimator = PyTorch(\n         entry_point=\"training/train.py\",\n@@ -186,7 +192,7 @@ def main_after_setup_move(args):\n         # Training using SMDataParallel Distributed Training Framework\n         distribution={\"torch_distributed\": {\"enabled\": True}},\n         # Max run 5 days\n-        max_run=5 * 24 * 60 * 60,\n+        max_run=10 * 24 * 60 * 60,\n         max_wait=5 * 24 * 60 * 60 if args.spot_instance else None,\n         input_mode=\"FastFile\",\n         # environment={\"TORCH_DISTRIBUTED_DEBUG\": \"DETAIL\", \"TORCH_CPP_LOG_LEVEL\": \"INFO\"},",
    "data_key": "json.gz",
    "sampling_yaml": null
}